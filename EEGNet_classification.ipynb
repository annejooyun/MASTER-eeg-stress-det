{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.data import extract_eeg_data, multi_to_binary_classification, split_dataset, dict_to_arr\n",
    "from utils.labels import get_stai_labels\n",
    "from utils.valid_recs import get_valid_recs\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification, EEGNet_TSGLEEGNet_classification\n",
    "import utils.variables as v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P010_S001_001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P013_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_002\n",
      "ERROR:root:1) Failed to read data for recording P020_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P023_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n"
     ]
    }
   ],
   "source": [
    "valid_recs = get_valid_recs(data_type='new_ica', output_type = 'np')\n",
    "print(f'Valid recs {valid_recs}')\n",
    "\n",
    "x_dict_ = extract_eeg_data(valid_recs, data_type='new_ica', output_type='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n"
     ]
    }
   ],
   "source": [
    "y_dict_ = get_stai_labels(valid_recs) \n",
    "#y_dict = get_pss_labels(valid_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing invalid labels: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing invalid labels: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n"
     ]
    }
   ],
   "source": [
    "x_dict, y_dict = multi_to_binary_classification(x_dict_, y_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing mildly stressed subjects: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing  mildly stressed subjects: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict, test_data_dict, val_data_dict, train_labels_dict, test_labels_dict, val_labels_dict = split_dataset(x_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train data set: {len(train_data_dict)}\")\n",
    "print(f\"Length of validation data set: {len(val_data_dict)}\")\n",
    "print(f\"Length of test data set: {len(test_data_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (44, 8, 38400)\n",
      "Shape of train labels set: (44, 1)\n",
      "Shape of validation data set: (16, 8, 38400)\n",
      "Shape of validation labels set: (16, 1)\n",
      "Shape of test data set: (19, 8, 38400)\n",
      "Shape of test labels set: (19, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = dict_to_arr(train_data_dict, 'new_ica')\n",
    "test_data = dict_to_arr(test_data_dict, 'new_ica')\n",
    "val_data = dict_to_arr(val_data_dict, 'new_ica')\n",
    "\n",
    "train_labels = np.reshape(np.array(list(train_labels_dict.values())), (len(train_data),1))\n",
    "test_labels = np.reshape(np.array(list(test_labels_dict.values())), (len(test_data),1))\n",
    "val_labels = np.reshape(np.array(list(val_labels_dict.values())), (len(val_data),1))\n",
    "\n",
    "print(f\"Shape of train data set: {train_data.shape}\")\n",
    "print(f\"Shape of train labels set: {train_labels.shape}\")\n",
    "\n",
    "print(f\"Shape of validation data set: {val_data.shape}\")\n",
    "print(f\"Shape of validation labels set: {val_labels.shape}\")\n",
    "\n",
    "print(f\"Shape of test data set: {test_data.shape}\")\n",
    "print(f\"Shape of test labels set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69268, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6897 - accuracy: 0.5000 - val_loss: 0.6927 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 0.69268 to 0.69198, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6907 - accuracy: 0.5682 - val_loss: 0.6920 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 0.69198 to 0.69120, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6912 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 0.69120 to 0.69084, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6836 - accuracy: 0.5682 - val_loss: 0.6908 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 0.69084 to 0.69083, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6866 - accuracy: 0.5682 - val_loss: 0.6908 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.69083\n",
      "1/1 - 7s - loss: 0.6850 - accuracy: 0.5682 - val_loss: 0.6911 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.69083\n",
      "1/1 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6915 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.69083\n",
      "1/1 - 8s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6917 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.69083\n",
      "1/1 - 7s - loss: 0.6850 - accuracy: 0.5682 - val_loss: 0.6919 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.69083\n",
      "1/1 - 6s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6919 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.69083\n",
      "1/1 - 6s - loss: 0.6858 - accuracy: 0.5682 - val_loss: 0.6917 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.69083\n",
      "1/1 - 6s - loss: 0.6829 - accuracy: 0.5682 - val_loss: 0.6912 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 0.69083 to 0.69053, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6850 - accuracy: 0.5682 - val_loss: 0.6905 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.69053 to 0.68979, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6836 - accuracy: 0.5682 - val_loss: 0.6898 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.68979 to 0.68903, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6850 - accuracy: 0.5682 - val_loss: 0.6890 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.68903 to 0.68831, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6812 - accuracy: 0.5682 - val_loss: 0.6883 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.68831 to 0.68767, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6814 - accuracy: 0.5682 - val_loss: 0.6877 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.68767 to 0.68710, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6816 - accuracy: 0.5682 - val_loss: 0.6871 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 0.68710 to 0.68668, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6832 - accuracy: 0.5682 - val_loss: 0.6867 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.68668 to 0.68641, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6830 - accuracy: 0.5682 - val_loss: 0.6864 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.68641 to 0.68622, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 10s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6862 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.68622 to 0.68613, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6822 - accuracy: 0.5682 - val_loss: 0.6861 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.68613 to 0.68610, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6845 - accuracy: 0.5682 - val_loss: 0.6861 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.68610\n",
      "1/1 - 6s - loss: 0.6824 - accuracy: 0.5682 - val_loss: 0.6862 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.68610\n",
      "1/1 - 6s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6863 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.68610\n",
      "1/1 - 6s - loss: 0.6827 - accuracy: 0.5682 - val_loss: 0.6864 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.68610\n",
      "1/1 - 7s - loss: 0.6866 - accuracy: 0.5682 - val_loss: 0.6864 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.68610\n",
      "1/1 - 6s - loss: 0.6858 - accuracy: 0.5682 - val_loss: 0.6864 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.68610\n",
      "1/1 - 6s - loss: 0.6825 - accuracy: 0.5682 - val_loss: 0.6863 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.68610\n",
      "1/1 - 6s - loss: 0.6848 - accuracy: 0.5682 - val_loss: 0.6861 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss improved from 0.68610 to 0.68599, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6856 - accuracy: 0.5682 - val_loss: 0.6860 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss improved from 0.68599 to 0.68585, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6819 - accuracy: 0.5682 - val_loss: 0.6858 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss improved from 0.68585 to 0.68567, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6861 - accuracy: 0.5682 - val_loss: 0.6857 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss improved from 0.68567 to 0.68548, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6794 - accuracy: 0.5682 - val_loss: 0.6855 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss improved from 0.68548 to 0.68538, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6854 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss improved from 0.68538 to 0.68534, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6830 - accuracy: 0.5682 - val_loss: 0.6853 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss improved from 0.68534 to 0.68534, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6843 - accuracy: 0.5682 - val_loss: 0.6853 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.68534\n",
      "1/1 - 6s - loss: 0.6847 - accuracy: 0.5682 - val_loss: 0.6854 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.68534\n",
      "1/1 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6855 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.68534\n",
      "1/1 - 6s - loss: 0.6827 - accuracy: 0.5682 - val_loss: 0.6856 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.68534\n",
      "1/1 - 7s - loss: 0.6867 - accuracy: 0.5682 - val_loss: 0.6857 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.68534\n",
      "1/1 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6858 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.68534\n",
      "1/1 - 7s - loss: 0.6809 - accuracy: 0.5682 - val_loss: 0.6858 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.68534\n",
      "1/1 - 7s - loss: 0.6855 - accuracy: 0.5682 - val_loss: 0.6858 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.68534\n",
      "1/1 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6857 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.68534\n",
      "1/1 - 6s - loss: 0.6820 - accuracy: 0.5682 - val_loss: 0.6855 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss improved from 0.68534 to 0.68529, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6853 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss improved from 0.68529 to 0.68513, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6809 - accuracy: 0.5682 - val_loss: 0.6851 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.68513 to 0.68498, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6844 - accuracy: 0.5682 - val_loss: 0.6850 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss improved from 0.68498 to 0.68484, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6866 - accuracy: 0.5682 - val_loss: 0.6848 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.68484 to 0.68473, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6890 - accuracy: 0.5682 - val_loss: 0.6847 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss improved from 0.68473 to 0.68465, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6847 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6832 - accuracy: 0.5682 - val_loss: 0.6847 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6820 - accuracy: 0.5682 - val_loss: 0.6847 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6845 - accuracy: 0.5682 - val_loss: 0.6848 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6794 - accuracy: 0.5682 - val_loss: 0.6850 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6823 - accuracy: 0.5682 - val_loss: 0.6851 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6857 - accuracy: 0.5682 - val_loss: 0.6851 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6851 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6876 - accuracy: 0.5682 - val_loss: 0.6849 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6848 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.68465\n",
      "1/1 - 6s - loss: 0.6817 - accuracy: 0.5682 - val_loss: 0.6847 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss improved from 0.68465 to 0.68455, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6845 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss improved from 0.68455 to 0.68432, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6804 - accuracy: 0.5682 - val_loss: 0.6843 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 0.68432 to 0.68417, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6842 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss improved from 0.68417 to 0.68400, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6830 - accuracy: 0.5682 - val_loss: 0.6840 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss improved from 0.68400 to 0.68379, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6854 - accuracy: 0.5682 - val_loss: 0.6838 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss improved from 0.68379 to 0.68358, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6852 - accuracy: 0.5682 - val_loss: 0.6836 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss improved from 0.68358 to 0.68350, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6832 - accuracy: 0.5682 - val_loss: 0.6835 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss improved from 0.68350 to 0.68339, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6834 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss improved from 0.68339 to 0.68332, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6783 - accuracy: 0.5682 - val_loss: 0.6833 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss improved from 0.68332 to 0.68325, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6899 - accuracy: 0.5682 - val_loss: 0.6833 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss improved from 0.68325 to 0.68317, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6832 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss improved from 0.68317 to 0.68314, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6832 - accuracy: 0.5682 - val_loss: 0.6831 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss improved from 0.68314 to 0.68310, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6827 - accuracy: 0.5682 - val_loss: 0.6831 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss improved from 0.68310 to 0.68300, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6811 - accuracy: 0.5682 - val_loss: 0.6830 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss improved from 0.68300 to 0.68281, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6809 - accuracy: 0.5682 - val_loss: 0.6828 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss improved from 0.68281 to 0.68255, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6823 - accuracy: 0.5682 - val_loss: 0.6825 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss improved from 0.68255 to 0.68229, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6816 - accuracy: 0.5682 - val_loss: 0.6823 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss improved from 0.68229 to 0.68207, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6900 - accuracy: 0.5682 - val_loss: 0.6821 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss improved from 0.68207 to 0.68175, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6818 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss improved from 0.68175 to 0.68147, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6820 - accuracy: 0.5682 - val_loss: 0.6815 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss improved from 0.68147 to 0.68126, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6853 - accuracy: 0.5682 - val_loss: 0.6813 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss improved from 0.68126 to 0.68107, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6808 - accuracy: 0.5682 - val_loss: 0.6811 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss improved from 0.68107 to 0.68096, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6818 - accuracy: 0.5682 - val_loss: 0.6810 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss improved from 0.68096 to 0.68076, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6783 - accuracy: 0.5682 - val_loss: 0.6808 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss improved from 0.68076 to 0.68056, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6833 - accuracy: 0.5682 - val_loss: 0.6806 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss improved from 0.68056 to 0.68037, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6900 - accuracy: 0.5682 - val_loss: 0.6804 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss improved from 0.68037 to 0.68022, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6802 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss improved from 0.68022 to 0.68006, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6903 - accuracy: 0.5682 - val_loss: 0.6801 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss improved from 0.68006 to 0.67989, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6799 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss improved from 0.67989 to 0.67975, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6797 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss improved from 0.67975 to 0.67950, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6867 - accuracy: 0.5682 - val_loss: 0.6795 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss improved from 0.67950 to 0.67909, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6801 - accuracy: 0.5682 - val_loss: 0.6791 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss improved from 0.67909 to 0.67863, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6788 - accuracy: 0.5682 - val_loss: 0.6786 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss improved from 0.67863 to 0.67826, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6851 - accuracy: 0.5682 - val_loss: 0.6783 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss improved from 0.67826 to 0.67798, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6867 - accuracy: 0.5682 - val_loss: 0.6780 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss improved from 0.67798 to 0.67780, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6826 - accuracy: 0.5682 - val_loss: 0.6778 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss improved from 0.67780 to 0.67757, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6780 - accuracy: 0.5682 - val_loss: 0.6776 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss improved from 0.67757 to 0.67749, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6814 - accuracy: 0.5682 - val_loss: 0.6775 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss improved from 0.67749 to 0.67741, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6774 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss improved from 0.67741 to 0.67733, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6836 - accuracy: 0.5682 - val_loss: 0.6773 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 0.67733 to 0.67720, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6807 - accuracy: 0.5682 - val_loss: 0.6772 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 0.67720 to 0.67698, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6847 - accuracy: 0.5682 - val_loss: 0.6770 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss improved from 0.67698 to 0.67669, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6897 - accuracy: 0.5682 - val_loss: 0.6767 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss improved from 0.67669 to 0.67624, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6760 - accuracy: 0.5682 - val_loss: 0.6762 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss improved from 0.67624 to 0.67587, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6759 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss improved from 0.67587 to 0.67563, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6832 - accuracy: 0.5682 - val_loss: 0.6756 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss improved from 0.67563 to 0.67552, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6856 - accuracy: 0.5682 - val_loss: 0.6755 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss improved from 0.67552 to 0.67550, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6755 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6800 - accuracy: 0.5682 - val_loss: 0.6755 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6756 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6897 - accuracy: 0.5682 - val_loss: 0.6757 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6864 - accuracy: 0.5682 - val_loss: 0.6758 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6881 - accuracy: 0.5682 - val_loss: 0.6758 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6814 - accuracy: 0.5682 - val_loss: 0.6759 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6834 - accuracy: 0.5682 - val_loss: 0.6759 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6860 - accuracy: 0.5682 - val_loss: 0.6758 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6815 - accuracy: 0.5682 - val_loss: 0.6758 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6808 - accuracy: 0.5682 - val_loss: 0.6757 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6777 - accuracy: 0.5682 - val_loss: 0.6757 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6823 - accuracy: 0.5682 - val_loss: 0.6757 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.67550\n",
      "1/1 - 6s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6756 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss improved from 0.67550 to 0.67536, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6799 - accuracy: 0.5682 - val_loss: 0.6754 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss improved from 0.67536 to 0.67495, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6815 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss improved from 0.67495 to 0.67458, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6746 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss improved from 0.67458 to 0.67439, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6813 - accuracy: 0.5682 - val_loss: 0.6744 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss improved from 0.67439 to 0.67428, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6828 - accuracy: 0.5682 - val_loss: 0.6743 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6910 - accuracy: 0.5682 - val_loss: 0.6744 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6829 - accuracy: 0.5682 - val_loss: 0.6746 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6794 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6865 - accuracy: 0.5682 - val_loss: 0.6748 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6823 - accuracy: 0.5682 - val_loss: 0.6747 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6862 - accuracy: 0.5682 - val_loss: 0.6746 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6881 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6836 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6852 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6828 - accuracy: 0.5682 - val_loss: 0.6751 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6793 - accuracy: 0.5682 - val_loss: 0.6752 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.67428\n",
      "1/1 - 6s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6747 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss improved from 0.67428 to 0.67393, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6802 - accuracy: 0.5682 - val_loss: 0.6739 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss improved from 0.67393 to 0.67362, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6862 - accuracy: 0.5682 - val_loss: 0.6736 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss improved from 0.67362 to 0.67328, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6828 - accuracy: 0.5682 - val_loss: 0.6733 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss improved from 0.67328 to 0.67297, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6801 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6805 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6787 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6774 - accuracy: 0.5682 - val_loss: 0.6731 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6907 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6760 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6849 - accuracy: 0.5682 - val_loss: 0.6731 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6859 - accuracy: 0.5682 - val_loss: 0.6733 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6736 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.7019 - accuracy: 0.5682 - val_loss: 0.6737 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6818 - accuracy: 0.5682 - val_loss: 0.6740 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6807 - accuracy: 0.5682 - val_loss: 0.6743 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6795 - accuracy: 0.5682 - val_loss: 0.6746 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6814 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.67297\n",
      "1/1 - 8s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6750 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6835 - accuracy: 0.5682 - val_loss: 0.6750 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6843 - accuracy: 0.5682 - val_loss: 0.6748 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6814 - accuracy: 0.5682 - val_loss: 0.6747 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6750 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6757 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6861 - accuracy: 0.5682 - val_loss: 0.6765 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6822 - accuracy: 0.5682 - val_loss: 0.6770 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6887 - accuracy: 0.5682 - val_loss: 0.6770 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6864 - accuracy: 0.5682 - val_loss: 0.6768 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6849 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.67297\n",
      "1/1 - 7s - loss: 0.6783 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.67297\n",
      "1/1 - 8s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6765 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.67297\n",
      "1/1 - 7s - loss: 0.6911 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.67297\n",
      "1/1 - 7s - loss: 0.6821 - accuracy: 0.5682 - val_loss: 0.6765 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6825 - accuracy: 0.5682 - val_loss: 0.6764 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6916 - accuracy: 0.5682 - val_loss: 0.6764 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6763 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6847 - accuracy: 0.5682 - val_loss: 0.6763 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6903 - accuracy: 0.5682 - val_loss: 0.6761 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6768 - accuracy: 0.5682 - val_loss: 0.6759 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6789 - accuracy: 0.5682 - val_loss: 0.6759 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6860 - accuracy: 0.5682 - val_loss: 0.6761 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.67297\n",
      "1/1 - 7s - loss: 0.6825 - accuracy: 0.5682 - val_loss: 0.6764 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6844 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6907 - accuracy: 0.5682 - val_loss: 0.6768 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.67297\n",
      "1/1 - 7s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6826 - accuracy: 0.5682 - val_loss: 0.6765 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.67297\n",
      "1/1 - 8s - loss: 0.6845 - accuracy: 0.5682 - val_loss: 0.6763 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.67297\n",
      "1/1 - 10s - loss: 0.6796 - accuracy: 0.5682 - val_loss: 0.6760 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6776 - accuracy: 0.5682 - val_loss: 0.6760 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.67297\n",
      "1/1 - 7s - loss: 0.6816 - accuracy: 0.5682 - val_loss: 0.6762 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6890 - accuracy: 0.5682 - val_loss: 0.6764 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6804 - accuracy: 0.5682 - val_loss: 0.6766 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6803 - accuracy: 0.5682 - val_loss: 0.6763 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6812 - accuracy: 0.5682 - val_loss: 0.6760 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6802 - accuracy: 0.5682 - val_loss: 0.6760 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6761 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6784 - accuracy: 0.5682 - val_loss: 0.6762 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6919 - accuracy: 0.5682 - val_loss: 0.6764 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6761 - accuracy: 0.5682 - val_loss: 0.6765 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6920 - accuracy: 0.5682 - val_loss: 0.6764 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6815 - accuracy: 0.5682 - val_loss: 0.6763 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6847 - accuracy: 0.5682 - val_loss: 0.6760 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6749 - accuracy: 0.5682 - val_loss: 0.6758 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6950 - accuracy: 0.5682 - val_loss: 0.6754 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6836 - accuracy: 0.5682 - val_loss: 0.6751 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6805 - accuracy: 0.5682 - val_loss: 0.6746 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6740 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.67297\n",
      "1/1 - 6s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6733 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss improved from 0.67297 to 0.67285, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6868 - accuracy: 0.5682 - val_loss: 0.6729 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss improved from 0.67285 to 0.67258, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6826 - accuracy: 0.5682 - val_loss: 0.6726 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.67258\n",
      "1/1 - 6s - loss: 0.6857 - accuracy: 0.5682 - val_loss: 0.6728 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.67258\n",
      "1/1 - 6s - loss: 0.6786 - accuracy: 0.5682 - val_loss: 0.6733 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.67258\n",
      "1/1 - 6s - loss: 0.6859 - accuracy: 0.5682 - val_loss: 0.6736 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.67258\n",
      "1/1 - 6s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6737 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.67258\n",
      "1/1 - 7s - loss: 0.6828 - accuracy: 0.5682 - val_loss: 0.6733 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.67258\n",
      "1/1 - 6s - loss: 0.6816 - accuracy: 0.5682 - val_loss: 0.6727 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss improved from 0.67258 to 0.67228, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6829 - accuracy: 0.5682 - val_loss: 0.6723 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss improved from 0.67228 to 0.67179, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 9s - loss: 0.6890 - accuracy: 0.5682 - val_loss: 0.6718 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss improved from 0.67179 to 0.67148, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 12s - loss: 0.6792 - accuracy: 0.5682 - val_loss: 0.6715 - val_accuracy: 0.6875 - 12s/epoch - 12s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss improved from 0.67148 to 0.67121, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 9s - loss: 0.6831 - accuracy: 0.5682 - val_loss: 0.6712 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss improved from 0.67121 to 0.67109, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 8s - loss: 0.6902 - accuracy: 0.5682 - val_loss: 0.6711 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss improved from 0.67109 to 0.67097, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 9s - loss: 0.6872 - accuracy: 0.5682 - val_loss: 0.6710 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss improved from 0.67097 to 0.67037, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 9s - loss: 0.6899 - accuracy: 0.5682 - val_loss: 0.6704 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss improved from 0.67037 to 0.66988, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6863 - accuracy: 0.5682 - val_loss: 0.6699 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss improved from 0.66988 to 0.66967, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 7s - loss: 0.6921 - accuracy: 0.5682 - val_loss: 0.6697 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss improved from 0.66967 to 0.66961, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 8s - loss: 0.6829 - accuracy: 0.5682 - val_loss: 0.6696 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6864 - accuracy: 0.5682 - val_loss: 0.6700 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6705 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6821 - accuracy: 0.5682 - val_loss: 0.6710 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6814 - accuracy: 0.5682 - val_loss: 0.6715 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.66961\n",
      "1/1 - 9s - loss: 0.6875 - accuracy: 0.5682 - val_loss: 0.6719 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.66961\n",
      "1/1 - 9s - loss: 0.6821 - accuracy: 0.5682 - val_loss: 0.6721 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.66961\n",
      "1/1 - 9s - loss: 0.6799 - accuracy: 0.5682 - val_loss: 0.6722 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.66961\n",
      "1/1 - 8s - loss: 0.6818 - accuracy: 0.5682 - val_loss: 0.6722 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6927 - accuracy: 0.5682 - val_loss: 0.6720 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6791 - accuracy: 0.5682 - val_loss: 0.6717 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6787 - accuracy: 0.5682 - val_loss: 0.6717 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6773 - accuracy: 0.5682 - val_loss: 0.6716 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6716 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.66961\n",
      "1/1 - 8s - loss: 0.6868 - accuracy: 0.5682 - val_loss: 0.6716 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.66961\n",
      "1/1 - 8s - loss: 0.6783 - accuracy: 0.5682 - val_loss: 0.6718 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.66961\n",
      "1/1 - 8s - loss: 0.6875 - accuracy: 0.5682 - val_loss: 0.6721 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6818 - accuracy: 0.5682 - val_loss: 0.6723 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6724 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6725 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6836 - accuracy: 0.5682 - val_loss: 0.6725 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6835 - accuracy: 0.5682 - val_loss: 0.6725 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6928 - accuracy: 0.5682 - val_loss: 0.6723 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6867 - accuracy: 0.5682 - val_loss: 0.6720 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6813 - accuracy: 0.5682 - val_loss: 0.6717 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6833 - accuracy: 0.5682 - val_loss: 0.6713 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6897 - accuracy: 0.5682 - val_loss: 0.6710 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6794 - accuracy: 0.5682 - val_loss: 0.6711 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6824 - accuracy: 0.5682 - val_loss: 0.6712 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.66961\n",
      "1/1 - 7s - loss: 0.6815 - accuracy: 0.5682 - val_loss: 0.6715 - val_accuracy: 0.6875 - 7s/epoch - 7s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6821 - accuracy: 0.5682 - val_loss: 0.6718 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6768 - accuracy: 0.5682 - val_loss: 0.6720 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6893 - accuracy: 0.5682 - val_loss: 0.6722 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6828 - accuracy: 0.5682 - val_loss: 0.6722 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6829 - accuracy: 0.5682 - val_loss: 0.6723 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6817 - accuracy: 0.5682 - val_loss: 0.6722 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6723 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6809 - accuracy: 0.5682 - val_loss: 0.6721 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6751 - accuracy: 0.5682 - val_loss: 0.6718 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6789 - accuracy: 0.5682 - val_loss: 0.6714 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6848 - accuracy: 0.5682 - val_loss: 0.6708 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6858 - accuracy: 0.5682 - val_loss: 0.6703 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6775 - accuracy: 0.5682 - val_loss: 0.6699 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.66961\n",
      "1/1 - 6s - loss: 0.6834 - accuracy: 0.5682 - val_loss: 0.6696 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss improved from 0.66961 to 0.66949, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6848 - accuracy: 0.5682 - val_loss: 0.6695 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss improved from 0.66949 to 0.66903, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6820 - accuracy: 0.5682 - val_loss: 0.6690 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss improved from 0.66903 to 0.66834, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6830 - accuracy: 0.5682 - val_loss: 0.6683 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss improved from 0.66834 to 0.66749, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6827 - accuracy: 0.5682 - val_loss: 0.6675 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss improved from 0.66749 to 0.66694, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6971 - accuracy: 0.5682 - val_loss: 0.6669 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss improved from 0.66694 to 0.66663, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6798 - accuracy: 0.5682 - val_loss: 0.6666 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss improved from 0.66663 to 0.66656, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6807 - accuracy: 0.5682 - val_loss: 0.6666 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss improved from 0.66656 to 0.66625, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6799 - accuracy: 0.5682 - val_loss: 0.6663 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss improved from 0.66625 to 0.66564, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6923 - accuracy: 0.5682 - val_loss: 0.6656 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss improved from 0.66564 to 0.66489, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6733 - accuracy: 0.5682 - val_loss: 0.6649 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss improved from 0.66489 to 0.66399, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6893 - accuracy: 0.5682 - val_loss: 0.6640 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss improved from 0.66399 to 0.66329, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6853 - accuracy: 0.5682 - val_loss: 0.6633 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss improved from 0.66329 to 0.66278, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6836 - accuracy: 0.5682 - val_loss: 0.6628 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.66278\n",
      "1/1 - 6s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6628 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.66278\n",
      "1/1 - 6s - loss: 0.6777 - accuracy: 0.5682 - val_loss: 0.6631 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.66278\n",
      "1/1 - 6s - loss: 0.6936 - accuracy: 0.5682 - val_loss: 0.6632 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.66278\n",
      "1/1 - 6s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6632 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.66278\n",
      "1/1 - 6s - loss: 0.6800 - accuracy: 0.5682 - val_loss: 0.6634 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.66278\n",
      "1/1 - 6s - loss: 0.6818 - accuracy: 0.5682 - val_loss: 0.6634 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss improved from 0.66278 to 0.66266, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6627 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss improved from 0.66266 to 0.66207, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6621 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss improved from 0.66207 to 0.66164, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6848 - accuracy: 0.5682 - val_loss: 0.6616 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss improved from 0.66164 to 0.66100, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6831 - accuracy: 0.5682 - val_loss: 0.6610 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss improved from 0.66100 to 0.66058, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6820 - accuracy: 0.5682 - val_loss: 0.6606 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss improved from 0.66058 to 0.66020, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 6s - loss: 0.6803 - accuracy: 0.5682 - val_loss: 0.6602 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.66020\n",
      "1/1 - 6s - loss: 0.6980 - accuracy: 0.5682 - val_loss: 0.6603 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss improved from 0.66020 to 0.66008, saving model to /tmp\\checkpoint.h5\n",
      "1/1 - 8s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6601 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.66008\n",
      "1/1 - 6s - loss: 0.6770 - accuracy: 0.5682 - val_loss: 0.6602 - val_accuracy: 0.6875 - 6s/epoch - 6s/step\n",
      "1/1 [==============================] - 1s 514ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    }
   ],
   "source": [
    "probs_EEGNet = EEGNet_classification('new_ica',train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[11  0]\n",
      " [ 8  0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5254859  0.47451416]\n",
      " [0.5254859  0.47451416]\n",
      " [0.5254859  0.47451416]\n",
      " [0.5254859  0.47451416]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254858  0.4745142 ]\n",
      " [0.5254859  0.47451413]\n",
      " [0.5254859  0.47451413]\n",
      " [0.5254859  0.47451413]\n",
      " [0.5254859  0.47451413]\n",
      " [0.5254859  0.47451416]\n",
      " [0.5254859  0.47451413]]\n"
     ]
    }
   ],
   "source": [
    "print(probs_EEGNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.78027, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 0.7943 - accuracy: 0.5227 - val_loss: 0.7803 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.78027\n",
      "2/2 - 10s - loss: 1.0050 - accuracy: 0.3409 - val_loss: 0.7808 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 0.78027 to 0.77966, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.9615 - accuracy: 0.4318 - val_loss: 0.7797 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 0.77966 to 0.77930, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7888 - accuracy: 0.5682 - val_loss: 0.7793 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 0.77930 to 0.77905, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7841 - accuracy: 0.5682 - val_loss: 0.7790 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 0.77905 to 0.77874, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.8036 - accuracy: 0.3864 - val_loss: 0.7787 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 0.77874 to 0.77857, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7979 - accuracy: 0.5909 - val_loss: 0.7786 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 0.77857 to 0.77825, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7904 - accuracy: 0.5909 - val_loss: 0.7783 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 0.77825 to 0.77775, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7840 - accuracy: 0.5682 - val_loss: 0.7777 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 0.77775 to 0.77724, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7851 - accuracy: 0.5682 - val_loss: 0.7772 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 0.77724 to 0.77675, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7810 - accuracy: 0.5682 - val_loss: 0.7767 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 0.77675 to 0.77584, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7907 - accuracy: 0.5455 - val_loss: 0.7758 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 0.77584 to 0.77515, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7775 - accuracy: 0.5455 - val_loss: 0.7751 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.77515 to 0.77474, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7788 - accuracy: 0.5682 - val_loss: 0.7747 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.77474 to 0.77433, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7883 - accuracy: 0.5682 - val_loss: 0.7743 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.77433 to 0.77346, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7672 - accuracy: 0.5682 - val_loss: 0.7735 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.77346 to 0.77290, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7694 - accuracy: 0.5682 - val_loss: 0.7729 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.77290\n",
      "2/2 - 11s - loss: 0.7779 - accuracy: 0.5682 - val_loss: 0.7733 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.77290\n",
      "2/2 - 10s - loss: 0.7728 - accuracy: 0.5682 - val_loss: 0.7738 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.77290 to 0.77125, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7782 - accuracy: 0.5682 - val_loss: 0.7712 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.77125 to 0.76808, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7729 - accuracy: 0.5682 - val_loss: 0.7681 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.76808 to 0.76669, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7814 - accuracy: 0.5682 - val_loss: 0.7667 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.76669 to 0.76498, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7744 - accuracy: 0.5682 - val_loss: 0.7650 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss improved from 0.76498 to 0.76491, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7800 - accuracy: 0.5682 - val_loss: 0.7649 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.76491\n",
      "2/2 - 11s - loss: 0.7850 - accuracy: 0.5682 - val_loss: 0.7659 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.76491\n",
      "2/2 - 11s - loss: 0.7718 - accuracy: 0.5682 - val_loss: 0.7667 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.76491\n",
      "2/2 - 10s - loss: 0.7827 - accuracy: 0.5682 - val_loss: 0.7668 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.76491\n",
      "2/2 - 10s - loss: 0.7695 - accuracy: 0.5682 - val_loss: 0.7667 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.76491\n",
      "2/2 - 11s - loss: 0.7660 - accuracy: 0.5682 - val_loss: 0.7669 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.76491\n",
      "2/2 - 11s - loss: 0.7749 - accuracy: 0.5682 - val_loss: 0.7668 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.76491\n",
      "2/2 - 11s - loss: 0.7666 - accuracy: 0.5682 - val_loss: 0.7677 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.76491\n",
      "2/2 - 11s - loss: 0.7733 - accuracy: 0.5682 - val_loss: 0.7673 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.76491\n",
      "2/2 - 11s - loss: 0.7654 - accuracy: 0.5682 - val_loss: 0.7659 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss improved from 0.76491 to 0.76471, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7708 - accuracy: 0.5682 - val_loss: 0.7647 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss improved from 0.76471 to 0.76240, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7662 - accuracy: 0.5682 - val_loss: 0.7624 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss improved from 0.76240 to 0.76069, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7643 - accuracy: 0.5682 - val_loss: 0.7607 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss improved from 0.76069 to 0.76035, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.7798 - accuracy: 0.5682 - val_loss: 0.7603 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7711 - accuracy: 0.5682 - val_loss: 0.7622 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7637 - accuracy: 0.5682 - val_loss: 0.7641 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7665 - accuracy: 0.5682 - val_loss: 0.7656 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7689 - accuracy: 0.5455 - val_loss: 0.7648 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7721 - accuracy: 0.5227 - val_loss: 0.7630 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7708 - accuracy: 0.5682 - val_loss: 0.7620 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7668 - accuracy: 0.5682 - val_loss: 0.7621 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7669 - accuracy: 0.5682 - val_loss: 0.7624 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7637 - accuracy: 0.5682 - val_loss: 0.7614 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.76035\n",
      "2/2 - 11s - loss: 0.7608 - accuracy: 0.5682 - val_loss: 0.7609 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss improved from 0.76035 to 0.75990, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7662 - accuracy: 0.5682 - val_loss: 0.7599 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.75990 to 0.75864, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7667 - accuracy: 0.5682 - val_loss: 0.7586 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.75864\n",
      "2/2 - 10s - loss: 0.7630 - accuracy: 0.5682 - val_loss: 0.7594 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.75864\n",
      "2/2 - 11s - loss: 0.7674 - accuracy: 0.5682 - val_loss: 0.7603 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.75864\n",
      "2/2 - 11s - loss: 0.7643 - accuracy: 0.5682 - val_loss: 0.7596 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.75864\n",
      "2/2 - 10s - loss: 0.7556 - accuracy: 0.5682 - val_loss: 0.7587 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss improved from 0.75864 to 0.75771, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7583 - accuracy: 0.5682 - val_loss: 0.7577 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss improved from 0.75771 to 0.75698, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7721 - accuracy: 0.5682 - val_loss: 0.7570 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss improved from 0.75698 to 0.75645, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7626 - accuracy: 0.5682 - val_loss: 0.7565 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss improved from 0.75645 to 0.75617, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7650 - accuracy: 0.5682 - val_loss: 0.7562 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.75617\n",
      "2/2 - 11s - loss: 0.7577 - accuracy: 0.5682 - val_loss: 0.7568 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.75617\n",
      "2/2 - 10s - loss: 0.7570 - accuracy: 0.5682 - val_loss: 0.7569 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss improved from 0.75617 to 0.75598, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7694 - accuracy: 0.4318 - val_loss: 0.7560 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss improved from 0.75598 to 0.75530, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7700 - accuracy: 0.5000 - val_loss: 0.7553 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss improved from 0.75530 to 0.75486, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7562 - accuracy: 0.5909 - val_loss: 0.7549 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss improved from 0.75486 to 0.75462, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7528 - accuracy: 0.5682 - val_loss: 0.7546 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss improved from 0.75462 to 0.75423, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7570 - accuracy: 0.5682 - val_loss: 0.7542 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 0.75423 to 0.75333, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7652 - accuracy: 0.5682 - val_loss: 0.7533 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss improved from 0.75333 to 0.75262, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7656 - accuracy: 0.5682 - val_loss: 0.7526 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss improved from 0.75262 to 0.75252, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7544 - accuracy: 0.5682 - val_loss: 0.7525 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss improved from 0.75252 to 0.75116, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7503 - accuracy: 0.5682 - val_loss: 0.7512 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss improved from 0.75116 to 0.74938, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7518 - accuracy: 0.5682 - val_loss: 0.7494 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss improved from 0.74938 to 0.74827, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7498 - accuracy: 0.5682 - val_loss: 0.7483 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss improved from 0.74827 to 0.74756, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7568 - accuracy: 0.5682 - val_loss: 0.7476 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss improved from 0.74756 to 0.74735, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7542 - accuracy: 0.5682 - val_loss: 0.7474 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.74735\n",
      "2/2 - 11s - loss: 0.7499 - accuracy: 0.5682 - val_loss: 0.7485 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.74735\n",
      "2/2 - 11s - loss: 0.7560 - accuracy: 0.5682 - val_loss: 0.7491 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.74735\n",
      "2/2 - 11s - loss: 0.7545 - accuracy: 0.5682 - val_loss: 0.7483 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss improved from 0.74735 to 0.74662, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7517 - accuracy: 0.5682 - val_loss: 0.7466 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss improved from 0.74662 to 0.74542, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7444 - accuracy: 0.5682 - val_loss: 0.7454 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.74542\n",
      "2/2 - 13s - loss: 0.7555 - accuracy: 0.5682 - val_loss: 0.7455 - val_accuracy: 0.6875 - 13s/epoch - 6s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.74542\n",
      "2/2 - 11s - loss: 0.7502 - accuracy: 0.5682 - val_loss: 0.7467 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.74542\n",
      "2/2 - 11s - loss: 0.7486 - accuracy: 0.5682 - val_loss: 0.7470 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.74542\n",
      "2/2 - 11s - loss: 0.7486 - accuracy: 0.5682 - val_loss: 0.7465 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss improved from 0.74542 to 0.74522, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7505 - accuracy: 0.5682 - val_loss: 0.7452 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss improved from 0.74522 to 0.74450, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7482 - accuracy: 0.5682 - val_loss: 0.7445 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.74450\n",
      "2/2 - 11s - loss: 0.7470 - accuracy: 0.5682 - val_loss: 0.7448 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.74450\n",
      "2/2 - 10s - loss: 0.7448 - accuracy: 0.5682 - val_loss: 0.7451 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.74450\n",
      "2/2 - 11s - loss: 0.7501 - accuracy: 0.5682 - val_loss: 0.7451 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss improved from 0.74450 to 0.74430, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7589 - accuracy: 0.4318 - val_loss: 0.7443 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss improved from 0.74430 to 0.74323, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7509 - accuracy: 0.5455 - val_loss: 0.7432 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss improved from 0.74323 to 0.74218, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7520 - accuracy: 0.5682 - val_loss: 0.7422 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss improved from 0.74218 to 0.74206, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7463 - accuracy: 0.5682 - val_loss: 0.7421 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss improved from 0.74206 to 0.74201, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7469 - accuracy: 0.5682 - val_loss: 0.7420 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss improved from 0.74201 to 0.74109, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7433 - accuracy: 0.5682 - val_loss: 0.7411 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss improved from 0.74109 to 0.73974, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7459 - accuracy: 0.5682 - val_loss: 0.7397 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss improved from 0.73974 to 0.73973, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7486 - accuracy: 0.5682 - val_loss: 0.7397 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.73973\n",
      "2/2 - 11s - loss: 0.7392 - accuracy: 0.5682 - val_loss: 0.7407 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.73973\n",
      "2/2 - 11s - loss: 0.7452 - accuracy: 0.5682 - val_loss: 0.7409 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.73973\n",
      "2/2 - 11s - loss: 0.7422 - accuracy: 0.5682 - val_loss: 0.7399 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss improved from 0.73973 to 0.73827, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7465 - accuracy: 0.5682 - val_loss: 0.7383 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss improved from 0.73827 to 0.73783, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7485 - accuracy: 0.5682 - val_loss: 0.7378 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.73783\n",
      "2/2 - 10s - loss: 0.7447 - accuracy: 0.5682 - val_loss: 0.7383 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.73783\n",
      "2/2 - 11s - loss: 0.7447 - accuracy: 0.5682 - val_loss: 0.7385 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.73783\n",
      "2/2 - 11s - loss: 0.7455 - accuracy: 0.5682 - val_loss: 0.7378 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 0.73783 to 0.73577, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7377 - accuracy: 0.5682 - val_loss: 0.7358 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 0.73577 to 0.73472, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7380 - accuracy: 0.5682 - val_loss: 0.7347 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss improved from 0.73472 to 0.73369, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7378 - accuracy: 0.5682 - val_loss: 0.7337 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss improved from 0.73369 to 0.73311, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7355 - accuracy: 0.5682 - val_loss: 0.7331 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss improved from 0.73311 to 0.73290, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7365 - accuracy: 0.5682 - val_loss: 0.7329 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.73290\n",
      "2/2 - 12s - loss: 0.7340 - accuracy: 0.5682 - val_loss: 0.7340 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.73290\n",
      "2/2 - 11s - loss: 0.7361 - accuracy: 0.5682 - val_loss: 0.7356 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.73290\n",
      "2/2 - 11s - loss: 0.7405 - accuracy: 0.5682 - val_loss: 0.7351 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.73290\n",
      "2/2 - 11s - loss: 0.7388 - accuracy: 0.5682 - val_loss: 0.7339 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.73290\n",
      "2/2 - 11s - loss: 0.7351 - accuracy: 0.5682 - val_loss: 0.7331 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.73290\n",
      "2/2 - 10s - loss: 0.7374 - accuracy: 0.5682 - val_loss: 0.7330 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.73290\n",
      "2/2 - 11s - loss: 0.7321 - accuracy: 0.5682 - val_loss: 0.7333 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.73290\n",
      "2/2 - 11s - loss: 0.7397 - accuracy: 0.5682 - val_loss: 0.7335 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.73290\n",
      "2/2 - 10s - loss: 0.7389 - accuracy: 0.5682 - val_loss: 0.7336 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.73290\n",
      "2/2 - 11s - loss: 0.7326 - accuracy: 0.5682 - val_loss: 0.7330 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss improved from 0.73290 to 0.73168, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7307 - accuracy: 0.5682 - val_loss: 0.7317 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss improved from 0.73168 to 0.73045, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7348 - accuracy: 0.5682 - val_loss: 0.7304 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss improved from 0.73045 to 0.72974, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7277 - accuracy: 0.5682 - val_loss: 0.7297 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss improved from 0.72974 to 0.72906, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7321 - accuracy: 0.5682 - val_loss: 0.7291 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss improved from 0.72906 to 0.72832, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7301 - accuracy: 0.5682 - val_loss: 0.7283 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss improved from 0.72832 to 0.72679, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7268 - accuracy: 0.5682 - val_loss: 0.7268 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss improved from 0.72679 to 0.72448, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7274 - accuracy: 0.5682 - val_loss: 0.7245 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss improved from 0.72448 to 0.72447, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7373 - accuracy: 0.5682 - val_loss: 0.7245 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss improved from 0.72447 to 0.72206, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7359 - accuracy: 0.5682 - val_loss: 0.7221 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss improved from 0.72206 to 0.71686, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7306 - accuracy: 0.5682 - val_loss: 0.7169 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss improved from 0.71686 to 0.71430, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7316 - accuracy: 0.5682 - val_loss: 0.7143 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss improved from 0.71430 to 0.71389, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7303 - accuracy: 0.5682 - val_loss: 0.7139 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.71389\n",
      "2/2 - 11s - loss: 0.7269 - accuracy: 0.5682 - val_loss: 0.7161 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.71389\n",
      "2/2 - 11s - loss: 0.7304 - accuracy: 0.5682 - val_loss: 0.7153 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss improved from 0.71389 to 0.70964, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7271 - accuracy: 0.5682 - val_loss: 0.7096 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss improved from 0.70964 to 0.70168, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7248 - accuracy: 0.5682 - val_loss: 0.7017 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss improved from 0.70168 to 0.69622, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7326 - accuracy: 0.5682 - val_loss: 0.6962 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7383 - accuracy: 0.5682 - val_loss: 0.7022 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7410 - accuracy: 0.5682 - val_loss: 0.7155 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7346 - accuracy: 0.5682 - val_loss: 0.7200 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7272 - accuracy: 0.5682 - val_loss: 0.7185 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7273 - accuracy: 0.5682 - val_loss: 0.7192 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7297 - accuracy: 0.5682 - val_loss: 0.7191 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7294 - accuracy: 0.5682 - val_loss: 0.7184 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7275 - accuracy: 0.5682 - val_loss: 0.7188 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7265 - accuracy: 0.5682 - val_loss: 0.7170 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7257 - accuracy: 0.5682 - val_loss: 0.7162 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7262 - accuracy: 0.5682 - val_loss: 0.7147 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7226 - accuracy: 0.5682 - val_loss: 0.7119 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7259 - accuracy: 0.5682 - val_loss: 0.7123 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7229 - accuracy: 0.5682 - val_loss: 0.7153 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7245 - accuracy: 0.5682 - val_loss: 0.7154 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7273 - accuracy: 0.5682 - val_loss: 0.7145 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7211 - accuracy: 0.5682 - val_loss: 0.7131 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7190 - accuracy: 0.5682 - val_loss: 0.7125 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7207 - accuracy: 0.5682 - val_loss: 0.7120 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7227 - accuracy: 0.5682 - val_loss: 0.7136 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7200 - accuracy: 0.5682 - val_loss: 0.7155 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7216 - accuracy: 0.5682 - val_loss: 0.7159 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7212 - accuracy: 0.5682 - val_loss: 0.7164 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7207 - accuracy: 0.5682 - val_loss: 0.7168 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7192 - accuracy: 0.5682 - val_loss: 0.7160 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7216 - accuracy: 0.5682 - val_loss: 0.7158 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7171 - accuracy: 0.5682 - val_loss: 0.7156 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7178 - accuracy: 0.5682 - val_loss: 0.7152 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.69622\n",
      "2/2 - 13s - loss: 0.7189 - accuracy: 0.5682 - val_loss: 0.7144 - val_accuracy: 0.6875 - 13s/epoch - 6s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7178 - accuracy: 0.5682 - val_loss: 0.7133 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7185 - accuracy: 0.5682 - val_loss: 0.7122 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7188 - accuracy: 0.5682 - val_loss: 0.7105 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7185 - accuracy: 0.5682 - val_loss: 0.7086 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7175 - accuracy: 0.5682 - val_loss: 0.7072 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7138 - accuracy: 0.5682 - val_loss: 0.7064 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7159 - accuracy: 0.5682 - val_loss: 0.7061 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7172 - accuracy: 0.5682 - val_loss: 0.7039 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7174 - accuracy: 0.5682 - val_loss: 0.7033 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7170 - accuracy: 0.5682 - val_loss: 0.7065 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7131 - accuracy: 0.5682 - val_loss: 0.7055 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7140 - accuracy: 0.5682 - val_loss: 0.7056 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7139 - accuracy: 0.5682 - val_loss: 0.7078 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7135 - accuracy: 0.5682 - val_loss: 0.7090 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7124 - accuracy: 0.5682 - val_loss: 0.7099 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7160 - accuracy: 0.5682 - val_loss: 0.7104 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7125 - accuracy: 0.5682 - val_loss: 0.7101 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7144 - accuracy: 0.5682 - val_loss: 0.7103 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7137 - accuracy: 0.5682 - val_loss: 0.7129 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7170 - accuracy: 0.5682 - val_loss: 0.7163 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.69622\n",
      "2/2 - 8s - loss: 0.7120 - accuracy: 0.5682 - val_loss: 0.7150 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7107 - accuracy: 0.5682 - val_loss: 0.7132 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7129 - accuracy: 0.5682 - val_loss: 0.7113 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7152 - accuracy: 0.5682 - val_loss: 0.7108 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.69622\n",
      "2/2 - 9s - loss: 0.7107 - accuracy: 0.5682 - val_loss: 0.7140 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7101 - accuracy: 0.5682 - val_loss: 0.7142 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7105 - accuracy: 0.5682 - val_loss: 0.7116 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7110 - accuracy: 0.5682 - val_loss: 0.7098 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7092 - accuracy: 0.5682 - val_loss: 0.7078 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7116 - accuracy: 0.5682 - val_loss: 0.7073 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7113 - accuracy: 0.5682 - val_loss: 0.7066 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7093 - accuracy: 0.5682 - val_loss: 0.7062 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7103 - accuracy: 0.5682 - val_loss: 0.7047 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7100 - accuracy: 0.5682 - val_loss: 0.7028 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7077 - accuracy: 0.5682 - val_loss: 0.7007 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7075 - accuracy: 0.5682 - val_loss: 0.6997 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7063 - accuracy: 0.5682 - val_loss: 0.6991 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7089 - accuracy: 0.5682 - val_loss: 0.6989 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7085 - accuracy: 0.5682 - val_loss: 0.6997 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7075 - accuracy: 0.5682 - val_loss: 0.7005 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7072 - accuracy: 0.5682 - val_loss: 0.7013 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7077 - accuracy: 0.5682 - val_loss: 0.7018 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7063 - accuracy: 0.5682 - val_loss: 0.7027 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7054 - accuracy: 0.5682 - val_loss: 0.7043 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7046 - accuracy: 0.5682 - val_loss: 0.7052 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7056 - accuracy: 0.5682 - val_loss: 0.7064 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7059 - accuracy: 0.5682 - val_loss: 0.7065 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.69622\n",
      "2/2 - 11s - loss: 0.7065 - accuracy: 0.5682 - val_loss: 0.7025 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.69622\n",
      "2/2 - 10s - loss: 0.7056 - accuracy: 0.5682 - val_loss: 0.6986 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss improved from 0.69622 to 0.69575, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7049 - accuracy: 0.5682 - val_loss: 0.6958 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss improved from 0.69575 to 0.69497, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7048 - accuracy: 0.5682 - val_loss: 0.6950 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.69497\n",
      "2/2 - 11s - loss: 0.7022 - accuracy: 0.5682 - val_loss: 0.6960 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.69497\n",
      "2/2 - 11s - loss: 0.6995 - accuracy: 0.5682 - val_loss: 0.6988 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.69497\n",
      "2/2 - 12s - loss: 0.7041 - accuracy: 0.5682 - val_loss: 0.7005 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.69497\n",
      "2/2 - 12s - loss: 0.7035 - accuracy: 0.5682 - val_loss: 0.7002 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.69497\n",
      "2/2 - 12s - loss: 0.7049 - accuracy: 0.5682 - val_loss: 0.6989 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.69497\n",
      "2/2 - 12s - loss: 0.7037 - accuracy: 0.5682 - val_loss: 0.6973 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.69497\n",
      "2/2 - 12s - loss: 0.7030 - accuracy: 0.5682 - val_loss: 0.6965 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.69497\n",
      "2/2 - 12s - loss: 0.7016 - accuracy: 0.5682 - val_loss: 0.6965 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.69497\n",
      "2/2 - 12s - loss: 0.7024 - accuracy: 0.5682 - val_loss: 0.6979 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.69497\n",
      "2/2 - 11s - loss: 0.7000 - accuracy: 0.5682 - val_loss: 0.6992 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.69497\n",
      "2/2 - 11s - loss: 0.7006 - accuracy: 0.5682 - val_loss: 0.7001 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.69497\n",
      "2/2 - 11s - loss: 0.6993 - accuracy: 0.5682 - val_loss: 0.7020 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.6993 - accuracy: 0.5682 - val_loss: 0.7042 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.7009 - accuracy: 0.5682 - val_loss: 0.7044 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.7010 - accuracy: 0.5682 - val_loss: 0.7042 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.6991 - accuracy: 0.5682 - val_loss: 0.7052 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.7003 - accuracy: 0.5682 - val_loss: 0.7063 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.7027 - accuracy: 0.5682 - val_loss: 0.7078 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.6998 - accuracy: 0.5682 - val_loss: 0.7083 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.6986 - accuracy: 0.5682 - val_loss: 0.7071 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.6997 - accuracy: 0.5682 - val_loss: 0.7050 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.7011 - accuracy: 0.5682 - val_loss: 0.7033 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.69497\n",
      "2/2 - 10s - loss: 0.7005 - accuracy: 0.5682 - val_loss: 0.6995 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss improved from 0.69497 to 0.69268, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.6995 - accuracy: 0.5682 - val_loss: 0.6927 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss improved from 0.69268 to 0.69033, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.6986 - accuracy: 0.5682 - val_loss: 0.6903 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss improved from 0.69033 to 0.68964, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.6981 - accuracy: 0.5682 - val_loss: 0.6896 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6976 - accuracy: 0.5682 - val_loss: 0.6907 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6986 - accuracy: 0.5682 - val_loss: 0.6919 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6981 - accuracy: 0.5682 - val_loss: 0.6931 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6989 - accuracy: 0.5682 - val_loss: 0.6939 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6981 - accuracy: 0.5682 - val_loss: 0.6937 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6976 - accuracy: 0.5682 - val_loss: 0.6930 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6981 - accuracy: 0.5682 - val_loss: 0.6927 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.68964\n",
      "2/2 - 10s - loss: 0.6988 - accuracy: 0.5682 - val_loss: 0.6917 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss improved from 0.68964 to 0.68938, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.6983 - accuracy: 0.5682 - val_loss: 0.6894 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss improved from 0.68938 to 0.68468, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.6971 - accuracy: 0.5682 - val_loss: 0.6847 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss improved from 0.68468 to 0.68075, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6978 - accuracy: 0.5682 - val_loss: 0.6808 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss improved from 0.68075 to 0.67944, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6975 - accuracy: 0.5682 - val_loss: 0.6794 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.67944\n",
      "2/2 - 11s - loss: 0.6948 - accuracy: 0.5682 - val_loss: 0.6806 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.67944\n",
      "2/2 - 11s - loss: 0.6948 - accuracy: 0.5682 - val_loss: 0.6814 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.67944\n",
      "2/2 - 11s - loss: 0.6964 - accuracy: 0.5682 - val_loss: 0.6796 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss improved from 0.67944 to 0.67670, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.6947 - accuracy: 0.5682 - val_loss: 0.6767 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss improved from 0.67670 to 0.67195, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.6933 - accuracy: 0.5682 - val_loss: 0.6720 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss improved from 0.67195 to 0.66527, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.6950 - accuracy: 0.5682 - val_loss: 0.6653 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss improved from 0.66527 to 0.65813, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.6961 - accuracy: 0.5682 - val_loss: 0.6581 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss improved from 0.65813 to 0.65269, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6989 - accuracy: 0.5682 - val_loss: 0.6527 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6950 - accuracy: 0.5682 - val_loss: 0.6575 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6960 - accuracy: 0.5682 - val_loss: 0.6681 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6940 - accuracy: 0.5682 - val_loss: 0.6710 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6942 - accuracy: 0.5682 - val_loss: 0.6724 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6940 - accuracy: 0.5682 - val_loss: 0.6782 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6958 - accuracy: 0.5682 - val_loss: 0.6819 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6930 - accuracy: 0.5682 - val_loss: 0.6789 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6939 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6936 - accuracy: 0.5682 - val_loss: 0.6699 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6938 - accuracy: 0.5682 - val_loss: 0.6690 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6934 - accuracy: 0.5682 - val_loss: 0.6715 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6958 - accuracy: 0.5682 - val_loss: 0.6718 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.65269\n",
      "2/2 - 12s - loss: 0.6937 - accuracy: 0.5682 - val_loss: 0.6716 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6953 - accuracy: 0.5682 - val_loss: 0.6727 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6924 - accuracy: 0.5682 - val_loss: 0.6753 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6941 - accuracy: 0.5682 - val_loss: 0.6775 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6947 - accuracy: 0.5682 - val_loss: 0.6788 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6937 - accuracy: 0.5682 - val_loss: 0.6792 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6923 - accuracy: 0.5682 - val_loss: 0.6790 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6914 - accuracy: 0.5682 - val_loss: 0.6778 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6910 - accuracy: 0.5682 - val_loss: 0.6765 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6915 - accuracy: 0.5682 - val_loss: 0.6754 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6934 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6944 - accuracy: 0.5682 - val_loss: 0.6731 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6928 - accuracy: 0.5682 - val_loss: 0.6759 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6920 - accuracy: 0.5682 - val_loss: 0.6776 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6909 - accuracy: 0.5682 - val_loss: 0.6757 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6908 - accuracy: 0.5682 - val_loss: 0.6736 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6901 - accuracy: 0.5682 - val_loss: 0.6728 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6894 - accuracy: 0.5682 - val_loss: 0.6723 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6903 - accuracy: 0.5682 - val_loss: 0.6700 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6894 - accuracy: 0.5682 - val_loss: 0.6677 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6900 - accuracy: 0.5682 - val_loss: 0.6657 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.65269\n",
      "2/2 - 10s - loss: 0.6894 - accuracy: 0.5682 - val_loss: 0.6646 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6627 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6631 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6909 - accuracy: 0.5682 - val_loss: 0.6648 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6641 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.65269\n",
      "2/2 - 11s - loss: 0.6914 - accuracy: 0.5682 - val_loss: 0.6635 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.65269\n",
      "2/2 - 12s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6647 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "1/1 [==============================] - 1s 647ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification('new_ica',train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54265636 0.45734364]\n",
      " [0.542629   0.45737097]\n",
      " [0.54260236 0.45739764]\n",
      " [0.54257625 0.45742375]\n",
      " [0.54198366 0.45801637]\n",
      " [0.5419707  0.4580293 ]\n",
      " [0.5423253  0.45767462]\n",
      " [0.54230547 0.45769456]\n",
      " [0.5422859  0.45771405]\n",
      " [0.54240954 0.45759046]\n",
      " [0.5423878  0.45761222]\n",
      " [0.5423665  0.4576335 ]\n",
      " [0.5423457  0.4576543 ]\n",
      " [0.5430049  0.45699513]\n",
      " [0.5429343  0.45706573]\n",
      " [0.54290026 0.4570997 ]\n",
      " [0.54283476 0.4571652 ]\n",
      " [0.5428032  0.45719677]\n",
      " [0.5427724  0.4572276 ]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[11  0]\n",
      " [ 8  0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.65343, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 14.2775 - accuracy: 0.4318 - val_loss: 0.6534 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.65343\n",
      "2/2 - 11s - loss: 46.1517 - accuracy: 0.5682 - val_loss: 1.8009 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 15.4540 - accuracy: 0.4318 - val_loss: 3.5466 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.65343\n",
      "2/2 - 11s - loss: 13.2943 - accuracy: 0.6136 - val_loss: 10.7280 - val_accuracy: 0.3125 - 11s/epoch - 6s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 25.5426 - accuracy: 0.3864 - val_loss: 4.5104 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 37.3861 - accuracy: 0.5682 - val_loss: 6.4450 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 18.7279 - accuracy: 0.5682 - val_loss: 26.3280 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 33.3929 - accuracy: 0.4318 - val_loss: 25.7879 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 18.5390 - accuracy: 0.4318 - val_loss: 17.9747 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 27.8349 - accuracy: 0.5682 - val_loss: 27.4263 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 29.2897 - accuracy: 0.5682 - val_loss: 6.1126 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 10.6640 - accuracy: 0.5227 - val_loss: 43.2637 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 24.0457 - accuracy: 0.4318 - val_loss: 0.8456 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 7.0925 - accuracy: 0.4773 - val_loss: 28.0987 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 17.3090 - accuracy: 0.5682 - val_loss: 19.6459 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.65343\n",
      "2/2 - 11s - loss: 9.4253 - accuracy: 0.5682 - val_loss: 28.2842 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 14.9396 - accuracy: 0.4318 - val_loss: 30.6772 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 14.2843 - accuracy: 0.4318 - val_loss: 14.2501 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.65343\n",
      "2/2 - 11s - loss: 10.9630 - accuracy: 0.5682 - val_loss: 18.7421 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.65343\n",
      "2/2 - 12s - loss: 10.4376 - accuracy: 0.5682 - val_loss: 2.7241 - val_accuracy: 0.3125 - 12s/epoch - 6s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.65343\n",
      "2/2 - 12s - loss: 3.6475 - accuracy: 0.4773 - val_loss: 4.2537 - val_accuracy: 0.3125 - 12s/epoch - 6s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.65343\n",
      "2/2 - 11s - loss: 3.9040 - accuracy: 0.4318 - val_loss: 11.5474 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.65343\n",
      "2/2 - 11s - loss: 7.5327 - accuracy: 0.5682 - val_loss: 3.8689 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 3.9150 - accuracy: 0.4318 - val_loss: 5.0972 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 2.8269 - accuracy: 0.5455 - val_loss: 1.4501 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.65343\n",
      "2/2 - 11s - loss: 4.3941 - accuracy: 0.4545 - val_loss: 7.1355 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.4478 - accuracy: 0.3864 - val_loss: 7.6822 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 4.5818 - accuracy: 0.5909 - val_loss: 13.3726 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.8851 - accuracy: 0.4318 - val_loss: 3.8283 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.5997 - accuracy: 0.4545 - val_loss: 1.6684 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.5594 - accuracy: 0.4318 - val_loss: 9.0168 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.1818 - accuracy: 0.4091 - val_loss: 7.5723 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 9.4771 - accuracy: 0.5682 - val_loss: 5.9087 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 6.2625 - accuracy: 0.5682 - val_loss: 8.1377 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.2390 - accuracy: 0.5682 - val_loss: 11.6475 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.6834 - accuracy: 0.4773 - val_loss: 27.9159 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 4.2409 - accuracy: 0.5682 - val_loss: 46.7322 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.8551 - accuracy: 0.4545 - val_loss: 28.3618 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 1.8158 - accuracy: 0.6136 - val_loss: 26.3061 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.4508 - accuracy: 0.5000 - val_loss: 7.4333 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 7.2827 - accuracy: 0.4318 - val_loss: 265.5502 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.9542 - accuracy: 0.5909 - val_loss: 459.5053 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.8153 - accuracy: 0.3409 - val_loss: 277.6116 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.3082 - accuracy: 0.5000 - val_loss: 26.5364 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.9156 - accuracy: 0.5682 - val_loss: 93.6475 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.6052 - accuracy: 0.5000 - val_loss: 148.3696 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.7982 - accuracy: 0.4091 - val_loss: 199.1100 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.3422 - accuracy: 0.5682 - val_loss: 238.6403 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.7610 - accuracy: 0.5227 - val_loss: 130.5276 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.7620 - accuracy: 0.5909 - val_loss: 210.2340 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.4591 - accuracy: 0.4318 - val_loss: 110.4446 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 4.8754 - accuracy: 0.5682 - val_loss: 48.1630 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 4.7790 - accuracy: 0.4318 - val_loss: 51.7466 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.7461 - accuracy: 0.5000 - val_loss: 75.4594 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 4.9993 - accuracy: 0.5682 - val_loss: 107.3451 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 5.4406 - accuracy: 0.4318 - val_loss: 206.2433 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.9119 - accuracy: 0.3864 - val_loss: 259.0274 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 5.6732 - accuracy: 0.5682 - val_loss: 517.9440 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.6861 - accuracy: 0.4318 - val_loss: 694.7019 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.1235 - accuracy: 0.5682 - val_loss: 681.9335 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.1678 - accuracy: 0.4318 - val_loss: 687.9957 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.6402 - accuracy: 0.5682 - val_loss: 817.0568 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.0931 - accuracy: 0.3864 - val_loss: 714.8309 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.8843 - accuracy: 0.5227 - val_loss: 491.4541 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.7787 - accuracy: 0.5455 - val_loss: 356.2773 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4079 - accuracy: 0.4773 - val_loss: 54.8909 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.3417 - accuracy: 0.5682 - val_loss: 248.3337 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3175 - accuracy: 0.4318 - val_loss: 589.7638 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.7951 - accuracy: 0.4545 - val_loss: 693.6159 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 3.5343 - accuracy: 0.4091 - val_loss: 537.0452 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0348 - accuracy: 0.5227 - val_loss: 708.5507 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0997 - accuracy: 0.5227 - val_loss: 512.7637 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2936 - accuracy: 0.4091 - val_loss: 236.3937 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0872 - accuracy: 0.5455 - val_loss: 52.4291 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0331 - accuracy: 0.4545 - val_loss: 43.9991 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.5517 - accuracy: 0.5682 - val_loss: 41.4679 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4525 - accuracy: 0.5000 - val_loss: 309.5545 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1509 - accuracy: 0.5909 - val_loss: 85.6701 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 5.7675 - accuracy: 0.4318 - val_loss: 49.1638 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.6485 - accuracy: 0.4545 - val_loss: 35.9760 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 4.1665 - accuracy: 0.5455 - val_loss: 111.4694 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 5.1168 - accuracy: 0.4318 - val_loss: 73.7514 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.1187 - accuracy: 0.5227 - val_loss: 109.0248 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.7489 - accuracy: 0.4545 - val_loss: 104.7921 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.7447 - accuracy: 0.4545 - val_loss: 216.6962 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.5531 - accuracy: 0.4773 - val_loss: 206.0668 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.5328 - accuracy: 0.5455 - val_loss: 249.6820 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.5147 - accuracy: 0.5227 - val_loss: 158.5666 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.8854 - accuracy: 0.4091 - val_loss: 756.0414 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.6809 - accuracy: 0.5682 - val_loss: 658.7408 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.2741 - accuracy: 0.4318 - val_loss: 849.1364 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.0346 - accuracy: 0.5682 - val_loss: 767.0040 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.6001 - accuracy: 0.5000 - val_loss: 896.0553 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.9932 - accuracy: 0.5682 - val_loss: 1265.3218 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 5.2878 - accuracy: 0.5682 - val_loss: 1178.0146 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.7898 - accuracy: 0.4773 - val_loss: 856.2457 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 5.3633 - accuracy: 0.2727 - val_loss: 276.2258 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.0204 - accuracy: 0.5682 - val_loss: 208.7518 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.4615 - accuracy: 0.4318 - val_loss: 546.8618 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.2146 - accuracy: 0.4091 - val_loss: 815.3147 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.0444 - accuracy: 0.6364 - val_loss: 611.7660 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.7159 - accuracy: 0.3864 - val_loss: 613.2550 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.0560 - accuracy: 0.5227 - val_loss: 590.1403 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3901 - accuracy: 0.4773 - val_loss: 314.1894 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.5268 - accuracy: 0.5227 - val_loss: 26.9917 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.8568 - accuracy: 0.4545 - val_loss: 213.6966 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.6081 - accuracy: 0.4773 - val_loss: 184.8666 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.6173 - accuracy: 0.4545 - val_loss: 871.0166 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.9809 - accuracy: 0.5000 - val_loss: 567.3874 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.9806 - accuracy: 0.3182 - val_loss: 1133.9910 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.4657 - accuracy: 0.5682 - val_loss: 816.4353 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.9831 - accuracy: 0.5227 - val_loss: 564.1190 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.0335 - accuracy: 0.5682 - val_loss: 107.1630 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.4506 - accuracy: 0.2955 - val_loss: 792.6790 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.9777 - accuracy: 0.3409 - val_loss: 1308.9045 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.5455 - accuracy: 0.6364 - val_loss: 1758.9146 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.2068 - accuracy: 0.4318 - val_loss: 2085.0298 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 4.2504 - accuracy: 0.4545 - val_loss: 1423.2688 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 7.5017 - accuracy: 0.5682 - val_loss: 1220.9741 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.5995 - accuracy: 0.3182 - val_loss: 1007.5850 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1966 - accuracy: 0.4773 - val_loss: 1141.6835 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1374 - accuracy: 0.4091 - val_loss: 988.0374 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.9267 - accuracy: 0.6364 - val_loss: 62.1806 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.9176 - accuracy: 0.5909 - val_loss: 337.5937 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4134 - accuracy: 0.4091 - val_loss: 746.8379 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.4951 - accuracy: 0.5682 - val_loss: 748.1163 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.6233 - accuracy: 0.4318 - val_loss: 731.5681 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 4.3690 - accuracy: 0.5682 - val_loss: 98.3911 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 3.8839 - accuracy: 0.4773 - val_loss: 1629.7897 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.6495 - accuracy: 0.3864 - val_loss: 1714.6741 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.8561 - accuracy: 0.3636 - val_loss: 1860.2729 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.8104 - accuracy: 0.5227 - val_loss: 2647.5376 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2489 - accuracy: 0.5455 - val_loss: 3143.5181 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.3609 - accuracy: 0.4773 - val_loss: 3068.7610 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.7781 - accuracy: 0.5682 - val_loss: 2474.2275 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.6083 - accuracy: 0.4318 - val_loss: 1607.4956 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3964 - accuracy: 0.4318 - val_loss: 820.4098 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2230 - accuracy: 0.4773 - val_loss: 63.5724 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.9544 - accuracy: 0.4773 - val_loss: 486.5127 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.6248 - accuracy: 0.5682 - val_loss: 161.6526 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0253 - accuracy: 0.5682 - val_loss: 432.4788 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2320 - accuracy: 0.5909 - val_loss: 97.8848 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.1954 - accuracy: 0.4318 - val_loss: 24.7060 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.6831 - accuracy: 0.5682 - val_loss: 289.5140 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.3143 - accuracy: 0.4545 - val_loss: 99.4557 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.9845 - accuracy: 0.5682 - val_loss: 336.4395 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2263 - accuracy: 0.4773 - val_loss: 266.6562 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.3014 - accuracy: 0.5227 - val_loss: 57.4971 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1999 - accuracy: 0.4318 - val_loss: 109.5417 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2562 - accuracy: 0.5000 - val_loss: 84.9573 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0200 - accuracy: 0.5455 - val_loss: 377.1612 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4157 - accuracy: 0.5909 - val_loss: 793.3618 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0503 - accuracy: 0.5682 - val_loss: 1238.1700 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.6943 - accuracy: 0.4318 - val_loss: 1558.5120 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.8287 - accuracy: 0.5682 - val_loss: 1156.5737 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.3149 - accuracy: 0.4318 - val_loss: 1282.2874 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 4.1564 - accuracy: 0.5682 - val_loss: 1169.2483 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.9149 - accuracy: 0.5227 - val_loss: 1020.4849 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 6.3466 - accuracy: 0.4318 - val_loss: 1188.3484 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.2472 - accuracy: 0.4091 - val_loss: 1282.4459 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 5.0238 - accuracy: 0.5682 - val_loss: 610.0957 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.7326 - accuracy: 0.5000 - val_loss: 443.5447 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4441 - accuracy: 0.5682 - val_loss: 285.3690 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1746 - accuracy: 0.4318 - val_loss: 115.8826 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.9658 - accuracy: 0.5455 - val_loss: 11.4619 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4731 - accuracy: 0.5000 - val_loss: 100.6158 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4769 - accuracy: 0.4091 - val_loss: 205.4572 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.9125 - accuracy: 0.4318 - val_loss: 213.8852 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.4948 - accuracy: 0.5682 - val_loss: 265.4395 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.2442 - accuracy: 0.4545 - val_loss: 301.1911 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4895 - accuracy: 0.4773 - val_loss: 319.7689 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4898 - accuracy: 0.5455 - val_loss: 222.9037 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1146 - accuracy: 0.5682 - val_loss: 177.1029 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.9985 - accuracy: 0.5682 - val_loss: 139.7028 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.6608 - accuracy: 0.3636 - val_loss: 56.1348 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.5291 - accuracy: 0.5455 - val_loss: 22.4444 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.7362 - accuracy: 0.4318 - val_loss: 277.1977 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.8070 - accuracy: 0.6136 - val_loss: 532.8718 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.5275 - accuracy: 0.3864 - val_loss: 312.0399 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.3241 - accuracy: 0.3864 - val_loss: 3.6074 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.8422 - accuracy: 0.5682 - val_loss: 110.7890 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.0898 - accuracy: 0.4545 - val_loss: 329.4954 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1127 - accuracy: 0.5682 - val_loss: 473.9227 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1704 - accuracy: 0.4091 - val_loss: 477.6547 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.7891 - accuracy: 0.5682 - val_loss: 412.6214 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.1168 - accuracy: 0.6364 - val_loss: 68.6262 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 5.4119 - accuracy: 0.4318 - val_loss: 519.3169 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 3.2042 - accuracy: 0.5000 - val_loss: 2596.1143 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 4.5696 - accuracy: 0.5682 - val_loss: 9056.0156 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 12.7297 - accuracy: 0.4318 - val_loss: 819.7856 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 6.0672 - accuracy: 0.5227 - val_loss: 422.5413 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2498 - accuracy: 0.5909 - val_loss: 869.2771 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 4.8418 - accuracy: 0.5000 - val_loss: 710.1865 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.8885 - accuracy: 0.5682 - val_loss: 530.9056 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.5928 - accuracy: 0.3864 - val_loss: 421.7596 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.3846 - accuracy: 0.4545 - val_loss: 440.5224 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.6624 - accuracy: 0.5682 - val_loss: 4203.1118 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.6068 - accuracy: 0.5682 - val_loss: 2446.1206 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 2.6565 - accuracy: 0.5227 - val_loss: 604.5825 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.5637 - accuracy: 0.6136 - val_loss: 1788.5608 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.0460 - accuracy: 0.3864 - val_loss: 829.6058 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.9282 - accuracy: 0.5455 - val_loss: 4252.4688 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.2191 - accuracy: 0.6364 - val_loss: 3849.7419 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.2448 - accuracy: 0.5455 - val_loss: 3856.7407 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.2989 - accuracy: 0.5227 - val_loss: 3206.4678 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1267 - accuracy: 0.4773 - val_loss: 2043.1252 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.0796 - accuracy: 0.5909 - val_loss: 2146.8887 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.0517 - accuracy: 0.3636 - val_loss: 1122.7227 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.2390 - accuracy: 0.5682 - val_loss: 860.2439 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4363 - accuracy: 0.4091 - val_loss: 77.6985 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.8430 - accuracy: 0.5682 - val_loss: 169.5730 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1636 - accuracy: 0.4091 - val_loss: 176.8776 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.2667 - accuracy: 0.4091 - val_loss: 245.3982 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2806 - accuracy: 0.5682 - val_loss: 138.1860 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.5782 - accuracy: 0.4545 - val_loss: 90.5169 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.8704 - accuracy: 0.5227 - val_loss: 80.6438 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.9846 - accuracy: 0.4773 - val_loss: 24.8504 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.3002 - accuracy: 0.4545 - val_loss: 126.4226 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.8862 - accuracy: 0.5455 - val_loss: 100.9546 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1493 - accuracy: 0.5909 - val_loss: 14.3918 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1069 - accuracy: 0.4545 - val_loss: 20.3772 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3285 - accuracy: 0.5000 - val_loss: 51.3025 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.9988 - accuracy: 0.4545 - val_loss: 70.2054 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 4.2243 - accuracy: 0.5682 - val_loss: 136.3371 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 3.3280 - accuracy: 0.5227 - val_loss: 31.8514 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 3.0939 - accuracy: 0.4318 - val_loss: 190.6669 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.9378 - accuracy: 0.5000 - val_loss: 5.6387 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.5359 - accuracy: 0.4318 - val_loss: 93.5532 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.7457 - accuracy: 0.4545 - val_loss: 52.6812 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.8594 - accuracy: 0.5455 - val_loss: 116.5933 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4499 - accuracy: 0.5909 - val_loss: 61.4012 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.0147 - accuracy: 0.5227 - val_loss: 114.8611 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.3717 - accuracy: 0.5227 - val_loss: 92.1820 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.0583 - accuracy: 0.4318 - val_loss: 208.7815 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3673 - accuracy: 0.5682 - val_loss: 118.8594 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 3.8384 - accuracy: 0.4318 - val_loss: 263.7521 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.0952 - accuracy: 0.5455 - val_loss: 178.4438 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3665 - accuracy: 0.4773 - val_loss: 157.6621 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.1173 - accuracy: 0.5455 - val_loss: 80.7043 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.5948 - accuracy: 0.4091 - val_loss: 95.1787 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.8813 - accuracy: 0.4773 - val_loss: 68.4619 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.8478 - accuracy: 0.6364 - val_loss: 0.9320 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1987 - accuracy: 0.5455 - val_loss: 11.8213 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.2239 - accuracy: 0.5909 - val_loss: 25.6309 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.2557 - accuracy: 0.4773 - val_loss: 3.2463 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.8777 - accuracy: 0.5682 - val_loss: 33.9941 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 3.4752 - accuracy: 0.4318 - val_loss: 0.9888 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 3.1801 - accuracy: 0.5682 - val_loss: 8.4851 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.0783 - accuracy: 0.6136 - val_loss: 3.8906 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.7395 - accuracy: 0.4091 - val_loss: 16.5754 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3003 - accuracy: 0.5682 - val_loss: 12.0381 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.8294 - accuracy: 0.4318 - val_loss: 51.8445 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.9550 - accuracy: 0.5227 - val_loss: 138.4845 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.8200 - accuracy: 0.6818 - val_loss: 452.5802 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 2.2064 - accuracy: 0.5682 - val_loss: 89.1181 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 4.1966 - accuracy: 0.4318 - val_loss: 294.8834 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.6479 - accuracy: 0.5000 - val_loss: 353.3764 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1688 - accuracy: 0.6591 - val_loss: 90.9724 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 4.6197 - accuracy: 0.4318 - val_loss: 224.0331 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.8742 - accuracy: 0.5682 - val_loss: 204.4652 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.2060 - accuracy: 0.5000 - val_loss: 113.3329 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.0690 - accuracy: 0.4318 - val_loss: 161.2202 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.9473 - accuracy: 0.5455 - val_loss: 13.5503 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.3815 - accuracy: 0.4545 - val_loss: 23.2570 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.1390 - accuracy: 0.5909 - val_loss: 100.8523 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.2665 - accuracy: 0.4091 - val_loss: 97.6442 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.5394 - accuracy: 0.5909 - val_loss: 83.8486 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 0.9757 - accuracy: 0.5909 - val_loss: 34.0557 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 2.0698 - accuracy: 0.5682 - val_loss: 16.4677 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.3256 - accuracy: 0.4318 - val_loss: 96.2820 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.6261 - accuracy: 0.5455 - val_loss: 128.3347 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4512 - accuracy: 0.4318 - val_loss: 54.9129 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1374 - accuracy: 0.6364 - val_loss: 10.1081 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.65343\n",
      "2/2 - 10s - loss: 1.1918 - accuracy: 0.5455 - val_loss: 9.1450 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.8253 - accuracy: 0.5682 - val_loss: 44.9088 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.6176 - accuracy: 0.3864 - val_loss: 27.7869 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.5073 - accuracy: 0.5909 - val_loss: 24.4146 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1914 - accuracy: 0.5455 - val_loss: 19.9673 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.1911 - accuracy: 0.4773 - val_loss: 23.7436 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4426 - accuracy: 0.5455 - val_loss: 19.5397 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.9980 - accuracy: 0.5682 - val_loss: 18.6730 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.4691 - accuracy: 0.3409 - val_loss: 106.9113 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.65343\n",
      "2/2 - 7s - loss: 1.7033 - accuracy: 0.5682 - val_loss: 346.5200 - val_accuracy: 0.3125 - 7s/epoch - 4s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.0656 - accuracy: 0.4773 - val_loss: 605.6165 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4072 - accuracy: 0.5455 - val_loss: 849.2697 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.0797 - accuracy: 0.4773 - val_loss: 1175.8649 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.3226 - accuracy: 0.5455 - val_loss: 1218.0278 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.7897 - accuracy: 0.4318 - val_loss: 1059.6965 - val_accuracy: 0.3125 - 9s/epoch - 4s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.0016 - accuracy: 0.4545 - val_loss: 851.5056 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.9563 - accuracy: 0.4773 - val_loss: 619.1029 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.8254 - accuracy: 0.4545 - val_loss: 347.9227 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.2610 - accuracy: 0.4773 - val_loss: 24.0700 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.0961 - accuracy: 0.4545 - val_loss: 38.2581 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.65343\n",
      "2/2 - 9s - loss: 1.3029 - accuracy: 0.5000 - val_loss: 28.6827 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 0.9225 - accuracy: 0.5227 - val_loss: 21.0156 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4839 - accuracy: 0.5682 - val_loss: 21.4938 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4124 - accuracy: 0.4318 - val_loss: 107.5200 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.4249 - accuracy: 0.5682 - val_loss: 30.9567 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 2.4169 - accuracy: 0.4091 - val_loss: 33.1131 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.65343\n",
      "2/2 - 8s - loss: 1.2250 - accuracy: 0.5227 - val_loss: 19.1018 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "1/1 [==============================] - 1s 841ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification('new_ica',train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5360175  0.47232914]\n",
      " [0.5360175  0.47232914]\n",
      " [0.5360175  0.47232914]\n",
      " [0.5360175  0.47232914]\n",
      " [0.5360173  0.47232932]\n",
      " [0.5360173  0.4723293 ]\n",
      " [0.53601736 0.4723292 ]\n",
      " [0.53601736 0.4723292 ]\n",
      " [0.53601736 0.4723292 ]\n",
      " [0.5360174  0.47232917]\n",
      " [0.53601736 0.4723292 ]\n",
      " [0.53601736 0.4723292 ]\n",
      " [0.53601736 0.4723292 ]\n",
      " [0.5360176  0.472329  ]\n",
      " [0.53601754 0.47232905]\n",
      " [0.53601754 0.47232905]\n",
      " [0.53601754 0.47232908]\n",
      " [0.5360175  0.47232908]\n",
      " [0.5360175  0.47232908]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[11  0]\n",
      " [ 8  0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 833.00818, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 7s - loss: 172.6616 - accuracy: 0.5455 - val_loss: 833.0082 - val_accuracy: 0.3125 - 7s/epoch - 3s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 833.00818 to 182.77060, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 5s - loss: 559.6158 - accuracy: 0.4318 - val_loss: 182.7706 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 182.77060 to 164.71844, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 5s - loss: 144.0832 - accuracy: 0.5227 - val_loss: 164.7184 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 164.71844 to 37.32534, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 5s - loss: 155.4380 - accuracy: 0.6136 - val_loss: 37.3253 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 30.3556 - accuracy: 0.5909 - val_loss: 143.0764 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 180.2862 - accuracy: 0.4773 - val_loss: 59.1748 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 58.8849 - accuracy: 0.5682 - val_loss: 468.3637 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 374.9949 - accuracy: 0.4318 - val_loss: 90.6126 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 34.4170 - accuracy: 0.7045 - val_loss: 467.6991 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 616.0861 - accuracy: 0.5682 - val_loss: 498.0420 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 598.8752 - accuracy: 0.5682 - val_loss: 118.8301 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 168.4389 - accuracy: 0.6136 - val_loss: 873.7965 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 652.6884 - accuracy: 0.4318 - val_loss: 591.2902 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 328.6961 - accuracy: 0.4318 - val_loss: 320.9190 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 458.8003 - accuracy: 0.5682 - val_loss: 578.8375 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 37.32534\n",
      "2/2 - 5s - loss: 696.5693 - accuracy: 0.5682 - val_loss: 444.0994 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 37.32534 to 7.98180, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 7s - loss: 484.4987 - accuracy: 0.5682 - val_loss: 7.9818 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 124.2991 - accuracy: 0.5682 - val_loss: 935.7389 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 743.1824 - accuracy: 0.4318 - val_loss: 724.6295 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 540.2279 - accuracy: 0.4318 - val_loss: 124.5696 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 221.9562 - accuracy: 0.5682 - val_loss: 289.6790 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 411.3682 - accuracy: 0.5682 - val_loss: 138.3004 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 143.3144 - accuracy: 0.6136 - val_loss: 229.7169 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 136.5841 - accuracy: 0.4318 - val_loss: 69.1998 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 168.1927 - accuracy: 0.5682 - val_loss: 93.0243 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 108.8496 - accuracy: 0.5909 - val_loss: 304.2286 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 346.8608 - accuracy: 0.4318 - val_loss: 249.2695 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 208.1118 - accuracy: 0.4318 - val_loss: 101.3141 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 158.6226 - accuracy: 0.5682 - val_loss: 24.6239 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 58.5588 - accuracy: 0.6136 - val_loss: 248.8016 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 7.98180\n",
      "2/2 - 7s - loss: 205.5492 - accuracy: 0.4318 - val_loss: 71.8366 - val_accuracy: 0.6875 - 7s/epoch - 3s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 148.1121 - accuracy: 0.5682 - val_loss: 118.0736 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 132.6174 - accuracy: 0.5682 - val_loss: 192.5254 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 289.6333 - accuracy: 0.4318 - val_loss: 199.5534 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 136.6309 - accuracy: 0.5455 - val_loss: 127.0456 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 221.9062 - accuracy: 0.5682 - val_loss: 156.5952 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 266.4651 - accuracy: 0.5682 - val_loss: 10.7985 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 133.9180 - accuracy: 0.5227 - val_loss: 335.1494 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 557.0455 - accuracy: 0.4318 - val_loss: 212.8985 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 7.98180\n",
      "2/2 - 5s - loss: 220.8853 - accuracy: 0.5227 - val_loss: 71.8117 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 178.7345 - accuracy: 0.5682 - val_loss: 55.4298 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 7.98180\n",
      "2/2 - 6s - loss: 110.2721 - accuracy: 0.4318 - val_loss: 12.1792 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss improved from 7.98180 to 3.02567, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 6s - loss: 23.9653 - accuracy: 0.5682 - val_loss: 3.0257 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 3.02567\n",
      "2/2 - 5s - loss: 63.7283 - accuracy: 0.4318 - val_loss: 101.6207 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 190.7209 - accuracy: 0.5682 - val_loss: 53.8443 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 118.4883 - accuracy: 0.5227 - val_loss: 130.2027 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 261.8176 - accuracy: 0.4318 - val_loss: 37.4268 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 85.4059 - accuracy: 0.5682 - val_loss: 63.7173 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 109.3736 - accuracy: 0.5682 - val_loss: 65.0587 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 185.8231 - accuracy: 0.4318 - val_loss: 31.0316 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 145.4391 - accuracy: 0.4773 - val_loss: 51.5426 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 112.5482 - accuracy: 0.5682 - val_loss: 22.0559 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 98.3014 - accuracy: 0.4773 - val_loss: 39.5449 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 314.4849 - accuracy: 0.4318 - val_loss: 31.5592 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 3.02567\n",
      "2/2 - 7s - loss: 35.0753 - accuracy: 0.5909 - val_loss: 63.2342 - val_accuracy: 0.6875 - 7s/epoch - 3s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 3.02567\n",
      "2/2 - 8s - loss: 106.6000 - accuracy: 0.5682 - val_loss: 23.1223 - val_accuracy: 0.3125 - 8s/epoch - 4s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 3.02567\n",
      "2/2 - 8s - loss: 225.7177 - accuracy: 0.4318 - val_loss: 17.3595 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 3.02567\n",
      "2/2 - 7s - loss: 101.0966 - accuracy: 0.4773 - val_loss: 35.7175 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 3.02567\n",
      "2/2 - 7s - loss: 260.6863 - accuracy: 0.5682 - val_loss: 11.2535 - val_accuracy: 0.6875 - 7s/epoch - 3s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 107.0219 - accuracy: 0.5227 - val_loss: 4.7007 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 3.02567\n",
      "2/2 - 7s - loss: 149.7830 - accuracy: 0.4318 - val_loss: 12.2296 - val_accuracy: 0.6875 - 7s/epoch - 3s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 3.02567\n",
      "2/2 - 9s - loss: 147.4907 - accuracy: 0.5682 - val_loss: 12.6887 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 138.7909 - accuracy: 0.5682 - val_loss: 8.8623 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 3.02567\n",
      "2/2 - 6s - loss: 145.5093 - accuracy: 0.4318 - val_loss: 10.2925 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 3.02567 to 2.68250, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 6s - loss: 88.9806 - accuracy: 0.4773 - val_loss: 2.6825 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 2.68250\n",
      "2/2 - 6s - loss: 292.3820 - accuracy: 0.5682 - val_loss: 16.5263 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 2.68250\n",
      "2/2 - 6s - loss: 116.9944 - accuracy: 0.6136 - val_loss: 32.8740 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 2.68250\n",
      "2/2 - 7s - loss: 39.0697 - accuracy: 0.4091 - val_loss: 31.3299 - val_accuracy: 0.6875 - 7s/epoch - 3s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 2.68250\n",
      "2/2 - 6s - loss: 26.3446 - accuracy: 0.6136 - val_loss: 35.3862 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 2.68250\n",
      "2/2 - 6s - loss: 97.6490 - accuracy: 0.3864 - val_loss: 16.3537 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 2.68250\n",
      "2/2 - 6s - loss: 138.8676 - accuracy: 0.5682 - val_loss: 11.3055 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 2.68250\n",
      "2/2 - 6s - loss: 143.6532 - accuracy: 0.4318 - val_loss: 21.5025 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss improved from 2.68250 to 0.62581, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 6s - loss: 87.4562 - accuracy: 0.3864 - val_loss: 0.6258 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 294.1525 - accuracy: 0.5682 - val_loss: 7.6413 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 56.9676 - accuracy: 0.6136 - val_loss: 1.0625 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 119.4413 - accuracy: 0.4318 - val_loss: 44.4492 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 119.2990 - accuracy: 0.5682 - val_loss: 103.9002 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 125.3205 - accuracy: 0.5227 - val_loss: 35.2604 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 82.4786 - accuracy: 0.4773 - val_loss: 45.3338 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 81.4413 - accuracy: 0.4773 - val_loss: 44.1830 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 42.9574 - accuracy: 0.5682 - val_loss: 47.7658 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 92.0526 - accuracy: 0.3864 - val_loss: 25.8623 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 113.6956 - accuracy: 0.5227 - val_loss: 30.4376 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 250.4447 - accuracy: 0.5682 - val_loss: 14.5431 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 139.6739 - accuracy: 0.5682 - val_loss: 23.3736 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 274.4837 - accuracy: 0.4318 - val_loss: 18.8591 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 302.7887 - accuracy: 0.4318 - val_loss: 0.7971 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 135.2628 - accuracy: 0.5682 - val_loss: 1.5920 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 192.4059 - accuracy: 0.5682 - val_loss: 7.3958 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 6.7793 - accuracy: 0.4318 - val_loss: 4.5161 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 37.4959 - accuracy: 0.5682 - val_loss: 10.2627 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 7.4400 - accuracy: 0.5455 - val_loss: 4.4695 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 50.8593 - accuracy: 0.4773 - val_loss: 13.3161 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.62581\n",
      "2/2 - 8s - loss: 67.3199 - accuracy: 0.4773 - val_loss: 32.6863 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.62581\n",
      "2/2 - 9s - loss: 15.1382 - accuracy: 0.6364 - val_loss: 42.5665 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.62581\n",
      "2/2 - 8s - loss: 41.5151 - accuracy: 0.5227 - val_loss: 41.0646 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.62581\n",
      "2/2 - 7s - loss: 167.0420 - accuracy: 0.5682 - val_loss: 33.6271 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 74.9880 - accuracy: 0.4773 - val_loss: 5.2471 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 287.3567 - accuracy: 0.4318 - val_loss: 0.8875 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 69.9690 - accuracy: 0.5455 - val_loss: 112.1702 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 446.6236 - accuracy: 0.5682 - val_loss: 171.6225 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 420.1983 - accuracy: 0.5682 - val_loss: 127.6255 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 141.0025 - accuracy: 0.5682 - val_loss: 58.1951 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 377.8178 - accuracy: 0.4318 - val_loss: 74.8405 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 227.6747 - accuracy: 0.4318 - val_loss: 153.2251 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 313.1184 - accuracy: 0.5682 - val_loss: 163.6142 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 400.6345 - accuracy: 0.5682 - val_loss: 150.9268 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 180.3266 - accuracy: 0.5682 - val_loss: 46.9535 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 275.0255 - accuracy: 0.4318 - val_loss: 23.9245 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 205.2473 - accuracy: 0.4318 - val_loss: 74.6002 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 220.2355 - accuracy: 0.5682 - val_loss: 86.5085 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 294.2766 - accuracy: 0.5682 - val_loss: 77.9128 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 91.2292 - accuracy: 0.4773 - val_loss: 30.7945 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 170.2967 - accuracy: 0.4318 - val_loss: 47.1327 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 97.3783 - accuracy: 0.5682 - val_loss: 35.8719 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 65.1123 - accuracy: 0.5682 - val_loss: 80.7615 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 228.7003 - accuracy: 0.4318 - val_loss: 69.8279 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 180.6648 - accuracy: 0.4318 - val_loss: 110.0018 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 197.3987 - accuracy: 0.5682 - val_loss: 191.1803 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 222.8842 - accuracy: 0.5682 - val_loss: 61.6453 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 51.5718 - accuracy: 0.5455 - val_loss: 27.6707 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 84.4951 - accuracy: 0.5682 - val_loss: 95.3631 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 57.6387 - accuracy: 0.3864 - val_loss: 110.4551 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 90.8496 - accuracy: 0.5682 - val_loss: 23.9031 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 51.3610 - accuracy: 0.5227 - val_loss: 20.4083 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 50.4722 - accuracy: 0.4318 - val_loss: 0.6554 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 29.9427 - accuracy: 0.5682 - val_loss: 61.5778 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 271.4099 - accuracy: 0.4318 - val_loss: 50.2988 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 111.4869 - accuracy: 0.4318 - val_loss: 37.1755 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 204.4996 - accuracy: 0.5682 - val_loss: 105.2586 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 408.6151 - accuracy: 0.5682 - val_loss: 127.6327 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 258.6778 - accuracy: 0.5682 - val_loss: 11.1893 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 104.9906 - accuracy: 0.4318 - val_loss: 5.8931 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 138.6463 - accuracy: 0.4318 - val_loss: 123.8517 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 146.7007 - accuracy: 0.5682 - val_loss: 195.9315 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 294.9615 - accuracy: 0.5682 - val_loss: 115.0316 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 115.3381 - accuracy: 0.5682 - val_loss: 0.8685 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 115.5336 - accuracy: 0.4318 - val_loss: 162.6482 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 72.4733 - accuracy: 0.5682 - val_loss: 113.7480 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 92.6984 - accuracy: 0.5682 - val_loss: 2.6741 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 194.9351 - accuracy: 0.4318 - val_loss: 4.5449 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 139.6040 - accuracy: 0.4318 - val_loss: 136.4523 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 46.4126 - accuracy: 0.5682 - val_loss: 94.0153 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 186.8181 - accuracy: 0.4318 - val_loss: 114.4231 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 75.8498 - accuracy: 0.5682 - val_loss: 83.5010 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 27.0383 - accuracy: 0.6591 - val_loss: 0.8183 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 161.3692 - accuracy: 0.4318 - val_loss: 39.9462 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 165.4735 - accuracy: 0.5682 - val_loss: 28.2244 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 116.6416 - accuracy: 0.5682 - val_loss: 22.4671 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 172.8163 - accuracy: 0.4318 - val_loss: 28.6222 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 82.1709 - accuracy: 0.4773 - val_loss: 34.5822 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 51.5734 - accuracy: 0.5682 - val_loss: 21.1186 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 53.4641 - accuracy: 0.4318 - val_loss: 12.7732 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 79.5653 - accuracy: 0.5682 - val_loss: 19.4825 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 48.6765 - accuracy: 0.5227 - val_loss: 48.0001 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 37.8147 - accuracy: 0.5909 - val_loss: 28.3315 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 265.6922 - accuracy: 0.4318 - val_loss: 72.2122 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 180.7453 - accuracy: 0.4318 - val_loss: 28.6694 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 226.9501 - accuracy: 0.5682 - val_loss: 51.9886 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 100.1342 - accuracy: 0.5682 - val_loss: 41.0610 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 243.3417 - accuracy: 0.4318 - val_loss: 52.1047 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 192.3539 - accuracy: 0.4318 - val_loss: 61.1483 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 161.3691 - accuracy: 0.5682 - val_loss: 7.3574 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 330.2018 - accuracy: 0.5682 - val_loss: 39.9340 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 163.9275 - accuracy: 0.5682 - val_loss: 28.0556 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 231.7183 - accuracy: 0.4318 - val_loss: 0.6859 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 347.9343 - accuracy: 0.4318 - val_loss: 55.2309 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 72.9633 - accuracy: 0.5682 - val_loss: 54.6508 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 187.4561 - accuracy: 0.5682 - val_loss: 16.3840 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 38.5199 - accuracy: 0.4318 - val_loss: 7.4980 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 34.6710 - accuracy: 0.5682 - val_loss: 2.0088 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 37.5256 - accuracy: 0.3409 - val_loss: 3.1162 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 73.2651 - accuracy: 0.4318 - val_loss: 37.7333 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 119.0061 - accuracy: 0.5682 - val_loss: 46.8136 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 102.8008 - accuracy: 0.5682 - val_loss: 7.1232 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 130.8078 - accuracy: 0.4318 - val_loss: 35.3602 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 86.7285 - accuracy: 0.4318 - val_loss: 14.4008 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 149.2971 - accuracy: 0.5682 - val_loss: 1.5382 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 65.2293 - accuracy: 0.5227 - val_loss: 73.2560 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 85.3413 - accuracy: 0.4318 - val_loss: 90.6754 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 124.5836 - accuracy: 0.5682 - val_loss: 141.5148 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 177.4692 - accuracy: 0.5682 - val_loss: 179.6120 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 90.2444 - accuracy: 0.4318 - val_loss: 188.1708 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 43.5769 - accuracy: 0.5227 - val_loss: 203.4381 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 195.1762 - accuracy: 0.5682 - val_loss: 162.4493 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 76.7064 - accuracy: 0.5682 - val_loss: 169.6400 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 67.4749 - accuracy: 0.4318 - val_loss: 166.8979 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 191.7175 - accuracy: 0.5682 - val_loss: 142.1920 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 112.3551 - accuracy: 0.5682 - val_loss: 197.2337 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 114.4460 - accuracy: 0.3864 - val_loss: 167.2175 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 184.6705 - accuracy: 0.4318 - val_loss: 165.1401 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 80.1132 - accuracy: 0.4773 - val_loss: 167.5811 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 216.0788 - accuracy: 0.5682 - val_loss: 132.6274 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 86.6742 - accuracy: 0.6591 - val_loss: 88.0514 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 77.2438 - accuracy: 0.4318 - val_loss: 115.6625 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 83.0779 - accuracy: 0.5682 - val_loss: 107.2169 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 81.0476 - accuracy: 0.4318 - val_loss: 101.4166 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 5.2628 - accuracy: 0.5000 - val_loss: 142.3199 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 113.3191 - accuracy: 0.5682 - val_loss: 145.5693 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 73.1812 - accuracy: 0.6591 - val_loss: 102.4931 - val_accuracy: 0.3125 - 6s/epoch - 3s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 75.7412 - accuracy: 0.4773 - val_loss: 107.1586 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 48.2699 - accuracy: 0.6136 - val_loss: 76.9131 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 28.4239 - accuracy: 0.5455 - val_loss: 59.6080 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 47.7091 - accuracy: 0.4318 - val_loss: 35.4862 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 33.3394 - accuracy: 0.5682 - val_loss: 26.1209 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 50.8474 - accuracy: 0.4318 - val_loss: 1.6114 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 91.5923 - accuracy: 0.5682 - val_loss: 7.5798 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 73.1548 - accuracy: 0.4773 - val_loss: 11.9046 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 36.7471 - accuracy: 0.5227 - val_loss: 34.7317 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 86.0014 - accuracy: 0.5682 - val_loss: 51.9614 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 122.8557 - accuracy: 0.4318 - val_loss: 3.3284 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 62.1946 - accuracy: 0.5227 - val_loss: 98.4818 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 203.9231 - accuracy: 0.5682 - val_loss: 47.3776 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 104.5906 - accuracy: 0.4773 - val_loss: 71.7712 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 118.9410 - accuracy: 0.4318 - val_loss: 57.0555 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 129.3600 - accuracy: 0.5682 - val_loss: 49.7557 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 111.4958 - accuracy: 0.5682 - val_loss: 9.7650 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 193.0760 - accuracy: 0.4318 - val_loss: 43.9203 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 110.1798 - accuracy: 0.4318 - val_loss: 54.3477 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 100.2939 - accuracy: 0.5682 - val_loss: 36.7462 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 28.0982 - accuracy: 0.5227 - val_loss: 9.8722 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 46.6923 - accuracy: 0.5227 - val_loss: 44.5768 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 51.0352 - accuracy: 0.5227 - val_loss: 0.6798 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 98.8136 - accuracy: 0.5682 - val_loss: 50.0062 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 134.1575 - accuracy: 0.4318 - val_loss: 24.8175 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 63.2459 - accuracy: 0.4773 - val_loss: 5.9492 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 256.1338 - accuracy: 0.5682 - val_loss: 6.2417 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 135.5175 - accuracy: 0.5682 - val_loss: 63.0166 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 249.8518 - accuracy: 0.4318 - val_loss: 73.2407 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 229.8211 - accuracy: 0.4318 - val_loss: 29.0622 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 170.1246 - accuracy: 0.5682 - val_loss: 36.0581 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 230.4771 - accuracy: 0.5682 - val_loss: 7.9213 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 106.2067 - accuracy: 0.4773 - val_loss: 10.0274 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 271.7607 - accuracy: 0.4318 - val_loss: 21.2397 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 95.6697 - accuracy: 0.4318 - val_loss: 33.6063 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 276.3577 - accuracy: 0.5682 - val_loss: 38.8393 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 197.0111 - accuracy: 0.5682 - val_loss: 23.4832 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 87.9500 - accuracy: 0.4318 - val_loss: 16.7160 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 123.7583 - accuracy: 0.4318 - val_loss: 51.1420 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 164.1407 - accuracy: 0.5682 - val_loss: 74.1208 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 233.8490 - accuracy: 0.5682 - val_loss: 51.7155 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 99.0866 - accuracy: 0.4773 - val_loss: 4.4042 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 161.0583 - accuracy: 0.4318 - val_loss: 57.6566 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 76.5278 - accuracy: 0.5682 - val_loss: 70.9955 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 79.0085 - accuracy: 0.5682 - val_loss: 16.5857 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 180.8244 - accuracy: 0.4318 - val_loss: 0.6923 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 81.3054 - accuracy: 0.5682 - val_loss: 91.7220 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 96.7591 - accuracy: 0.5682 - val_loss: 46.4894 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 55.3385 - accuracy: 0.5227 - val_loss: 96.2713 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 164.3094 - accuracy: 0.4318 - val_loss: 79.3526 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 88.7955 - accuracy: 0.5682 - val_loss: 88.3686 - val_accuracy: 0.6875 - 5s/epoch - 3s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 62.4908 - accuracy: 0.6364 - val_loss: 79.4180 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 138.3643 - accuracy: 0.4318 - val_loss: 15.7578 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 68.0632 - accuracy: 0.4318 - val_loss: 151.3611 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 174.9205 - accuracy: 0.5682 - val_loss: 65.6525 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 84.0063 - accuracy: 0.5227 - val_loss: 178.8528 - val_accuracy: 0.3125 - 5s/epoch - 3s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 159.6274 - accuracy: 0.4318 - val_loss: 62.4467 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 86.4982 - accuracy: 0.5682 - val_loss: 102.3455 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 105.3983 - accuracy: 0.5682 - val_loss: 127.6643 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 170.7154 - accuracy: 0.4318 - val_loss: 67.0317 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 134.8071 - accuracy: 0.3864 - val_loss: 116.9742 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 184.8453 - accuracy: 0.5682 - val_loss: 62.4926 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 70.2808 - accuracy: 0.5682 - val_loss: 27.5971 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 66.7677 - accuracy: 0.4318 - val_loss: 48.8405 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 116.9180 - accuracy: 0.5682 - val_loss: 54.1427 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 100.6133 - accuracy: 0.5682 - val_loss: 94.9602 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 182.5599 - accuracy: 0.4318 - val_loss: 67.1774 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 94.5264 - accuracy: 0.5227 - val_loss: 56.2260 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 118.3314 - accuracy: 0.5682 - val_loss: 18.7268 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 37.6685 - accuracy: 0.6591 - val_loss: 156.0293 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 214.7897 - accuracy: 0.4318 - val_loss: 3.0220 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 74.3542 - accuracy: 0.4318 - val_loss: 137.7981 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.62581\n",
      "2/2 - 6s - loss: 279.2245 - accuracy: 0.5682 - val_loss: 128.3020 - val_accuracy: 0.6875 - 6s/epoch - 3s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 230.1747 - accuracy: 0.5682 - val_loss: 4.2996 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 27.5114 - accuracy: 0.3636 - val_loss: 8.8404 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 5.5822 - accuracy: 0.5227 - val_loss: 51.9933 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 80.9956 - accuracy: 0.5682 - val_loss: 17.8605 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 52.5758 - accuracy: 0.5227 - val_loss: 154.6036 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 133.8447 - accuracy: 0.4318 - val_loss: 73.7269 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 120.7036 - accuracy: 0.5682 - val_loss: 117.9111 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 157.7624 - accuracy: 0.5682 - val_loss: 12.9681 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 29.5328 - accuracy: 0.4318 - val_loss: 44.3038 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 64.1432 - accuracy: 0.5682 - val_loss: 20.7347 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 50.6878 - accuracy: 0.5227 - val_loss: 143.3979 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 106.1572 - accuracy: 0.4318 - val_loss: 99.0828 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 149.0117 - accuracy: 0.5682 - val_loss: 129.9867 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 164.6597 - accuracy: 0.5682 - val_loss: 29.8105 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 40.3241 - accuracy: 0.4318 - val_loss: 17.2477 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 23.0988 - accuracy: 0.5682 - val_loss: 62.3829 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 47.4496 - accuracy: 0.3864 - val_loss: 66.1323 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 37.6788 - accuracy: 0.4773 - val_loss: 35.7109 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 20.8282 - accuracy: 0.5682 - val_loss: 33.9657 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 36.2352 - accuracy: 0.5909 - val_loss: 24.7530 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 32.1746 - accuracy: 0.4318 - val_loss: 61.0076 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 64.3456 - accuracy: 0.5682 - val_loss: 221.2753 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 163.7776 - accuracy: 0.4318 - val_loss: 155.0570 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 93.9622 - accuracy: 0.4773 - val_loss: 117.1934 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 135.6195 - accuracy: 0.5682 - val_loss: 40.2619 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 61.9900 - accuracy: 0.5682 - val_loss: 271.2648 - val_accuracy: 0.3125 - 5s/epoch - 2s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.62581\n",
      "2/2 - 5s - loss: 188.3907 - accuracy: 0.4318 - val_loss: 24.5005 - val_accuracy: 0.6875 - 5s/epoch - 2s/step\n",
      "1/1 [==============================] - 1s 522ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification('new_ica',train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57271165 0.42840087]\n",
      " [0.57271165 0.42840084]\n",
      " [0.5727115  0.42840093]\n",
      " [0.5727117  0.42840075]\n",
      " [0.572709   0.42840335]\n",
      " [0.5727086  0.42840353]\n",
      " [0.57271045 0.42840186]\n",
      " [0.5727103  0.42840186]\n",
      " [0.5727105  0.42840186]\n",
      " [0.5727108  0.42840147]\n",
      " [0.5727109  0.42840162]\n",
      " [0.57271045 0.42840177]\n",
      " [0.5727103  0.42840195]\n",
      " [0.5727126  0.42839977]\n",
      " [0.5727124  0.42840007]\n",
      " [0.57271224 0.42840007]\n",
      " [0.5727122  0.4284003 ]\n",
      " [0.57271194 0.42840013]\n",
      " [0.572712   0.42840016]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[11  0]\n",
      " [ 8  0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.77997, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7975 - accuracy: 0.5227 - val_loss: 0.7800 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 0.77997 to 0.77827, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 7s - loss: 1.4099 - accuracy: 0.4318 - val_loss: 0.7783 - val_accuracy: 0.6875 - 7s/epoch - 4s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.77827\n",
      "2/2 - 8s - loss: 0.8408 - accuracy: 0.5227 - val_loss: 0.7785 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 0.77827 to 0.77750, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 8s - loss: 0.8414 - accuracy: 0.3864 - val_loss: 0.7775 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 0.77750 to 0.77736, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 8s - loss: 0.7919 - accuracy: 0.5455 - val_loss: 0.7774 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.77736\n",
      "2/2 - 9s - loss: 0.7838 - accuracy: 0.5227 - val_loss: 0.7776 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.77736\n",
      "2/2 - 9s - loss: 0.7837 - accuracy: 0.5682 - val_loss: 0.7779 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.77736\n",
      "2/2 - 9s - loss: 0.7777 - accuracy: 0.5682 - val_loss: 0.7779 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.77736\n",
      "2/2 - 9s - loss: 0.7833 - accuracy: 0.5682 - val_loss: 0.7776 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.77736\n",
      "2/2 - 9s - loss: 0.7689 - accuracy: 0.5682 - val_loss: 0.7774 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.77736\n",
      "2/2 - 9s - loss: 0.7889 - accuracy: 0.5682 - val_loss: 0.7776 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.77736\n",
      "2/2 - 9s - loss: 0.8046 - accuracy: 0.5682 - val_loss: 0.7775 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 0.77736 to 0.77715, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 0.7702 - accuracy: 0.5682 - val_loss: 0.7772 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.77715 to 0.77618, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 0.7760 - accuracy: 0.5682 - val_loss: 0.7762 - val_accuracy: 0.3125 - 9s/epoch - 5s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.77618 to 0.77528, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7843 - accuracy: 0.5682 - val_loss: 0.7753 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.77528 to 0.77514, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7827 - accuracy: 0.5682 - val_loss: 0.7751 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.77514\n",
      "2/2 - 11s - loss: 0.7870 - accuracy: 0.5682 - val_loss: 0.7753 - val_accuracy: 0.3125 - 11s/epoch - 6s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.77514 to 0.77492, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7934 - accuracy: 0.4545 - val_loss: 0.7749 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 0.77492 to 0.77469, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7772 - accuracy: 0.5682 - val_loss: 0.7747 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.77469 to 0.77420, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7848 - accuracy: 0.4773 - val_loss: 0.7742 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.77420 to 0.77320, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7822 - accuracy: 0.4773 - val_loss: 0.7732 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.77320 to 0.77179, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7796 - accuracy: 0.5682 - val_loss: 0.7718 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.77179 to 0.77109, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7815 - accuracy: 0.5682 - val_loss: 0.7711 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss improved from 0.77109 to 0.77097, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7729 - accuracy: 0.5682 - val_loss: 0.7710 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss improved from 0.77097 to 0.77030, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.7898 - accuracy: 0.5455 - val_loss: 0.7703 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss improved from 0.77030 to 0.76912, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7787 - accuracy: 0.5682 - val_loss: 0.7691 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss improved from 0.76912 to 0.76804, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7769 - accuracy: 0.5682 - val_loss: 0.7680 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 0.76804 to 0.76695, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7680 - accuracy: 0.5682 - val_loss: 0.7670 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.76695 to 0.76688, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7785 - accuracy: 0.5682 - val_loss: 0.7669 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.76688\n",
      "2/2 - 11s - loss: 0.7584 - accuracy: 0.5682 - val_loss: 0.7683 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.76688\n",
      "2/2 - 10s - loss: 0.7643 - accuracy: 0.6136 - val_loss: 0.7694 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.76688\n",
      "2/2 - 11s - loss: 0.7779 - accuracy: 0.5455 - val_loss: 0.7697 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.76688\n",
      "2/2 - 10s - loss: 0.7845 - accuracy: 0.4545 - val_loss: 0.7687 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.76688\n",
      "2/2 - 11s - loss: 0.7787 - accuracy: 0.5455 - val_loss: 0.7669 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss improved from 0.76688 to 0.76537, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7784 - accuracy: 0.5455 - val_loss: 0.7654 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss improved from 0.76537 to 0.76489, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7737 - accuracy: 0.5682 - val_loss: 0.7649 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss improved from 0.76489 to 0.76424, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7747 - accuracy: 0.5682 - val_loss: 0.7642 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss improved from 0.76424 to 0.76276, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7679 - accuracy: 0.5682 - val_loss: 0.7628 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss improved from 0.76276 to 0.76221, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 0.7684 - accuracy: 0.5682 - val_loss: 0.7622 - val_accuracy: 0.6875 - 14s/epoch - 7s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss improved from 0.76221 to 0.76217, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 13s - loss: 0.7663 - accuracy: 0.5682 - val_loss: 0.7622 - val_accuracy: 0.6875 - 13s/epoch - 7s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.76217\n",
      "2/2 - 10s - loss: 0.7676 - accuracy: 0.5682 - val_loss: 0.7622 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss improved from 0.76217 to 0.76139, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7684 - accuracy: 0.5682 - val_loss: 0.7614 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.76139\n",
      "2/2 - 10s - loss: 0.7733 - accuracy: 0.5682 - val_loss: 0.7624 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.76139\n",
      "2/2 - 10s - loss: 0.7595 - accuracy: 0.5682 - val_loss: 0.7655 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.76139\n",
      "2/2 - 11s - loss: 0.7728 - accuracy: 0.4545 - val_loss: 0.7658 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.76139\n",
      "2/2 - 11s - loss: 0.7714 - accuracy: 0.5455 - val_loss: 0.7629 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss improved from 0.76139 to 0.76104, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.7642 - accuracy: 0.5682 - val_loss: 0.7610 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss improved from 0.76104 to 0.76089, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7660 - accuracy: 0.5682 - val_loss: 0.7609 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.76089\n",
      "2/2 - 10s - loss: 0.7644 - accuracy: 0.5682 - val_loss: 0.7619 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.76089\n",
      "2/2 - 10s - loss: 0.7637 - accuracy: 0.5682 - val_loss: 0.7621 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.76089 to 0.76032, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7688 - accuracy: 0.5455 - val_loss: 0.7603 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss improved from 0.76032 to 0.75862, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7607 - accuracy: 0.5682 - val_loss: 0.7586 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss improved from 0.75862 to 0.75772, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7682 - accuracy: 0.5682 - val_loss: 0.7577 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.75772\n",
      "2/2 - 10s - loss: 0.7649 - accuracy: 0.5682 - val_loss: 0.7584 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.75772\n",
      "2/2 - 10s - loss: 0.7733 - accuracy: 0.5682 - val_loss: 0.7583 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss improved from 0.75772 to 0.75707, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7594 - accuracy: 0.5682 - val_loss: 0.7571 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss improved from 0.75707 to 0.75673, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7572 - accuracy: 0.5682 - val_loss: 0.7567 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss improved from 0.75673 to 0.75640, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7646 - accuracy: 0.5682 - val_loss: 0.7564 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss improved from 0.75640 to 0.75554, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7550 - accuracy: 0.5682 - val_loss: 0.7555 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss improved from 0.75554 to 0.75413, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7607 - accuracy: 0.5682 - val_loss: 0.7541 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss improved from 0.75413 to 0.75284, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7687 - accuracy: 0.5682 - val_loss: 0.7528 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss improved from 0.75284 to 0.75240, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7590 - accuracy: 0.5682 - val_loss: 0.7524 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.75240\n",
      "2/2 - 10s - loss: 0.7599 - accuracy: 0.5682 - val_loss: 0.7532 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.75240\n",
      "2/2 - 11s - loss: 0.7718 - accuracy: 0.5682 - val_loss: 0.7535 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 0.75240 to 0.75180, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.7630 - accuracy: 0.5682 - val_loss: 0.7518 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss improved from 0.75180 to 0.75017, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7531 - accuracy: 0.5682 - val_loss: 0.7502 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.75017\n",
      "2/2 - 11s - loss: 0.7596 - accuracy: 0.5682 - val_loss: 0.7505 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.75017\n",
      "2/2 - 10s - loss: 0.7584 - accuracy: 0.5682 - val_loss: 0.7526 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.75017\n",
      "2/2 - 10s - loss: 0.7566 - accuracy: 0.5682 - val_loss: 0.7539 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.75017\n",
      "2/2 - 11s - loss: 0.7579 - accuracy: 0.5682 - val_loss: 0.7542 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.75017\n",
      "2/2 - 10s - loss: 0.7599 - accuracy: 0.5682 - val_loss: 0.7538 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.75017\n",
      "2/2 - 10s - loss: 0.7555 - accuracy: 0.5682 - val_loss: 0.7522 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.75017\n",
      "2/2 - 10s - loss: 0.7597 - accuracy: 0.5682 - val_loss: 0.7503 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss improved from 0.75017 to 0.74923, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7643 - accuracy: 0.5682 - val_loss: 0.7492 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.74923\n",
      "2/2 - 10s - loss: 0.7572 - accuracy: 0.5682 - val_loss: 0.7494 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.74923\n",
      "2/2 - 11s - loss: 0.7523 - accuracy: 0.5682 - val_loss: 0.7503 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.74923\n",
      "2/2 - 10s - loss: 0.7541 - accuracy: 0.5909 - val_loss: 0.7499 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss improved from 0.74923 to 0.74865, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7520 - accuracy: 0.5682 - val_loss: 0.7486 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss improved from 0.74865 to 0.74838, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7567 - accuracy: 0.5682 - val_loss: 0.7484 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.74838\n",
      "2/2 - 10s - loss: 0.7530 - accuracy: 0.5682 - val_loss: 0.7486 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.74838\n",
      "2/2 - 11s - loss: 0.7493 - accuracy: 0.5682 - val_loss: 0.7487 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.74838\n",
      "2/2 - 10s - loss: 0.7518 - accuracy: 0.5682 - val_loss: 0.7486 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.74838\n",
      "2/2 - 10s - loss: 0.7519 - accuracy: 0.5682 - val_loss: 0.7491 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.74838\n",
      "2/2 - 10s - loss: 0.7502 - accuracy: 0.5682 - val_loss: 0.7497 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.74838\n",
      "2/2 - 11s - loss: 0.7470 - accuracy: 0.5682 - val_loss: 0.7494 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss improved from 0.74838 to 0.74821, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7503 - accuracy: 0.5682 - val_loss: 0.7482 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss improved from 0.74821 to 0.74684, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7512 - accuracy: 0.5682 - val_loss: 0.7468 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss improved from 0.74684 to 0.74633, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7510 - accuracy: 0.5682 - val_loss: 0.7463 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.74633\n",
      "2/2 - 11s - loss: 0.7517 - accuracy: 0.5682 - val_loss: 0.7464 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss improved from 0.74633 to 0.74627, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7471 - accuracy: 0.5682 - val_loss: 0.7463 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss improved from 0.74627 to 0.74578, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7529 - accuracy: 0.5682 - val_loss: 0.7458 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.74578\n",
      "2/2 - 10s - loss: 0.7443 - accuracy: 0.5682 - val_loss: 0.7459 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss improved from 0.74578 to 0.74553, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7579 - accuracy: 0.5682 - val_loss: 0.7455 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss improved from 0.74553 to 0.74477, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7514 - accuracy: 0.5682 - val_loss: 0.7448 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss improved from 0.74477 to 0.74444, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7540 - accuracy: 0.5682 - val_loss: 0.7444 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.74444\n",
      "2/2 - 11s - loss: 0.7465 - accuracy: 0.5682 - val_loss: 0.7445 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.74444\n",
      "2/2 - 10s - loss: 0.7527 - accuracy: 0.5682 - val_loss: 0.7447 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.74444\n",
      "2/2 - 10s - loss: 0.7530 - accuracy: 0.5682 - val_loss: 0.7449 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss improved from 0.74444 to 0.74431, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7500 - accuracy: 0.5682 - val_loss: 0.7443 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss improved from 0.74431 to 0.74369, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7487 - accuracy: 0.5682 - val_loss: 0.7437 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss improved from 0.74369 to 0.74307, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7500 - accuracy: 0.5682 - val_loss: 0.7431 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss improved from 0.74307 to 0.74241, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7454 - accuracy: 0.5682 - val_loss: 0.7424 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 0.74241 to 0.74188, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7427 - accuracy: 0.5682 - val_loss: 0.7419 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 0.74188 to 0.74152, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7387 - accuracy: 0.5682 - val_loss: 0.7415 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss improved from 0.74152 to 0.74125, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7429 - accuracy: 0.5682 - val_loss: 0.7412 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss improved from 0.74125 to 0.74092, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7416 - accuracy: 0.5682 - val_loss: 0.7409 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss improved from 0.74092 to 0.74072, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7411 - accuracy: 0.5682 - val_loss: 0.7407 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss improved from 0.74072 to 0.74033, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7388 - accuracy: 0.5682 - val_loss: 0.7403 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss improved from 0.74033 to 0.73988, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7395 - accuracy: 0.5682 - val_loss: 0.7399 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss improved from 0.73988 to 0.73951, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7419 - accuracy: 0.5682 - val_loss: 0.7395 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.73951\n",
      "2/2 - 11s - loss: 0.7385 - accuracy: 0.5682 - val_loss: 0.7399 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.73951\n",
      "2/2 - 10s - loss: 0.7360 - accuracy: 0.5682 - val_loss: 0.7397 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss improved from 0.73951 to 0.73935, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7377 - accuracy: 0.5682 - val_loss: 0.7393 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.73935\n",
      "2/2 - 11s - loss: 0.7374 - accuracy: 0.5682 - val_loss: 0.7395 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.73935\n",
      "2/2 - 11s - loss: 0.7385 - accuracy: 0.5682 - val_loss: 0.7396 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss improved from 0.73935 to 0.73874, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7350 - accuracy: 0.5682 - val_loss: 0.7387 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss improved from 0.73874 to 0.73791, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7433 - accuracy: 0.5682 - val_loss: 0.7379 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss improved from 0.73791 to 0.73726, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7426 - accuracy: 0.5455 - val_loss: 0.7373 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.73726\n",
      "2/2 - 10s - loss: 0.7366 - accuracy: 0.5682 - val_loss: 0.7375 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.73726\n",
      "2/2 - 11s - loss: 0.7397 - accuracy: 0.5682 - val_loss: 0.7374 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss improved from 0.73726 to 0.73611, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7342 - accuracy: 0.5682 - val_loss: 0.7361 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss improved from 0.73611 to 0.73450, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7379 - accuracy: 0.5682 - val_loss: 0.7345 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss improved from 0.73450 to 0.73322, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7403 - accuracy: 0.5682 - val_loss: 0.7332 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.73322\n",
      "2/2 - 11s - loss: 0.7398 - accuracy: 0.5682 - val_loss: 0.7332 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.73322\n",
      "2/2 - 10s - loss: 0.7357 - accuracy: 0.5682 - val_loss: 0.7333 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss improved from 0.73322 to 0.73257, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7492 - accuracy: 0.5682 - val_loss: 0.7326 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss improved from 0.73257 to 0.73175, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7370 - accuracy: 0.5682 - val_loss: 0.7317 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss improved from 0.73175 to 0.73149, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7329 - accuracy: 0.5682 - val_loss: 0.7315 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss improved from 0.73149 to 0.73038, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7412 - accuracy: 0.5682 - val_loss: 0.7304 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss improved from 0.73038 to 0.72905, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7322 - accuracy: 0.5682 - val_loss: 0.7291 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss improved from 0.72905 to 0.72890, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7403 - accuracy: 0.5682 - val_loss: 0.7289 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss improved from 0.72890 to 0.72882, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7340 - accuracy: 0.5682 - val_loss: 0.7288 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.72882\n",
      "2/2 - 10s - loss: 0.7342 - accuracy: 0.5682 - val_loss: 0.7291 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.72882\n",
      "2/2 - 10s - loss: 0.7381 - accuracy: 0.5682 - val_loss: 0.7301 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.72882\n",
      "2/2 - 10s - loss: 0.7326 - accuracy: 0.5682 - val_loss: 0.7303 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.72882\n",
      "2/2 - 10s - loss: 0.7348 - accuracy: 0.5682 - val_loss: 0.7295 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss improved from 0.72882 to 0.72814, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7366 - accuracy: 0.5682 - val_loss: 0.7281 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss improved from 0.72814 to 0.72706, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7327 - accuracy: 0.5682 - val_loss: 0.7271 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss improved from 0.72706 to 0.72602, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7288 - accuracy: 0.5682 - val_loss: 0.7260 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss improved from 0.72602 to 0.72505, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7313 - accuracy: 0.5682 - val_loss: 0.7251 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss improved from 0.72505 to 0.72399, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7279 - accuracy: 0.5682 - val_loss: 0.7240 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss improved from 0.72399 to 0.72267, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7342 - accuracy: 0.5682 - val_loss: 0.7227 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.72267\n",
      "2/2 - 11s - loss: 0.7272 - accuracy: 0.5682 - val_loss: 0.7227 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.72267\n",
      "2/2 - 11s - loss: 0.7282 - accuracy: 0.5682 - val_loss: 0.7228 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss improved from 0.72267 to 0.72266, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7317 - accuracy: 0.5682 - val_loss: 0.7227 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss improved from 0.72266 to 0.72243, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7290 - accuracy: 0.5682 - val_loss: 0.7224 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.72243\n",
      "2/2 - 10s - loss: 0.7287 - accuracy: 0.5682 - val_loss: 0.7224 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss improved from 0.72243 to 0.72241, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7316 - accuracy: 0.5682 - val_loss: 0.7224 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.72241\n",
      "2/2 - 12s - loss: 0.7248 - accuracy: 0.5682 - val_loss: 0.7226 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.72241\n",
      "2/2 - 12s - loss: 0.7266 - accuracy: 0.5682 - val_loss: 0.7230 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.72241\n",
      "2/2 - 10s - loss: 0.7298 - accuracy: 0.5682 - val_loss: 0.7230 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.72241\n",
      "2/2 - 10s - loss: 0.7305 - accuracy: 0.5682 - val_loss: 0.7228 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.72241\n",
      "2/2 - 11s - loss: 0.7303 - accuracy: 0.5682 - val_loss: 0.7227 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss improved from 0.72241 to 0.72170, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7318 - accuracy: 0.5682 - val_loss: 0.7217 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss improved from 0.72170 to 0.71982, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 0.7284 - accuracy: 0.5682 - val_loss: 0.7198 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss improved from 0.71982 to 0.71890, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7261 - accuracy: 0.5682 - val_loss: 0.7189 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss improved from 0.71890 to 0.71785, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7258 - accuracy: 0.5682 - val_loss: 0.7179 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss improved from 0.71785 to 0.71750, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7237 - accuracy: 0.5682 - val_loss: 0.7175 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.71750\n",
      "2/2 - 10s - loss: 0.7327 - accuracy: 0.5682 - val_loss: 0.7188 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.71750\n",
      "2/2 - 11s - loss: 0.7252 - accuracy: 0.5682 - val_loss: 0.7217 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.71750\n",
      "2/2 - 11s - loss: 0.7277 - accuracy: 0.5682 - val_loss: 0.7222 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.71750\n",
      "2/2 - 11s - loss: 0.7268 - accuracy: 0.5682 - val_loss: 0.7208 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.71750\n",
      "2/2 - 11s - loss: 0.7296 - accuracy: 0.5682 - val_loss: 0.7182 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss improved from 0.71750 to 0.71358, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7255 - accuracy: 0.5682 - val_loss: 0.7136 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss improved from 0.71358 to 0.70816, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7193 - accuracy: 0.5682 - val_loss: 0.7082 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss improved from 0.70816 to 0.70306, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7217 - accuracy: 0.5682 - val_loss: 0.7031 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss improved from 0.70306 to 0.69853, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7220 - accuracy: 0.5682 - val_loss: 0.6985 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.69853\n",
      "2/2 - 10s - loss: 0.7315 - accuracy: 0.5682 - val_loss: 0.7008 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.69853\n",
      "2/2 - 10s - loss: 0.7255 - accuracy: 0.5682 - val_loss: 0.7122 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.69853\n",
      "2/2 - 11s - loss: 0.7189 - accuracy: 0.5682 - val_loss: 0.7176 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.69853\n",
      "2/2 - 11s - loss: 0.7202 - accuracy: 0.5682 - val_loss: 0.7185 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.69853\n",
      "2/2 - 10s - loss: 0.7218 - accuracy: 0.5682 - val_loss: 0.7163 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.69853\n",
      "2/2 - 11s - loss: 0.7211 - accuracy: 0.5682 - val_loss: 0.7129 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.69853\n",
      "2/2 - 11s - loss: 0.7216 - accuracy: 0.5682 - val_loss: 0.7093 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.69853\n",
      "2/2 - 11s - loss: 0.7220 - accuracy: 0.5682 - val_loss: 0.7051 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.69853\n",
      "2/2 - 11s - loss: 0.7223 - accuracy: 0.5682 - val_loss: 0.7009 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.69853\n",
      "2/2 - 11s - loss: 0.7190 - accuracy: 0.5682 - val_loss: 0.7001 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss improved from 0.69853 to 0.69840, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7187 - accuracy: 0.5682 - val_loss: 0.6984 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss improved from 0.69840 to 0.69753, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7184 - accuracy: 0.5682 - val_loss: 0.6975 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.69753\n",
      "2/2 - 11s - loss: 0.7141 - accuracy: 0.5682 - val_loss: 0.6989 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.69753\n",
      "2/2 - 11s - loss: 0.7179 - accuracy: 0.5682 - val_loss: 0.6990 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss improved from 0.69753 to 0.69529, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7162 - accuracy: 0.5682 - val_loss: 0.6953 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss improved from 0.69529 to 0.69238, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7150 - accuracy: 0.5682 - val_loss: 0.6924 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss improved from 0.69238 to 0.68699, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7156 - accuracy: 0.5682 - val_loss: 0.6870 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss improved from 0.68699 to 0.68176, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7178 - accuracy: 0.5682 - val_loss: 0.6818 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss improved from 0.68176 to 0.67999, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7157 - accuracy: 0.5682 - val_loss: 0.6800 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss improved from 0.67999 to 0.67754, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.7153 - accuracy: 0.5682 - val_loss: 0.6775 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.67754\n",
      "2/2 - 11s - loss: 0.7181 - accuracy: 0.5682 - val_loss: 0.6789 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss improved from 0.67754 to 0.67705, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7160 - accuracy: 0.5682 - val_loss: 0.6770 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss improved from 0.67705 to 0.67213, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7153 - accuracy: 0.5682 - val_loss: 0.6721 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss improved from 0.67213 to 0.67211, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7157 - accuracy: 0.5682 - val_loss: 0.6721 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.67211\n",
      "2/2 - 11s - loss: 0.7154 - accuracy: 0.5682 - val_loss: 0.6770 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.67211\n",
      "2/2 - 11s - loss: 0.7138 - accuracy: 0.5682 - val_loss: 0.6852 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.67211\n",
      "2/2 - 11s - loss: 0.7145 - accuracy: 0.5682 - val_loss: 0.6956 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.67211\n",
      "2/2 - 10s - loss: 0.7125 - accuracy: 0.5682 - val_loss: 0.6987 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.67211\n",
      "2/2 - 10s - loss: 0.7119 - accuracy: 0.5682 - val_loss: 0.6978 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.67211\n",
      "2/2 - 11s - loss: 0.7107 - accuracy: 0.5682 - val_loss: 0.6880 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.67211\n",
      "2/2 - 10s - loss: 0.7120 - accuracy: 0.5682 - val_loss: 0.6749 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss improved from 0.67211 to 0.66625, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.7135 - accuracy: 0.5682 - val_loss: 0.6662 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7141 - accuracy: 0.5682 - val_loss: 0.6784 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7128 - accuracy: 0.5682 - val_loss: 0.6956 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7135 - accuracy: 0.5682 - val_loss: 0.6984 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7132 - accuracy: 0.5682 - val_loss: 0.7099 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7165 - accuracy: 0.5682 - val_loss: 0.7099 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7143 - accuracy: 0.5682 - val_loss: 0.7017 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7152 - accuracy: 0.5682 - val_loss: 0.6922 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7123 - accuracy: 0.5682 - val_loss: 0.6896 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7121 - accuracy: 0.5682 - val_loss: 0.6849 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7111 - accuracy: 0.5682 - val_loss: 0.6826 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7111 - accuracy: 0.5682 - val_loss: 0.6835 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7077 - accuracy: 0.5682 - val_loss: 0.6853 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7112 - accuracy: 0.5682 - val_loss: 0.6876 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7110 - accuracy: 0.5682 - val_loss: 0.6887 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7094 - accuracy: 0.5682 - val_loss: 0.6893 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7091 - accuracy: 0.5682 - val_loss: 0.6922 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7096 - accuracy: 0.5682 - val_loss: 0.6950 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7078 - accuracy: 0.5682 - val_loss: 0.6937 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7081 - accuracy: 0.5682 - val_loss: 0.6949 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7124 - accuracy: 0.5682 - val_loss: 0.7013 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7079 - accuracy: 0.5682 - val_loss: 0.7102 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7072 - accuracy: 0.5682 - val_loss: 0.7218 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7068 - accuracy: 0.5682 - val_loss: 0.7303 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7095 - accuracy: 0.5682 - val_loss: 0.7371 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7102 - accuracy: 0.5682 - val_loss: 0.7355 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7080 - accuracy: 0.5682 - val_loss: 0.7263 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7082 - accuracy: 0.5682 - val_loss: 0.7242 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7075 - accuracy: 0.5682 - val_loss: 0.7250 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7067 - accuracy: 0.5682 - val_loss: 0.7216 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7089 - accuracy: 0.5682 - val_loss: 0.7176 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7056 - accuracy: 0.5682 - val_loss: 0.7163 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7051 - accuracy: 0.5682 - val_loss: 0.7163 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7048 - accuracy: 0.5682 - val_loss: 0.7120 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7052 - accuracy: 0.5682 - val_loss: 0.7045 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.66625\n",
      "2/2 - 12s - loss: 0.7055 - accuracy: 0.5682 - val_loss: 0.6987 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.66625\n",
      "2/2 - 12s - loss: 0.7048 - accuracy: 0.5682 - val_loss: 0.6946 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7050 - accuracy: 0.5682 - val_loss: 0.6919 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7047 - accuracy: 0.5682 - val_loss: 0.6916 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7070 - accuracy: 0.5682 - val_loss: 0.6925 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7035 - accuracy: 0.5682 - val_loss: 0.6933 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7031 - accuracy: 0.5682 - val_loss: 0.6954 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7054 - accuracy: 0.5682 - val_loss: 0.6967 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7014 - accuracy: 0.5682 - val_loss: 0.6998 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7039 - accuracy: 0.5682 - val_loss: 0.7038 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7049 - accuracy: 0.5682 - val_loss: 0.7102 - val_accuracy: 0.7500 - 11s/epoch - 5s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7024 - accuracy: 0.5682 - val_loss: 0.7169 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7019 - accuracy: 0.5682 - val_loss: 0.7194 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7015 - accuracy: 0.5682 - val_loss: 0.7169 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7032 - accuracy: 0.5682 - val_loss: 0.7127 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7017 - accuracy: 0.5682 - val_loss: 0.7080 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7048 - accuracy: 0.5682 - val_loss: 0.7043 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7017 - accuracy: 0.5682 - val_loss: 0.7010 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7007 - accuracy: 0.5682 - val_loss: 0.6960 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7005 - accuracy: 0.5682 - val_loss: 0.6930 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7025 - accuracy: 0.5682 - val_loss: 0.6964 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.6993 - accuracy: 0.5682 - val_loss: 0.7113 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.7011 - accuracy: 0.5682 - val_loss: 0.7293 - val_accuracy: 0.3125 - 11s/epoch - 6s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.6993 - accuracy: 0.5682 - val_loss: 0.7407 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.6989 - accuracy: 0.5682 - val_loss: 0.7447 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.6990 - accuracy: 0.5682 - val_loss: 0.7433 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.6993 - accuracy: 0.5682 - val_loss: 0.7500 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.6981 - accuracy: 0.5682 - val_loss: 0.7589 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7003 - accuracy: 0.5682 - val_loss: 0.7660 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7007 - accuracy: 0.5682 - val_loss: 0.7644 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.6987 - accuracy: 0.5682 - val_loss: 0.7500 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.66625\n",
      "2/2 - 11s - loss: 0.6976 - accuracy: 0.5682 - val_loss: 0.7310 - val_accuracy: 0.3125 - 11s/epoch - 6s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.6990 - accuracy: 0.5682 - val_loss: 0.7111 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.7001 - accuracy: 0.5682 - val_loss: 0.6877 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.66625\n",
      "2/2 - 10s - loss: 0.6969 - accuracy: 0.5682 - val_loss: 0.6668 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss improved from 0.66625 to 0.65824, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6966 - accuracy: 0.5682 - val_loss: 0.6582 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss improved from 0.65824 to 0.65292, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6966 - accuracy: 0.5682 - val_loss: 0.6529 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss improved from 0.65292 to 0.65241, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6959 - accuracy: 0.5682 - val_loss: 0.6524 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss improved from 0.65241 to 0.65154, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6957 - accuracy: 0.5682 - val_loss: 0.6515 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.65154\n",
      "2/2 - 10s - loss: 0.6969 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.65154\n",
      "2/2 - 10s - loss: 0.6963 - accuracy: 0.5682 - val_loss: 0.6547 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.65154\n",
      "2/2 - 11s - loss: 0.6942 - accuracy: 0.5682 - val_loss: 0.6565 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.65154\n",
      "2/2 - 11s - loss: 0.6970 - accuracy: 0.5682 - val_loss: 0.6578 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.65154\n",
      "2/2 - 10s - loss: 0.6974 - accuracy: 0.5682 - val_loss: 0.6589 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.65154\n",
      "2/2 - 10s - loss: 0.6964 - accuracy: 0.5682 - val_loss: 0.6579 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.65154\n",
      "2/2 - 11s - loss: 0.6953 - accuracy: 0.5682 - val_loss: 0.6593 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.65154\n",
      "2/2 - 10s - loss: 0.6941 - accuracy: 0.5682 - val_loss: 0.6602 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.65154\n",
      "2/2 - 11s - loss: 0.6944 - accuracy: 0.5682 - val_loss: 0.6556 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss improved from 0.65154 to 0.64929, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6974 - accuracy: 0.5682 - val_loss: 0.6493 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss improved from 0.64929 to 0.64752, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6939 - accuracy: 0.5682 - val_loss: 0.6475 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss improved from 0.64752 to 0.63956, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6941 - accuracy: 0.5682 - val_loss: 0.6396 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss improved from 0.63956 to 0.63275, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6949 - accuracy: 0.5682 - val_loss: 0.6327 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.63275\n",
      "2/2 - 12s - loss: 0.6952 - accuracy: 0.5682 - val_loss: 0.6335 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6945 - accuracy: 0.5682 - val_loss: 0.6393 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6928 - accuracy: 0.5682 - val_loss: 0.6469 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.63275\n",
      "2/2 - 10s - loss: 0.6940 - accuracy: 0.5682 - val_loss: 0.6533 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6942 - accuracy: 0.5682 - val_loss: 0.6576 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6935 - accuracy: 0.5682 - val_loss: 0.6629 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6933 - accuracy: 0.5682 - val_loss: 0.6726 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6963 - accuracy: 0.5682 - val_loss: 0.6820 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6943 - accuracy: 0.5682 - val_loss: 0.6770 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.63275\n",
      "2/2 - 10s - loss: 0.6945 - accuracy: 0.5682 - val_loss: 0.6684 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6933 - accuracy: 0.5682 - val_loss: 0.6630 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6921 - accuracy: 0.5682 - val_loss: 0.6568 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6929 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6925 - accuracy: 0.5682 - val_loss: 0.6489 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.63275\n",
      "2/2 - 11s - loss: 0.6918 - accuracy: 0.5682 - val_loss: 0.6482 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000262A828B9A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000262A828B9A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 598ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGLEEGNet = EEGNet_TSGLEEGNet_classification('new_ica', train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57277614 0.4272239 ]\n",
      " [0.57273    0.42727003]\n",
      " [0.57268506 0.42731488]\n",
      " [0.57264143 0.42735854]\n",
      " [0.57171977 0.4282802 ]\n",
      " [0.5717015  0.42829838]\n",
      " [0.5722332  0.4277668 ]\n",
      " [0.5722018  0.4277982 ]\n",
      " [0.5721712  0.4278288 ]\n",
      " [0.57236767 0.42763233]\n",
      " [0.5723327  0.42766732]\n",
      " [0.5722986  0.42770135]\n",
      " [0.5722655  0.42773452]\n",
      " [0.5733805  0.42661947]\n",
      " [0.5732559  0.426744  ]\n",
      " [0.57319635 0.42680368]\n",
      " [0.57308215 0.42691785]\n",
      " [0.57302743 0.42697254]\n",
      " [0.5729743  0.4270257 ]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[11  0]\n",
      " [ 8  0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGLEEGNet)\n",
    "preds_TSGLEEGNet = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_TSGLEEGNet)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_TSGLEEGNet = compute_metrics(test_labels, preds_TSGLEEGNet)\n",
    "print(performance_TSGLEEGNet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
