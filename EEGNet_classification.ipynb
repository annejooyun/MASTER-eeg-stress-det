{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from load_data import load_data\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification\n",
    "import utils.variables as v\n",
    "\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P010_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P013_S001_002\n",
      "ERROR:root:1) Failed to read data for recording P020_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P023_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs: \n",
      " ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n",
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n",
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n",
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n",
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n",
      "(44, 8, 38400)\n",
      "(13200, 8, 128)\n",
      "(13200, 1)\n",
      "(5700, 8, 128)\n",
      "(5700, 1)\n",
      "(4800, 8, 128)\n",
      "(4800, 1)\n",
      "Shape of train data set: (13200, 8, 128)\n",
      "Shape of train labels set: (13200, 1)\n",
      "Shape of validation data set: (4800, 8, 128)\n",
      "Shape of validation labels set: (4800, 1)\n",
      "Shape of test data set: (5700, 8, 128)\n",
      "Shape of test labels set: (5700, 1)\n"
     ]
    }
   ],
   "source": [
    "data_type = 'new_ica'\n",
    "label_type = 'stai'\n",
    "\n",
    "train_data, test_data, val_data, train_labels, test_labels, val_labels = load_data(data_type, label_type, epoched = True, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "new_ica\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11992\\1181731541.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprobs_EEGNet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEEGNet_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\annej\\OneDrive\\Documents\\GitHub\\MASTER-eeg-stress-det\\classifiers.py\u001b[0m in \u001b[0;36mEEGNet_classification\u001b[1;34m(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;31m# Riemannian geometry classification (below)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     fittedModel = model.fit(train_data, train_labels, batch_size = 64, epochs = 300, \n\u001b[0m\u001b[0;32m    213\u001b[0m                             \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                             callbacks=[checkpointer], class_weight = class_weights)\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[2264 1036]\n",
      " [1763  637]]\n",
      "[50.89 56.22 38.08]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for EEGNet on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)\n",
    "\n",
    "plot_confusion_matrix(preds_EEGNet, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for EEGNet on ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.03131, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 72s - loss: 1.2047 - accuracy: 0.6973 - val_loss: 1.0313 - val_accuracy: 0.6425 - 72s/epoch - 174ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.03131\n",
      "413/413 - 73s - loss: 0.7103 - accuracy: 0.8936 - val_loss: 1.3192 - val_accuracy: 0.6046 - 73s/epoch - 177ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.03131\n",
      "413/413 - 76s - loss: 0.5425 - accuracy: 0.9195 - val_loss: 1.5403 - val_accuracy: 0.6290 - 76s/epoch - 185ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.03131\n",
      "413/413 - 78s - loss: 0.4195 - accuracy: 0.9444 - val_loss: 2.0542 - val_accuracy: 0.6290 - 78s/epoch - 188ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.03131\n",
      "413/413 - 74s - loss: 0.3960 - accuracy: 0.9452 - val_loss: 1.5726 - val_accuracy: 0.6156 - 74s/epoch - 178ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.03131\n",
      "413/413 - 83s - loss: 0.3489 - accuracy: 0.9568 - val_loss: 1.3701 - val_accuracy: 0.6210 - 83s/epoch - 200ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.03131\n",
      "413/413 - 79s - loss: 0.3521 - accuracy: 0.9565 - val_loss: 1.2777 - val_accuracy: 0.6421 - 79s/epoch - 192ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.03131\n",
      "413/413 - 74s - loss: 0.3326 - accuracy: 0.9549 - val_loss: 1.5720 - val_accuracy: 0.6448 - 74s/epoch - 179ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.03131\n",
      "413/413 - 68s - loss: 0.3186 - accuracy: 0.9628 - val_loss: 1.2289 - val_accuracy: 0.6373 - 68s/epoch - 164ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.03131\n",
      "413/413 - 71s - loss: 0.3025 - accuracy: 0.9645 - val_loss: 1.4757 - val_accuracy: 0.6558 - 71s/epoch - 172ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.03131\n",
      "413/413 - 76s - loss: 0.2941 - accuracy: 0.9657 - val_loss: 1.3532 - val_accuracy: 0.6413 - 76s/epoch - 184ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.03131\n",
      "413/413 - 78s - loss: 0.2874 - accuracy: 0.9655 - val_loss: 1.1568 - val_accuracy: 0.6365 - 78s/epoch - 188ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.03131\n",
      "413/413 - 79s - loss: 0.2534 - accuracy: 0.9698 - val_loss: 1.1588 - val_accuracy: 0.5875 - 79s/epoch - 190ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.03131\n",
      "413/413 - 80s - loss: 0.2336 - accuracy: 0.9731 - val_loss: 1.9561 - val_accuracy: 0.6694 - 80s/epoch - 193ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.03131\n",
      "413/413 - 88s - loss: 0.2114 - accuracy: 0.9750 - val_loss: 1.0649 - val_accuracy: 0.6473 - 88s/epoch - 213ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.03131\n",
      "413/413 - 85s - loss: 0.2106 - accuracy: 0.9761 - val_loss: 1.2980 - val_accuracy: 0.6569 - 85s/epoch - 206ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.03131\n",
      "413/413 - 83s - loss: 0.2066 - accuracy: 0.9759 - val_loss: 1.2924 - val_accuracy: 0.6794 - 83s/epoch - 201ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.03131\n",
      "413/413 - 87s - loss: 0.1874 - accuracy: 0.9752 - val_loss: 1.2913 - val_accuracy: 0.7300 - 87s/epoch - 211ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.03131\n",
      "413/413 - 87s - loss: 0.1788 - accuracy: 0.9783 - val_loss: 1.7390 - val_accuracy: 0.6460 - 87s/epoch - 210ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.03131\n",
      "413/413 - 86s - loss: 0.1855 - accuracy: 0.9772 - val_loss: 1.2607 - val_accuracy: 0.6840 - 86s/epoch - 209ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.03131\n",
      "413/413 - 89s - loss: 0.1637 - accuracy: 0.9798 - val_loss: 1.1325 - val_accuracy: 0.6379 - 89s/epoch - 216ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.03131\n",
      "413/413 - 89s - loss: 0.1484 - accuracy: 0.9843 - val_loss: 1.2363 - val_accuracy: 0.6948 - 89s/epoch - 215ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.03131\n",
      "413/413 - 87s - loss: 0.1608 - accuracy: 0.9801 - val_loss: 1.9479 - val_accuracy: 0.6927 - 87s/epoch - 212ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.03131\n",
      "413/413 - 84s - loss: 0.1789 - accuracy: 0.9759 - val_loss: 2.2212 - val_accuracy: 0.6910 - 84s/epoch - 203ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.03131\n",
      "413/413 - 86s - loss: 0.1438 - accuracy: 0.9836 - val_loss: 1.3036 - val_accuracy: 0.7265 - 86s/epoch - 209ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.03131\n",
      "413/413 - 88s - loss: 0.1592 - accuracy: 0.9811 - val_loss: 1.4828 - val_accuracy: 0.6694 - 88s/epoch - 212ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.03131\n",
      "413/413 - 89s - loss: 0.1462 - accuracy: 0.9814 - val_loss: 1.3245 - val_accuracy: 0.7144 - 89s/epoch - 216ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.03131\n",
      "413/413 - 86s - loss: 0.1353 - accuracy: 0.9847 - val_loss: 2.0202 - val_accuracy: 0.6644 - 86s/epoch - 208ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 1.03131 to 0.89644, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 81s - loss: 0.1533 - accuracy: 0.9812 - val_loss: 0.8964 - val_accuracy: 0.7119 - 81s/epoch - 197ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.89644\n",
      "413/413 - 85s - loss: 0.1192 - accuracy: 0.9875 - val_loss: 1.6326 - val_accuracy: 0.7546 - 85s/epoch - 206ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.1509 - accuracy: 0.9803 - val_loss: 1.4874 - val_accuracy: 0.7206 - 79s/epoch - 190ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.89644\n",
      "413/413 - 85s - loss: 0.1139 - accuracy: 0.9878 - val_loss: 1.7316 - val_accuracy: 0.7310 - 85s/epoch - 205ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.1257 - accuracy: 0.9836 - val_loss: 1.4182 - val_accuracy: 0.7527 - 77s/epoch - 188ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.89644\n",
      "413/413 - 78s - loss: 0.1234 - accuracy: 0.9873 - val_loss: 1.4648 - val_accuracy: 0.7556 - 78s/epoch - 190ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.1316 - accuracy: 0.9848 - val_loss: 2.0937 - val_accuracy: 0.7094 - 75s/epoch - 182ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.1166 - accuracy: 0.9877 - val_loss: 1.7222 - val_accuracy: 0.7481 - 74s/epoch - 179ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.1306 - accuracy: 0.9846 - val_loss: 1.2775 - val_accuracy: 0.6971 - 75s/epoch - 182ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.1108 - accuracy: 0.9873 - val_loss: 1.8021 - val_accuracy: 0.7475 - 77s/epoch - 187ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.1158 - accuracy: 0.9869 - val_loss: 1.2712 - val_accuracy: 0.7315 - 79s/epoch - 192ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.1049 - accuracy: 0.9881 - val_loss: 1.1065 - val_accuracy: 0.6765 - 79s/epoch - 192ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.89644\n",
      "413/413 - 80s - loss: 0.0935 - accuracy: 0.9911 - val_loss: 1.6046 - val_accuracy: 0.7317 - 80s/epoch - 193ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.89644\n",
      "413/413 - 94s - loss: 0.1174 - accuracy: 0.9873 - val_loss: 1.3697 - val_accuracy: 0.7183 - 94s/epoch - 228ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.1320 - accuracy: 0.9845 - val_loss: 1.3594 - val_accuracy: 0.6669 - 79s/epoch - 192ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.0958 - accuracy: 0.9896 - val_loss: 2.3653 - val_accuracy: 0.7142 - 79s/epoch - 191ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.89644\n",
      "413/413 - 86s - loss: 0.1238 - accuracy: 0.9842 - val_loss: 1.8630 - val_accuracy: 0.7040 - 86s/epoch - 208ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.89644\n",
      "413/413 - 81s - loss: 0.1121 - accuracy: 0.9873 - val_loss: 1.9035 - val_accuracy: 0.6862 - 81s/epoch - 196ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.89644\n",
      "413/413 - 81s - loss: 0.1169 - accuracy: 0.9858 - val_loss: 0.9684 - val_accuracy: 0.7033 - 81s/epoch - 196ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.89644\n",
      "413/413 - 82s - loss: 0.0955 - accuracy: 0.9905 - val_loss: 1.4112 - val_accuracy: 0.7446 - 82s/epoch - 199ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.89644\n",
      "413/413 - 81s - loss: 0.1039 - accuracy: 0.9895 - val_loss: 1.6990 - val_accuracy: 0.7467 - 81s/epoch - 197ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0893 - accuracy: 0.9916 - val_loss: 1.9972 - val_accuracy: 0.7119 - 77s/epoch - 186ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0987 - accuracy: 0.9889 - val_loss: 1.1384 - val_accuracy: 0.7346 - 75s/epoch - 181ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0996 - accuracy: 0.9902 - val_loss: 0.9370 - val_accuracy: 0.7079 - 77s/epoch - 186ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0957 - accuracy: 0.9902 - val_loss: 2.1310 - val_accuracy: 0.7179 - 74s/epoch - 179ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0932 - accuracy: 0.9908 - val_loss: 1.6815 - val_accuracy: 0.7540 - 75s/epoch - 181ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0869 - accuracy: 0.9913 - val_loss: 1.6119 - val_accuracy: 0.7354 - 74s/epoch - 179ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.1056 - accuracy: 0.9888 - val_loss: 1.8172 - val_accuracy: 0.6929 - 76s/epoch - 183ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0738 - accuracy: 0.9946 - val_loss: 1.5001 - val_accuracy: 0.7237 - 76s/epoch - 185ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0966 - accuracy: 0.9914 - val_loss: 1.8322 - val_accuracy: 0.7223 - 76s/epoch - 185ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0978 - accuracy: 0.9892 - val_loss: 1.1383 - val_accuracy: 0.7433 - 76s/epoch - 183ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0949 - accuracy: 0.9905 - val_loss: 1.4191 - val_accuracy: 0.7519 - 76s/epoch - 184ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0642 - accuracy: 0.9942 - val_loss: 2.0178 - val_accuracy: 0.7548 - 76s/epoch - 183ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0936 - accuracy: 0.9906 - val_loss: 1.7499 - val_accuracy: 0.6935 - 75s/epoch - 181ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0838 - accuracy: 0.9916 - val_loss: 1.5731 - val_accuracy: 0.7248 - 74s/epoch - 180ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0708 - accuracy: 0.9939 - val_loss: 1.8755 - val_accuracy: 0.7375 - 73s/epoch - 177ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.1037 - accuracy: 0.9878 - val_loss: 1.5075 - val_accuracy: 0.7023 - 74s/epoch - 180ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0914 - accuracy: 0.9900 - val_loss: 2.2670 - val_accuracy: 0.6569 - 74s/epoch - 179ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0696 - accuracy: 0.9942 - val_loss: 2.6927 - val_accuracy: 0.7233 - 76s/epoch - 185ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0896 - accuracy: 0.9923 - val_loss: 2.0670 - val_accuracy: 0.6904 - 76s/epoch - 185ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0868 - accuracy: 0.9912 - val_loss: 1.7798 - val_accuracy: 0.7419 - 76s/epoch - 184ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0860 - accuracy: 0.9906 - val_loss: 1.6441 - val_accuracy: 0.6921 - 75s/epoch - 181ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.89644\n",
      "413/413 - 78s - loss: 0.0747 - accuracy: 0.9936 - val_loss: 2.2283 - val_accuracy: 0.6777 - 78s/epoch - 189ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0829 - accuracy: 0.9926 - val_loss: 1.1949 - val_accuracy: 0.7290 - 75s/epoch - 182ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0837 - accuracy: 0.9906 - val_loss: 1.7795 - val_accuracy: 0.7125 - 76s/epoch - 183ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0836 - accuracy: 0.9906 - val_loss: 2.0718 - val_accuracy: 0.6969 - 74s/epoch - 179ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0811 - accuracy: 0.9918 - val_loss: 1.5235 - val_accuracy: 0.5871 - 74s/epoch - 180ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0856 - accuracy: 0.9914 - val_loss: 1.6773 - val_accuracy: 0.6804 - 75s/epoch - 182ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0797 - accuracy: 0.9916 - val_loss: 1.3116 - val_accuracy: 0.6856 - 73s/epoch - 177ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0777 - accuracy: 0.9928 - val_loss: 2.4199 - val_accuracy: 0.7329 - 71s/epoch - 172ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0628 - accuracy: 0.9945 - val_loss: 2.1349 - val_accuracy: 0.7329 - 72s/epoch - 175ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0829 - accuracy: 0.9902 - val_loss: 2.1466 - val_accuracy: 0.7423 - 74s/epoch - 179ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0828 - accuracy: 0.9910 - val_loss: 1.4850 - val_accuracy: 0.7085 - 75s/epoch - 180ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0831 - accuracy: 0.9916 - val_loss: 1.7649 - val_accuracy: 0.6781 - 75s/epoch - 181ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0767 - accuracy: 0.9914 - val_loss: 1.9808 - val_accuracy: 0.7460 - 75s/epoch - 181ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0914 - accuracy: 0.9921 - val_loss: 1.3735 - val_accuracy: 0.6617 - 73s/epoch - 176ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0777 - accuracy: 0.9921 - val_loss: 1.8978 - val_accuracy: 0.6877 - 75s/epoch - 180ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0704 - accuracy: 0.9942 - val_loss: 1.7171 - val_accuracy: 0.7231 - 77s/epoch - 188ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0753 - accuracy: 0.9925 - val_loss: 1.6261 - val_accuracy: 0.7342 - 73s/epoch - 178ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0841 - accuracy: 0.9905 - val_loss: 1.6718 - val_accuracy: 0.7256 - 76s/epoch - 184ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.89644\n",
      "413/413 - 78s - loss: 0.0731 - accuracy: 0.9931 - val_loss: 2.3541 - val_accuracy: 0.7163 - 78s/epoch - 189ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0735 - accuracy: 0.9920 - val_loss: 2.3627 - val_accuracy: 0.6913 - 76s/epoch - 184ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0519 - accuracy: 0.9959 - val_loss: 1.3324 - val_accuracy: 0.6848 - 77s/epoch - 187ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0883 - accuracy: 0.9908 - val_loss: 1.9571 - val_accuracy: 0.6392 - 76s/epoch - 185ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0766 - accuracy: 0.9936 - val_loss: 1.7542 - val_accuracy: 0.7385 - 75s/epoch - 182ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0772 - accuracy: 0.9923 - val_loss: 2.3443 - val_accuracy: 0.6858 - 75s/epoch - 182ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0670 - accuracy: 0.9941 - val_loss: 1.8327 - val_accuracy: 0.7425 - 75s/epoch - 181ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0644 - accuracy: 0.9940 - val_loss: 2.3466 - val_accuracy: 0.7435 - 75s/epoch - 183ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.89644\n",
      "413/413 - 82s - loss: 0.0735 - accuracy: 0.9924 - val_loss: 2.5343 - val_accuracy: 0.7127 - 82s/epoch - 199ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.89644\n",
      "413/413 - 87s - loss: 0.0676 - accuracy: 0.9941 - val_loss: 1.5308 - val_accuracy: 0.7290 - 87s/epoch - 211ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.89644\n",
      "413/413 - 85s - loss: 0.0751 - accuracy: 0.9931 - val_loss: 1.1595 - val_accuracy: 0.7210 - 85s/epoch - 207ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.89644\n",
      "413/413 - 107s - loss: 0.0489 - accuracy: 0.9965 - val_loss: 1.8169 - val_accuracy: 0.7223 - 107s/epoch - 259ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0805 - accuracy: 0.9918 - val_loss: 1.3234 - val_accuracy: 0.7129 - 76s/epoch - 184ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.89644\n",
      "413/413 - 78s - loss: 0.0818 - accuracy: 0.9917 - val_loss: 1.8841 - val_accuracy: 0.6923 - 78s/epoch - 189ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0632 - accuracy: 0.9941 - val_loss: 2.2378 - val_accuracy: 0.6933 - 72s/epoch - 174ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0599 - accuracy: 0.9957 - val_loss: 3.0699 - val_accuracy: 0.7287 - 76s/epoch - 185ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0840 - accuracy: 0.9920 - val_loss: 1.5519 - val_accuracy: 0.7075 - 77s/epoch - 188ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0782 - accuracy: 0.9920 - val_loss: 1.4472 - val_accuracy: 0.7517 - 77s/epoch - 185ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0654 - accuracy: 0.9945 - val_loss: 1.4973 - val_accuracy: 0.7340 - 73s/epoch - 176ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0622 - accuracy: 0.9945 - val_loss: 2.6291 - val_accuracy: 0.7287 - 74s/epoch - 179ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0678 - accuracy: 0.9941 - val_loss: 1.7429 - val_accuracy: 0.7369 - 75s/epoch - 182ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0796 - accuracy: 0.9926 - val_loss: 3.1327 - val_accuracy: 0.6950 - 72s/epoch - 173ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.89644\n",
      "413/413 - 95s - loss: 0.0592 - accuracy: 0.9948 - val_loss: 2.2555 - val_accuracy: 0.7148 - 95s/epoch - 230ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.89644\n",
      "413/413 - 82s - loss: 0.0648 - accuracy: 0.9956 - val_loss: 1.7748 - val_accuracy: 0.7517 - 82s/epoch - 198ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.89644\n",
      "413/413 - 89s - loss: 0.0691 - accuracy: 0.9938 - val_loss: 2.3778 - val_accuracy: 0.7563 - 89s/epoch - 216ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0715 - accuracy: 0.9920 - val_loss: 1.6408 - val_accuracy: 0.6848 - 76s/epoch - 183ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0704 - accuracy: 0.9941 - val_loss: 1.6988 - val_accuracy: 0.7348 - 77s/epoch - 186ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.89644\n",
      "413/413 - 80s - loss: 0.0621 - accuracy: 0.9955 - val_loss: 2.2948 - val_accuracy: 0.7423 - 80s/epoch - 195ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0786 - accuracy: 0.9919 - val_loss: 1.2416 - val_accuracy: 0.7544 - 69s/epoch - 167ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.89644\n",
      "413/413 - 61s - loss: 0.0659 - accuracy: 0.9939 - val_loss: 2.2236 - val_accuracy: 0.7519 - 61s/epoch - 148ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.89644\n",
      "413/413 - 59s - loss: 0.0705 - accuracy: 0.9927 - val_loss: 1.7958 - val_accuracy: 0.6606 - 59s/epoch - 143ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.89644\n",
      "413/413 - 66s - loss: 0.0741 - accuracy: 0.9928 - val_loss: 2.3693 - val_accuracy: 0.7535 - 66s/epoch - 160ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0795 - accuracy: 0.9919 - val_loss: 1.5378 - val_accuracy: 0.6654 - 71s/epoch - 171ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0707 - accuracy: 0.9936 - val_loss: 2.9853 - val_accuracy: 0.6983 - 69s/epoch - 168ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0660 - accuracy: 0.9944 - val_loss: 2.0045 - val_accuracy: 0.7075 - 69s/epoch - 168ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0714 - accuracy: 0.9930 - val_loss: 1.9970 - val_accuracy: 0.7415 - 73s/epoch - 178ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0643 - accuracy: 0.9952 - val_loss: 1.9333 - val_accuracy: 0.7452 - 69s/epoch - 167ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0655 - accuracy: 0.9954 - val_loss: 1.8238 - val_accuracy: 0.7219 - 69s/epoch - 167ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0659 - accuracy: 0.9945 - val_loss: 2.0743 - val_accuracy: 0.7396 - 69s/epoch - 168ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0487 - accuracy: 0.9964 - val_loss: 2.6439 - val_accuracy: 0.7456 - 70s/epoch - 169ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0489 - accuracy: 0.9961 - val_loss: 2.1629 - val_accuracy: 0.7106 - 68s/epoch - 166ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.89644\n",
      "413/413 - 67s - loss: 0.0893 - accuracy: 0.9902 - val_loss: 2.1090 - val_accuracy: 0.6877 - 67s/epoch - 161ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.89644\n",
      "413/413 - 59s - loss: 0.0583 - accuracy: 0.9954 - val_loss: 1.6961 - val_accuracy: 0.6948 - 59s/epoch - 143ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.89644\n",
      "413/413 - 59s - loss: 0.0830 - accuracy: 0.9920 - val_loss: 2.5571 - val_accuracy: 0.7552 - 59s/epoch - 144ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0769 - accuracy: 0.9940 - val_loss: 2.0674 - val_accuracy: 0.7096 - 69s/epoch - 167ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0785 - accuracy: 0.9926 - val_loss: 1.8964 - val_accuracy: 0.6319 - 70s/epoch - 169ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0565 - accuracy: 0.9965 - val_loss: 2.4831 - val_accuracy: 0.6658 - 69s/epoch - 168ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0587 - accuracy: 0.9951 - val_loss: 1.6538 - val_accuracy: 0.7275 - 69s/epoch - 168ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.0483 - accuracy: 0.9965 - val_loss: 2.0584 - val_accuracy: 0.7088 - 79s/epoch - 190ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0735 - accuracy: 0.9914 - val_loss: 1.6116 - val_accuracy: 0.6925 - 70s/epoch - 169ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0543 - accuracy: 0.9964 - val_loss: 2.2992 - val_accuracy: 0.7254 - 69s/epoch - 167ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0785 - accuracy: 0.9923 - val_loss: 2.6353 - val_accuracy: 0.6446 - 69s/epoch - 166ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0750 - accuracy: 0.9930 - val_loss: 2.3140 - val_accuracy: 0.6762 - 72s/epoch - 175ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0562 - accuracy: 0.9948 - val_loss: 2.7225 - val_accuracy: 0.7173 - 69s/epoch - 166ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0699 - accuracy: 0.9933 - val_loss: 2.1751 - val_accuracy: 0.7090 - 69s/epoch - 167ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0596 - accuracy: 0.9952 - val_loss: 2.2710 - val_accuracy: 0.6525 - 70s/epoch - 169ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0557 - accuracy: 0.9951 - val_loss: 2.2601 - val_accuracy: 0.6435 - 71s/epoch - 172ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0647 - accuracy: 0.9945 - val_loss: 1.3391 - val_accuracy: 0.7433 - 71s/epoch - 171ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.89644\n",
      "413/413 - 80s - loss: 0.0656 - accuracy: 0.9931 - val_loss: 1.9447 - val_accuracy: 0.7367 - 80s/epoch - 195ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.0768 - accuracy: 0.9917 - val_loss: 1.9671 - val_accuracy: 0.7450 - 79s/epoch - 190ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0599 - accuracy: 0.9956 - val_loss: 2.5069 - val_accuracy: 0.6706 - 75s/epoch - 182ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0585 - accuracy: 0.9956 - val_loss: 2.2193 - val_accuracy: 0.7258 - 72s/epoch - 174ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0642 - accuracy: 0.9934 - val_loss: 2.6977 - val_accuracy: 0.7335 - 73s/epoch - 176ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0698 - accuracy: 0.9933 - val_loss: 1.7963 - val_accuracy: 0.7119 - 69s/epoch - 166ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0486 - accuracy: 0.9972 - val_loss: 2.2228 - val_accuracy: 0.6979 - 71s/epoch - 171ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0630 - accuracy: 0.9942 - val_loss: 2.2838 - val_accuracy: 0.7344 - 69s/epoch - 167ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0792 - accuracy: 0.9921 - val_loss: 1.2031 - val_accuracy: 0.6717 - 69s/epoch - 166ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0570 - accuracy: 0.9958 - val_loss: 2.3090 - val_accuracy: 0.7444 - 70s/epoch - 168ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0799 - accuracy: 0.9920 - val_loss: 1.6740 - val_accuracy: 0.6733 - 69s/epoch - 166ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0568 - accuracy: 0.9961 - val_loss: 1.4183 - val_accuracy: 0.6804 - 69s/epoch - 167ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0528 - accuracy: 0.9961 - val_loss: 3.0318 - val_accuracy: 0.6923 - 68s/epoch - 166ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0573 - accuracy: 0.9954 - val_loss: 1.9430 - val_accuracy: 0.6477 - 74s/epoch - 179ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.89644\n",
      "413/413 - 87s - loss: 0.0892 - accuracy: 0.9905 - val_loss: 1.4868 - val_accuracy: 0.7492 - 87s/epoch - 210ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0712 - accuracy: 0.9934 - val_loss: 2.2983 - val_accuracy: 0.7073 - 77s/epoch - 186ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0454 - accuracy: 0.9973 - val_loss: 2.7364 - val_accuracy: 0.7040 - 71s/epoch - 172ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.89644\n",
      "413/413 - 80s - loss: 0.0663 - accuracy: 0.9933 - val_loss: 1.8902 - val_accuracy: 0.6975 - 80s/epoch - 194ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.89644\n",
      "413/413 - 80s - loss: 0.0812 - accuracy: 0.9915 - val_loss: 2.0947 - val_accuracy: 0.7227 - 80s/epoch - 194ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.89644\n",
      "413/413 - 86s - loss: 0.0609 - accuracy: 0.9953 - val_loss: 2.1848 - val_accuracy: 0.7579 - 86s/epoch - 208ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0531 - accuracy: 0.9957 - val_loss: 2.3832 - val_accuracy: 0.7475 - 73s/epoch - 177ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0500 - accuracy: 0.9970 - val_loss: 1.8596 - val_accuracy: 0.7465 - 68s/epoch - 165ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0671 - accuracy: 0.9939 - val_loss: 2.3148 - val_accuracy: 0.6981 - 75s/epoch - 181ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0800 - accuracy: 0.9917 - val_loss: 1.7404 - val_accuracy: 0.7300 - 69s/epoch - 167ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0462 - accuracy: 0.9966 - val_loss: 1.3822 - val_accuracy: 0.6954 - 71s/epoch - 171ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0711 - accuracy: 0.9930 - val_loss: 1.6273 - val_accuracy: 0.7354 - 68s/epoch - 166ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0551 - accuracy: 0.9954 - val_loss: 2.1611 - val_accuracy: 0.7352 - 68s/epoch - 165ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0591 - accuracy: 0.9952 - val_loss: 2.3063 - val_accuracy: 0.6981 - 71s/epoch - 173ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0632 - accuracy: 0.9948 - val_loss: 2.4113 - val_accuracy: 0.7044 - 73s/epoch - 177ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0667 - accuracy: 0.9930 - val_loss: 1.5423 - val_accuracy: 0.7485 - 73s/epoch - 176ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.89644\n",
      "413/413 - 78s - loss: 0.0618 - accuracy: 0.9946 - val_loss: 2.0261 - val_accuracy: 0.7206 - 78s/epoch - 190ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0636 - accuracy: 0.9945 - val_loss: 2.7534 - val_accuracy: 0.7135 - 75s/epoch - 182ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0530 - accuracy: 0.9956 - val_loss: 1.2738 - val_accuracy: 0.6746 - 73s/epoch - 178ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0558 - accuracy: 0.9956 - val_loss: 1.4782 - val_accuracy: 0.6744 - 76s/epoch - 184ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.89644\n",
      "413/413 - 81s - loss: 0.0434 - accuracy: 0.9973 - val_loss: 2.6410 - val_accuracy: 0.7060 - 81s/epoch - 196ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.89644\n",
      "413/413 - 86s - loss: 0.0832 - accuracy: 0.9908 - val_loss: 2.4616 - val_accuracy: 0.6637 - 86s/epoch - 209ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0632 - accuracy: 0.9938 - val_loss: 1.7638 - val_accuracy: 0.6919 - 72s/epoch - 174ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0456 - accuracy: 0.9973 - val_loss: 2.5140 - val_accuracy: 0.7235 - 76s/epoch - 184ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0677 - accuracy: 0.9936 - val_loss: 2.3168 - val_accuracy: 0.7017 - 74s/epoch - 180ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0613 - accuracy: 0.9948 - val_loss: 1.8633 - val_accuracy: 0.7398 - 75s/epoch - 182ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0657 - accuracy: 0.9956 - val_loss: 2.0644 - val_accuracy: 0.7183 - 74s/epoch - 179ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0554 - accuracy: 0.9961 - val_loss: 1.8445 - val_accuracy: 0.7142 - 74s/epoch - 180ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0562 - accuracy: 0.9961 - val_loss: 2.1903 - val_accuracy: 0.7327 - 77s/epoch - 186ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0548 - accuracy: 0.9959 - val_loss: 2.1869 - val_accuracy: 0.7421 - 72s/epoch - 174ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.89644\n",
      "413/413 - 73s - loss: 0.0608 - accuracy: 0.9949 - val_loss: 1.3741 - val_accuracy: 0.7019 - 73s/epoch - 177ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0720 - accuracy: 0.9929 - val_loss: 2.9830 - val_accuracy: 0.7056 - 70s/epoch - 169ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0553 - accuracy: 0.9957 - val_loss: 1.9047 - val_accuracy: 0.7188 - 69s/epoch - 168ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.89644\n",
      "413/413 - 83s - loss: 0.0475 - accuracy: 0.9973 - val_loss: 1.8191 - val_accuracy: 0.7350 - 83s/epoch - 200ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0512 - accuracy: 0.9961 - val_loss: 2.0641 - val_accuracy: 0.6979 - 72s/epoch - 174ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0635 - accuracy: 0.9945 - val_loss: 1.7948 - val_accuracy: 0.6735 - 74s/epoch - 179ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0671 - accuracy: 0.9942 - val_loss: 2.8808 - val_accuracy: 0.7067 - 69s/epoch - 166ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0570 - accuracy: 0.9945 - val_loss: 1.6485 - val_accuracy: 0.7304 - 70s/epoch - 169ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0504 - accuracy: 0.9965 - val_loss: 2.2186 - val_accuracy: 0.7658 - 70s/epoch - 169ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0561 - accuracy: 0.9959 - val_loss: 2.1216 - val_accuracy: 0.7398 - 69s/epoch - 167ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0661 - accuracy: 0.9921 - val_loss: 1.6956 - val_accuracy: 0.6802 - 68s/epoch - 166ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0540 - accuracy: 0.9955 - val_loss: 2.1369 - val_accuracy: 0.6956 - 77s/epoch - 186ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0720 - accuracy: 0.9930 - val_loss: 1.7898 - val_accuracy: 0.7365 - 75s/epoch - 182ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0421 - accuracy: 0.9969 - val_loss: 3.1407 - val_accuracy: 0.6927 - 69s/epoch - 166ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0582 - accuracy: 0.9952 - val_loss: 1.6248 - val_accuracy: 0.7250 - 69s/epoch - 166ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0610 - accuracy: 0.9948 - val_loss: 1.3761 - val_accuracy: 0.7340 - 69s/epoch - 167ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0626 - accuracy: 0.9935 - val_loss: 2.4897 - val_accuracy: 0.6525 - 72s/epoch - 173ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0535 - accuracy: 0.9962 - val_loss: 2.0900 - val_accuracy: 0.7412 - 68s/epoch - 166ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0525 - accuracy: 0.9952 - val_loss: 1.5052 - val_accuracy: 0.7427 - 68s/epoch - 166ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0757 - accuracy: 0.9934 - val_loss: 2.1672 - val_accuracy: 0.7425 - 69s/epoch - 167ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0488 - accuracy: 0.9963 - val_loss: 1.7488 - val_accuracy: 0.7713 - 71s/epoch - 171ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0553 - accuracy: 0.9950 - val_loss: 1.7410 - val_accuracy: 0.7058 - 69s/epoch - 167ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0706 - accuracy: 0.9948 - val_loss: 1.9012 - val_accuracy: 0.6942 - 70s/epoch - 168ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.89644\n",
      "413/413 - 76s - loss: 0.0548 - accuracy: 0.9957 - val_loss: 2.7550 - val_accuracy: 0.6910 - 76s/epoch - 185ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0489 - accuracy: 0.9962 - val_loss: 2.1456 - val_accuracy: 0.7015 - 75s/epoch - 182ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0838 - accuracy: 0.9917 - val_loss: 1.2025 - val_accuracy: 0.7027 - 69s/epoch - 167ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0525 - accuracy: 0.9972 - val_loss: 1.9002 - val_accuracy: 0.7373 - 68s/epoch - 164ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0562 - accuracy: 0.9955 - val_loss: 2.5061 - val_accuracy: 0.7285 - 75s/epoch - 182ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0495 - accuracy: 0.9961 - val_loss: 2.3961 - val_accuracy: 0.6958 - 75s/epoch - 183ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.89644\n",
      "413/413 - 75s - loss: 0.0596 - accuracy: 0.9943 - val_loss: 1.6365 - val_accuracy: 0.7535 - 75s/epoch - 183ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0545 - accuracy: 0.9963 - val_loss: 1.7459 - val_accuracy: 0.7577 - 69s/epoch - 167ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0481 - accuracy: 0.9964 - val_loss: 2.2056 - val_accuracy: 0.7052 - 68s/epoch - 165ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0716 - accuracy: 0.9938 - val_loss: 1.3493 - val_accuracy: 0.7467 - 68s/epoch - 165ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0644 - accuracy: 0.9950 - val_loss: 1.7106 - val_accuracy: 0.7298 - 74s/epoch - 179ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0726 - accuracy: 0.9930 - val_loss: 1.5560 - val_accuracy: 0.7487 - 68s/epoch - 165ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0640 - accuracy: 0.9942 - val_loss: 1.6768 - val_accuracy: 0.7354 - 69s/epoch - 167ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0501 - accuracy: 0.9959 - val_loss: 2.6279 - val_accuracy: 0.7621 - 69s/epoch - 166ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0699 - accuracy: 0.9936 - val_loss: 2.1146 - val_accuracy: 0.7269 - 72s/epoch - 175ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0584 - accuracy: 0.9946 - val_loss: 1.8723 - val_accuracy: 0.7640 - 69s/epoch - 166ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0503 - accuracy: 0.9965 - val_loss: 1.7769 - val_accuracy: 0.7415 - 68s/epoch - 164ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0633 - accuracy: 0.9940 - val_loss: 1.7120 - val_accuracy: 0.6871 - 69s/epoch - 167ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0606 - accuracy: 0.9951 - val_loss: 1.7759 - val_accuracy: 0.6648 - 72s/epoch - 175ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.89644\n",
      "413/413 - 74s - loss: 0.0534 - accuracy: 0.9967 - val_loss: 2.0676 - val_accuracy: 0.7479 - 74s/epoch - 179ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0594 - accuracy: 0.9942 - val_loss: 2.4520 - val_accuracy: 0.7088 - 68s/epoch - 164ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0564 - accuracy: 0.9955 - val_loss: 2.3236 - val_accuracy: 0.7125 - 71s/epoch - 171ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.89644\n",
      "413/413 - 79s - loss: 0.0623 - accuracy: 0.9952 - val_loss: 2.3884 - val_accuracy: 0.7119 - 79s/epoch - 191ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0576 - accuracy: 0.9952 - val_loss: 1.9819 - val_accuracy: 0.7342 - 69s/epoch - 166ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0528 - accuracy: 0.9968 - val_loss: 2.1572 - val_accuracy: 0.6596 - 68s/epoch - 164ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0401 - accuracy: 0.9980 - val_loss: 2.1752 - val_accuracy: 0.6677 - 68s/epoch - 164ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0412 - accuracy: 0.9975 - val_loss: 2.3088 - val_accuracy: 0.7169 - 68s/epoch - 164ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0518 - accuracy: 0.9960 - val_loss: 2.4895 - val_accuracy: 0.7262 - 69s/epoch - 167ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0502 - accuracy: 0.9959 - val_loss: 2.9259 - val_accuracy: 0.7171 - 68s/epoch - 164ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.89644\n",
      "413/413 - 67s - loss: 0.0630 - accuracy: 0.9946 - val_loss: 2.3634 - val_accuracy: 0.6898 - 67s/epoch - 163ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0555 - accuracy: 0.9960 - val_loss: 2.0161 - val_accuracy: 0.7469 - 68s/epoch - 165ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0635 - accuracy: 0.9944 - val_loss: 2.0609 - val_accuracy: 0.7437 - 68s/epoch - 164ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0512 - accuracy: 0.9964 - val_loss: 1.6284 - val_accuracy: 0.7210 - 68s/epoch - 166ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0633 - accuracy: 0.9944 - val_loss: 1.4429 - val_accuracy: 0.7146 - 68s/epoch - 164ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0563 - accuracy: 0.9954 - val_loss: 1.8613 - val_accuracy: 0.6175 - 68s/epoch - 163ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0583 - accuracy: 0.9953 - val_loss: 3.6425 - val_accuracy: 0.6385 - 68s/epoch - 164ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0458 - accuracy: 0.9969 - val_loss: 2.5192 - val_accuracy: 0.7000 - 68s/epoch - 165ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.89644\n",
      "413/413 - 67s - loss: 0.0405 - accuracy: 0.9979 - val_loss: 2.1817 - val_accuracy: 0.6517 - 67s/epoch - 163ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0449 - accuracy: 0.9960 - val_loss: 1.6768 - val_accuracy: 0.7038 - 69s/epoch - 166ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0520 - accuracy: 0.9960 - val_loss: 2.0417 - val_accuracy: 0.7331 - 68s/epoch - 164ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0669 - accuracy: 0.9932 - val_loss: 2.0268 - val_accuracy: 0.7171 - 68s/epoch - 164ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0475 - accuracy: 0.9970 - val_loss: 2.9353 - val_accuracy: 0.7023 - 68s/epoch - 164ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.89644\n",
      "413/413 - 67s - loss: 0.0739 - accuracy: 0.9943 - val_loss: 2.1493 - val_accuracy: 0.7371 - 67s/epoch - 163ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.89644\n",
      "413/413 - 67s - loss: 0.0480 - accuracy: 0.9972 - val_loss: 1.5549 - val_accuracy: 0.6994 - 67s/epoch - 163ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0586 - accuracy: 0.9950 - val_loss: 2.3516 - val_accuracy: 0.7237 - 68s/epoch - 166ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0499 - accuracy: 0.9961 - val_loss: 1.6759 - val_accuracy: 0.6635 - 68s/epoch - 164ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0688 - accuracy: 0.9934 - val_loss: 2.4432 - val_accuracy: 0.6515 - 68s/epoch - 165ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0523 - accuracy: 0.9967 - val_loss: 2.3233 - val_accuracy: 0.6748 - 68s/epoch - 164ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0620 - accuracy: 0.9942 - val_loss: 2.5322 - val_accuracy: 0.7115 - 68s/epoch - 165ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0552 - accuracy: 0.9959 - val_loss: 1.6684 - val_accuracy: 0.7090 - 68s/epoch - 165ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0612 - accuracy: 0.9956 - val_loss: 2.5469 - val_accuracy: 0.6856 - 70s/epoch - 170ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0542 - accuracy: 0.9955 - val_loss: 2.8952 - val_accuracy: 0.6944 - 68s/epoch - 165ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0522 - accuracy: 0.9963 - val_loss: 2.1248 - val_accuracy: 0.6779 - 68s/epoch - 164ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.89644\n",
      "413/413 - 71s - loss: 0.0615 - accuracy: 0.9945 - val_loss: 1.9476 - val_accuracy: 0.7252 - 71s/epoch - 173ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0372 - accuracy: 0.9986 - val_loss: 2.7592 - val_accuracy: 0.7169 - 70s/epoch - 170ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0414 - accuracy: 0.9968 - val_loss: 1.5225 - val_accuracy: 0.6900 - 68s/epoch - 165ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0734 - accuracy: 0.9941 - val_loss: 1.5298 - val_accuracy: 0.7046 - 68s/epoch - 165ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0642 - accuracy: 0.9951 - val_loss: 1.4173 - val_accuracy: 0.7533 - 68s/epoch - 165ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0459 - accuracy: 0.9970 - val_loss: 1.8238 - val_accuracy: 0.7608 - 70s/epoch - 170ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0614 - accuracy: 0.9937 - val_loss: 2.7386 - val_accuracy: 0.6696 - 69s/epoch - 167ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0628 - accuracy: 0.9955 - val_loss: 1.5154 - val_accuracy: 0.6919 - 68s/epoch - 165ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0640 - accuracy: 0.9952 - val_loss: 1.8355 - val_accuracy: 0.7606 - 68s/epoch - 165ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.89644\n",
      "413/413 - 70s - loss: 0.0432 - accuracy: 0.9971 - val_loss: 2.7813 - val_accuracy: 0.7158 - 70s/epoch - 168ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0826 - accuracy: 0.9926 - val_loss: 2.5265 - val_accuracy: 0.7219 - 68s/epoch - 165ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0497 - accuracy: 0.9976 - val_loss: 0.9204 - val_accuracy: 0.6940 - 68s/epoch - 165ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0477 - accuracy: 0.9968 - val_loss: 1.4331 - val_accuracy: 0.7050 - 68s/epoch - 165ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0432 - accuracy: 0.9971 - val_loss: 3.1963 - val_accuracy: 0.6940 - 68s/epoch - 165ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0500 - accuracy: 0.9964 - val_loss: 2.9860 - val_accuracy: 0.6933 - 68s/epoch - 165ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0470 - accuracy: 0.9969 - val_loss: 2.7484 - val_accuracy: 0.6756 - 68s/epoch - 166ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0476 - accuracy: 0.9966 - val_loss: 1.7908 - val_accuracy: 0.7262 - 68s/epoch - 164ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0550 - accuracy: 0.9956 - val_loss: 3.6525 - val_accuracy: 0.6923 - 69s/epoch - 166ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.89644\n",
      "413/413 - 77s - loss: 0.0532 - accuracy: 0.9961 - val_loss: 2.5456 - val_accuracy: 0.7092 - 77s/epoch - 185ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0481 - accuracy: 0.9960 - val_loss: 1.5534 - val_accuracy: 0.6775 - 69s/epoch - 167ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0467 - accuracy: 0.9976 - val_loss: 2.0179 - val_accuracy: 0.6775 - 68s/epoch - 165ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0471 - accuracy: 0.9965 - val_loss: 1.6592 - val_accuracy: 0.6177 - 69s/epoch - 167ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0685 - accuracy: 0.9929 - val_loss: 2.2467 - val_accuracy: 0.7069 - 69s/epoch - 167ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0631 - accuracy: 0.9953 - val_loss: 3.3278 - val_accuracy: 0.7021 - 69s/epoch - 168ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0482 - accuracy: 0.9970 - val_loss: 2.3607 - val_accuracy: 0.7575 - 68s/epoch - 165ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0499 - accuracy: 0.9966 - val_loss: 2.2573 - val_accuracy: 0.7558 - 68s/epoch - 166ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.89644\n",
      "413/413 - 72s - loss: 0.0509 - accuracy: 0.9959 - val_loss: 1.9987 - val_accuracy: 0.7523 - 72s/epoch - 174ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0515 - accuracy: 0.9948 - val_loss: 1.4916 - val_accuracy: 0.6967 - 69s/epoch - 166ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0631 - accuracy: 0.9955 - val_loss: 2.6083 - val_accuracy: 0.7335 - 68s/epoch - 165ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0386 - accuracy: 0.9981 - val_loss: 2.2205 - val_accuracy: 0.6679 - 68s/epoch - 164ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0570 - accuracy: 0.9953 - val_loss: 1.8621 - val_accuracy: 0.6629 - 69s/epoch - 166ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.89644\n",
      "413/413 - 69s - loss: 0.0562 - accuracy: 0.9953 - val_loss: 2.6878 - val_accuracy: 0.6760 - 69s/epoch - 167ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0425 - accuracy: 0.9969 - val_loss: 2.9067 - val_accuracy: 0.6946 - 68s/epoch - 165ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.89644\n",
      "413/413 - 68s - loss: 0.0620 - accuracy: 0.9942 - val_loss: 2.7623 - val_accuracy: 0.6725 - 68s/epoch - 165ms/step\n",
      "179/179 [==============================] - 7s 40ms/step\n",
      "Classification accuracy: 0.506620 \n"
     ]
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39211366 0.6078864 ]\n",
      " [0.57004416 0.42995587]\n",
      " [0.7428666  0.25713348]\n",
      " ...\n",
      " [0.98333436 0.01666567]\n",
      " [0.9812373  0.0187627 ]\n",
      " [0.9636203  0.03637971]]\n",
      "[1 0 0 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[1719 1581]\n",
      " [1370 1030]]\n",
      "[48.23 55.65 39.45]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for TSGL on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)\n",
    "\n",
    "plot_confusion_matrix(preds_TSGL, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for TSGL on ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.44949, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 21s - loss: 0.6007 - accuracy: 0.7744 - val_loss: 1.4495 - val_accuracy: 0.6979 - 21s/epoch - 52ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.1365 - accuracy: 0.9686 - val_loss: 1.9505 - val_accuracy: 0.6521 - 19s/epoch - 46ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0756 - accuracy: 0.9820 - val_loss: 3.0483 - val_accuracy: 0.6846 - 19s/epoch - 46ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0587 - accuracy: 0.9854 - val_loss: 2.7182 - val_accuracy: 0.6590 - 19s/epoch - 45ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0463 - accuracy: 0.9889 - val_loss: 2.1040 - val_accuracy: 0.6637 - 19s/epoch - 46ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0436 - accuracy: 0.9891 - val_loss: 2.6279 - val_accuracy: 0.6590 - 19s/epoch - 46ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0375 - accuracy: 0.9904 - val_loss: 2.8088 - val_accuracy: 0.6550 - 19s/epoch - 46ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0256 - accuracy: 0.9931 - val_loss: 2.3826 - val_accuracy: 0.6435 - 19s/epoch - 46ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0440 - accuracy: 0.9902 - val_loss: 2.0223 - val_accuracy: 0.6946 - 19s/epoch - 46ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0260 - accuracy: 0.9943 - val_loss: 3.2365 - val_accuracy: 0.6517 - 19s/epoch - 45ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0268 - accuracy: 0.9930 - val_loss: 4.1233 - val_accuracy: 0.6790 - 19s/epoch - 46ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0302 - accuracy: 0.9930 - val_loss: 3.2365 - val_accuracy: 0.6662 - 19s/epoch - 46ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0191 - accuracy: 0.9963 - val_loss: 2.2904 - val_accuracy: 0.6585 - 19s/epoch - 46ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0206 - accuracy: 0.9948 - val_loss: 2.3633 - val_accuracy: 0.6619 - 19s/epoch - 46ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0214 - accuracy: 0.9946 - val_loss: 3.7907 - val_accuracy: 0.6510 - 19s/epoch - 46ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0250 - accuracy: 0.9946 - val_loss: 2.5000 - val_accuracy: 0.6881 - 19s/epoch - 46ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0236 - accuracy: 0.9946 - val_loss: 1.8077 - val_accuracy: 0.6881 - 19s/epoch - 46ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0276 - accuracy: 0.9942 - val_loss: 3.2646 - val_accuracy: 0.7010 - 19s/epoch - 46ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0207 - accuracy: 0.9947 - val_loss: 2.1052 - val_accuracy: 0.7442 - 19s/epoch - 46ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0185 - accuracy: 0.9954 - val_loss: 2.1641 - val_accuracy: 0.7188 - 19s/epoch - 45ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0220 - accuracy: 0.9951 - val_loss: 2.7183 - val_accuracy: 0.6558 - 19s/epoch - 46ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0166 - accuracy: 0.9959 - val_loss: 1.9042 - val_accuracy: 0.6910 - 20s/epoch - 48ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.44949\n",
      "413/413 - 21s - loss: 0.0116 - accuracy: 0.9969 - val_loss: 3.1667 - val_accuracy: 0.6650 - 21s/epoch - 50ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0106 - accuracy: 0.9970 - val_loss: 3.6131 - val_accuracy: 0.6531 - 19s/epoch - 47ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0277 - accuracy: 0.9946 - val_loss: 3.5203 - val_accuracy: 0.6623 - 19s/epoch - 46ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0152 - accuracy: 0.9965 - val_loss: 3.3439 - val_accuracy: 0.6600 - 19s/epoch - 47ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0079 - accuracy: 0.9981 - val_loss: 3.1679 - val_accuracy: 0.7044 - 19s/epoch - 46ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0237 - accuracy: 0.9946 - val_loss: 2.2261 - val_accuracy: 0.6915 - 19s/epoch - 46ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0269 - accuracy: 0.9938 - val_loss: 1.9473 - val_accuracy: 0.7160 - 19s/epoch - 46ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0222 - accuracy: 0.9952 - val_loss: 2.9417 - val_accuracy: 0.6913 - 19s/epoch - 46ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0105 - accuracy: 0.9977 - val_loss: 3.1303 - val_accuracy: 0.7215 - 19s/epoch - 47ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0072 - accuracy: 0.9982 - val_loss: 3.5007 - val_accuracy: 0.6894 - 19s/epoch - 46ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0075 - accuracy: 0.9983 - val_loss: 3.1945 - val_accuracy: 0.6852 - 19s/epoch - 46ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0138 - accuracy: 0.9967 - val_loss: 2.5869 - val_accuracy: 0.7079 - 19s/epoch - 46ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0227 - accuracy: 0.9961 - val_loss: 2.4387 - val_accuracy: 0.6673 - 19s/epoch - 46ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0171 - accuracy: 0.9961 - val_loss: 3.4182 - val_accuracy: 0.6602 - 19s/epoch - 47ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0081 - accuracy: 0.9977 - val_loss: 2.8522 - val_accuracy: 0.6833 - 20s/epoch - 48ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0147 - accuracy: 0.9968 - val_loss: 3.0467 - val_accuracy: 0.6623 - 19s/epoch - 46ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0078 - accuracy: 0.9977 - val_loss: 2.8384 - val_accuracy: 0.6644 - 19s/epoch - 47ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0131 - accuracy: 0.9972 - val_loss: 2.3983 - val_accuracy: 0.6467 - 19s/epoch - 46ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0074 - accuracy: 0.9979 - val_loss: 3.0770 - val_accuracy: 0.7190 - 19s/epoch - 46ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0049 - accuracy: 0.9986 - val_loss: 2.6267 - val_accuracy: 0.6808 - 19s/epoch - 46ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0063 - accuracy: 0.9984 - val_loss: 2.5646 - val_accuracy: 0.7448 - 19s/epoch - 46ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0317 - accuracy: 0.9946 - val_loss: 2.0369 - val_accuracy: 0.6825 - 19s/epoch - 46ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0067 - accuracy: 0.9980 - val_loss: 3.2894 - val_accuracy: 0.6583 - 19s/epoch - 46ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0056 - accuracy: 0.9986 - val_loss: 3.9111 - val_accuracy: 0.6815 - 19s/epoch - 46ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0081 - accuracy: 0.9977 - val_loss: 2.8919 - val_accuracy: 0.6710 - 19s/epoch - 46ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0168 - accuracy: 0.9966 - val_loss: 3.7165 - val_accuracy: 0.6558 - 19s/epoch - 46ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0105 - accuracy: 0.9971 - val_loss: 3.0493 - val_accuracy: 0.6687 - 20s/epoch - 47ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0064 - accuracy: 0.9983 - val_loss: 3.6250 - val_accuracy: 0.7175 - 20s/epoch - 48ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0052 - accuracy: 0.9986 - val_loss: 2.1576 - val_accuracy: 0.7733 - 19s/epoch - 47ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0052 - accuracy: 0.9987 - val_loss: 2.8112 - val_accuracy: 0.7179 - 19s/epoch - 47ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0099 - accuracy: 0.9983 - val_loss: 1.6402 - val_accuracy: 0.7390 - 20s/epoch - 48ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0241 - accuracy: 0.9954 - val_loss: 2.5727 - val_accuracy: 0.7271 - 19s/epoch - 47ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0032 - accuracy: 0.9993 - val_loss: 2.5496 - val_accuracy: 0.7013 - 19s/epoch - 46ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0051 - accuracy: 0.9987 - val_loss: 2.9418 - val_accuracy: 0.7065 - 20s/epoch - 47ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 3.6726 - val_accuracy: 0.7163 - 19s/epoch - 46ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0089 - accuracy: 0.9981 - val_loss: 2.7794 - val_accuracy: 0.7152 - 19s/epoch - 46ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0133 - accuracy: 0.9977 - val_loss: 2.3387 - val_accuracy: 0.7483 - 19s/epoch - 46ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0178 - accuracy: 0.9961 - val_loss: 2.1129 - val_accuracy: 0.7375 - 19s/epoch - 47ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9995 - val_loss: 3.1847 - val_accuracy: 0.7067 - 19s/epoch - 47ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0062 - accuracy: 0.9984 - val_loss: 3.0376 - val_accuracy: 0.7033 - 19s/epoch - 46ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0148 - accuracy: 0.9964 - val_loss: 2.5184 - val_accuracy: 0.7302 - 19s/epoch - 46ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0161 - accuracy: 0.9975 - val_loss: 2.7459 - val_accuracy: 0.7329 - 19s/epoch - 46ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0054 - accuracy: 0.9986 - val_loss: 2.2646 - val_accuracy: 0.7260 - 19s/epoch - 46ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0057 - accuracy: 0.9986 - val_loss: 2.7543 - val_accuracy: 0.7748 - 19s/epoch - 47ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0051 - accuracy: 0.9986 - val_loss: 2.2038 - val_accuracy: 0.7408 - 19s/epoch - 46ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 3.1344 - val_accuracy: 0.7121 - 19s/epoch - 47ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0050 - accuracy: 0.9986 - val_loss: 2.7175 - val_accuracy: 0.7167 - 20s/epoch - 48ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0111 - accuracy: 0.9974 - val_loss: 2.5901 - val_accuracy: 0.6990 - 19s/epoch - 46ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0044 - accuracy: 0.9988 - val_loss: 3.8368 - val_accuracy: 0.7063 - 19s/epoch - 46ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0270 - accuracy: 0.9955 - val_loss: 2.1897 - val_accuracy: 0.7427 - 19s/epoch - 46ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 3.1448 - val_accuracy: 0.6900 - 19s/epoch - 46ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 2.8438 - val_accuracy: 0.6840 - 19s/epoch - 46ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0045 - accuracy: 0.9985 - val_loss: 3.6805 - val_accuracy: 0.7004 - 19s/epoch - 46ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 3.2330 - val_accuracy: 0.6906 - 19s/epoch - 46ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0052 - accuracy: 0.9987 - val_loss: 3.0095 - val_accuracy: 0.7192 - 19s/epoch - 47ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.44949\n",
      "413/413 - 21s - loss: 0.0141 - accuracy: 0.9965 - val_loss: 2.7011 - val_accuracy: 0.7323 - 21s/epoch - 50ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0020 - accuracy: 0.9993 - val_loss: 3.0613 - val_accuracy: 0.7456 - 19s/epoch - 46ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0099 - accuracy: 0.9977 - val_loss: 2.6858 - val_accuracy: 0.6904 - 19s/epoch - 46ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0098 - accuracy: 0.9977 - val_loss: 2.7336 - val_accuracy: 0.7713 - 19s/epoch - 46ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0065 - accuracy: 0.9985 - val_loss: 2.8383 - val_accuracy: 0.7223 - 19s/epoch - 46ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0059 - accuracy: 0.9984 - val_loss: 1.8870 - val_accuracy: 0.7381 - 19s/epoch - 47ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0027 - accuracy: 0.9994 - val_loss: 3.1082 - val_accuracy: 0.7265 - 19s/epoch - 47ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 3.4997 - val_accuracy: 0.7167 - 19s/epoch - 47ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0076 - accuracy: 0.9980 - val_loss: 2.9093 - val_accuracy: 0.7379 - 20s/epoch - 48ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0028 - accuracy: 0.9994 - val_loss: 3.3258 - val_accuracy: 0.7404 - 20s/epoch - 49ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0079 - accuracy: 0.9978 - val_loss: 2.8345 - val_accuracy: 0.7029 - 19s/epoch - 46ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0089 - accuracy: 0.9978 - val_loss: 2.5213 - val_accuracy: 0.7854 - 19s/epoch - 46ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0123 - accuracy: 0.9973 - val_loss: 3.1745 - val_accuracy: 0.7829 - 19s/epoch - 46ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 3.3688 - val_accuracy: 0.7177 - 19s/epoch - 47ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0198 - accuracy: 0.9957 - val_loss: 3.3082 - val_accuracy: 0.7148 - 19s/epoch - 46ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0093 - accuracy: 0.9982 - val_loss: 3.0211 - val_accuracy: 0.7173 - 19s/epoch - 46ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0077 - accuracy: 0.9983 - val_loss: 3.0733 - val_accuracy: 0.7231 - 19s/epoch - 46ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0020 - accuracy: 0.9993 - val_loss: 2.8796 - val_accuracy: 0.7021 - 19s/epoch - 46ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 3.0377 - val_accuracy: 0.7231 - 19s/epoch - 47ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0044 - accuracy: 0.9987 - val_loss: 3.7423 - val_accuracy: 0.7538 - 19s/epoch - 47ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0136 - accuracy: 0.9969 - val_loss: 2.8242 - val_accuracy: 0.7706 - 19s/epoch - 46ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0098 - accuracy: 0.9982 - val_loss: 2.5605 - val_accuracy: 0.7644 - 20s/epoch - 49ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0047 - accuracy: 0.9987 - val_loss: 3.5426 - val_accuracy: 0.7465 - 19s/epoch - 47ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 3.4807 - val_accuracy: 0.7335 - 19s/epoch - 47ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0061 - accuracy: 0.9989 - val_loss: 3.0448 - val_accuracy: 0.7706 - 19s/epoch - 46ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 3.8102 - val_accuracy: 0.7254 - 19s/epoch - 47ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 3.2738 - val_accuracy: 0.7610 - 19s/epoch - 46ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0246 - accuracy: 0.9965 - val_loss: 2.2139 - val_accuracy: 0.7829 - 19s/epoch - 46ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0034 - accuracy: 0.9989 - val_loss: 3.0124 - val_accuracy: 0.7648 - 19s/epoch - 47ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0054 - accuracy: 0.9983 - val_loss: 2.9322 - val_accuracy: 0.7448 - 19s/epoch - 47ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0063 - accuracy: 0.9983 - val_loss: 3.2909 - val_accuracy: 0.7240 - 19s/epoch - 46ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0026 - accuracy: 0.9995 - val_loss: 2.6894 - val_accuracy: 0.7906 - 19s/epoch - 47ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0087 - accuracy: 0.9983 - val_loss: 3.4546 - val_accuracy: 0.6660 - 19s/epoch - 46ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0032 - accuracy: 0.9993 - val_loss: 3.0734 - val_accuracy: 0.7219 - 19s/epoch - 47ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0065 - accuracy: 0.9984 - val_loss: 2.5135 - val_accuracy: 0.7154 - 19s/epoch - 46ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0093 - accuracy: 0.9980 - val_loss: 3.1144 - val_accuracy: 0.7150 - 19s/epoch - 46ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0188 - accuracy: 0.9964 - val_loss: 2.7628 - val_accuracy: 0.7029 - 19s/epoch - 46ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 2.5851 - val_accuracy: 0.7177 - 19s/epoch - 47ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0028 - accuracy: 0.9995 - val_loss: 3.4587 - val_accuracy: 0.7144 - 20s/epoch - 48ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0051 - accuracy: 0.9988 - val_loss: 3.0770 - val_accuracy: 0.7206 - 19s/epoch - 47ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0079 - accuracy: 0.9983 - val_loss: 2.4280 - val_accuracy: 0.8073 - 19s/epoch - 46ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0067 - accuracy: 0.9982 - val_loss: 3.1097 - val_accuracy: 0.6862 - 19s/epoch - 47ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 3.4025 - val_accuracy: 0.7163 - 19s/epoch - 47ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0048 - accuracy: 0.9986 - val_loss: 2.6349 - val_accuracy: 0.7102 - 19s/epoch - 46ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0051 - accuracy: 0.9985 - val_loss: 2.5148 - val_accuracy: 0.7502 - 19s/epoch - 47ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0188 - accuracy: 0.9973 - val_loss: 2.0433 - val_accuracy: 0.7792 - 19s/epoch - 47ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0066 - accuracy: 0.9983 - val_loss: 3.2310 - val_accuracy: 0.7229 - 19s/epoch - 46ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0040 - accuracy: 0.9991 - val_loss: 3.3013 - val_accuracy: 0.7265 - 19s/epoch - 46ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9992 - val_loss: 3.7530 - val_accuracy: 0.7552 - 19s/epoch - 47ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0063 - accuracy: 0.9982 - val_loss: 2.5475 - val_accuracy: 0.7146 - 19s/epoch - 46ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0051 - accuracy: 0.9986 - val_loss: 3.2996 - val_accuracy: 0.6792 - 19s/epoch - 47ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0051 - accuracy: 0.9989 - val_loss: 2.8693 - val_accuracy: 0.7408 - 19s/epoch - 47ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0071 - accuracy: 0.9986 - val_loss: 2.4009 - val_accuracy: 0.7658 - 19s/epoch - 47ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0039 - accuracy: 0.9989 - val_loss: 3.0634 - val_accuracy: 0.7742 - 19s/epoch - 47ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0052 - accuracy: 0.9986 - val_loss: 2.4618 - val_accuracy: 0.7927 - 19s/epoch - 46ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0163 - accuracy: 0.9974 - val_loss: 2.4816 - val_accuracy: 0.7487 - 19s/epoch - 46ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0095 - accuracy: 0.9984 - val_loss: 3.0936 - val_accuracy: 0.7150 - 19s/epoch - 46ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 3.6982 - val_accuracy: 0.7342 - 19s/epoch - 46ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 3.3088 - val_accuracy: 0.7227 - 19s/epoch - 46ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0079 - accuracy: 0.9982 - val_loss: 1.9077 - val_accuracy: 0.7283 - 19s/epoch - 47ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0041 - accuracy: 0.9987 - val_loss: 3.1741 - val_accuracy: 0.7240 - 19s/epoch - 46ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 2.6123 - val_accuracy: 0.7410 - 19s/epoch - 47ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 2.5950 - val_accuracy: 0.7485 - 19s/epoch - 46ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.9002 - val_accuracy: 0.7050 - 20s/epoch - 47ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0079 - accuracy: 0.9981 - val_loss: 2.2870 - val_accuracy: 0.7158 - 20s/epoch - 48ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0056 - accuracy: 0.9985 - val_loss: 1.8581 - val_accuracy: 0.8052 - 19s/epoch - 46ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0159 - accuracy: 0.9972 - val_loss: 2.2734 - val_accuracy: 0.7040 - 19s/epoch - 47ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0117 - accuracy: 0.9980 - val_loss: 2.9283 - val_accuracy: 0.7138 - 19s/epoch - 46ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0016 - accuracy: 0.9997 - val_loss: 3.3519 - val_accuracy: 0.6904 - 20s/epoch - 48ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 3.4438 - val_accuracy: 0.6742 - 19s/epoch - 47ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0048 - accuracy: 0.9987 - val_loss: 2.2121 - val_accuracy: 0.7044 - 19s/epoch - 47ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0078 - accuracy: 0.9983 - val_loss: 3.7017 - val_accuracy: 0.6744 - 19s/epoch - 47ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0102 - accuracy: 0.9977 - val_loss: 2.3855 - val_accuracy: 0.7448 - 20s/epoch - 48ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0042 - accuracy: 0.9986 - val_loss: 2.9451 - val_accuracy: 0.7298 - 19s/epoch - 47ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0020 - accuracy: 0.9996 - val_loss: 3.1819 - val_accuracy: 0.7533 - 19s/epoch - 47ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0087 - accuracy: 0.9977 - val_loss: 2.1699 - val_accuracy: 0.7194 - 19s/epoch - 47ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 2.9614 - val_accuracy: 0.7252 - 19s/epoch - 47ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0020 - accuracy: 0.9995 - val_loss: 3.2072 - val_accuracy: 0.7583 - 19s/epoch - 47ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0018 - accuracy: 0.9995 - val_loss: 2.7489 - val_accuracy: 0.7188 - 19s/epoch - 47ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0031 - accuracy: 0.9994 - val_loss: 1.6494 - val_accuracy: 0.7552 - 19s/epoch - 47ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0051 - accuracy: 0.9985 - val_loss: 1.9455 - val_accuracy: 0.7660 - 19s/epoch - 47ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0020 - accuracy: 0.9995 - val_loss: 2.8012 - val_accuracy: 0.7333 - 19s/epoch - 47ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0041 - accuracy: 0.9987 - val_loss: 3.5589 - val_accuracy: 0.7442 - 19s/epoch - 46ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 3.8210 - val_accuracy: 0.7235 - 20s/epoch - 48ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0133 - accuracy: 0.9971 - val_loss: 3.0923 - val_accuracy: 0.7579 - 19s/epoch - 47ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0109 - accuracy: 0.9983 - val_loss: 2.5977 - val_accuracy: 0.7181 - 19s/epoch - 47ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0044 - accuracy: 0.9989 - val_loss: 2.8225 - val_accuracy: 0.7346 - 19s/epoch - 47ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 6.2246e-04 - accuracy: 0.9999 - val_loss: 4.0387 - val_accuracy: 0.7317 - 19s/epoch - 47ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0079 - accuracy: 0.9985 - val_loss: 2.8925 - val_accuracy: 0.7208 - 19s/epoch - 46ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0022 - accuracy: 0.9995 - val_loss: 3.9597 - val_accuracy: 0.7242 - 19s/epoch - 47ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0080 - accuracy: 0.9980 - val_loss: 3.9519 - val_accuracy: 0.7075 - 19s/epoch - 47ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0066 - accuracy: 0.9982 - val_loss: 2.1504 - val_accuracy: 0.7700 - 19s/epoch - 46ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0022 - accuracy: 0.9995 - val_loss: 2.7576 - val_accuracy: 0.7721 - 19s/epoch - 47ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9994 - val_loss: 3.0774 - val_accuracy: 0.7481 - 19s/epoch - 47ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0059 - accuracy: 0.9980 - val_loss: 2.9956 - val_accuracy: 0.7192 - 19s/epoch - 47ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0018 - accuracy: 0.9994 - val_loss: 2.4879 - val_accuracy: 0.7369 - 19s/epoch - 47ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0027 - accuracy: 0.9996 - val_loss: 2.8449 - val_accuracy: 0.7848 - 19s/epoch - 47ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0056 - accuracy: 0.9985 - val_loss: 3.5021 - val_accuracy: 0.7900 - 19s/epoch - 47ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0038 - accuracy: 0.9991 - val_loss: 2.3515 - val_accuracy: 0.7142 - 19s/epoch - 47ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0065 - accuracy: 0.9985 - val_loss: 2.1393 - val_accuracy: 0.7735 - 19s/epoch - 47ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0047 - accuracy: 0.9986 - val_loss: 2.7438 - val_accuracy: 0.7796 - 19s/epoch - 47ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 2.9447 - val_accuracy: 0.7765 - 20s/epoch - 47ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0046 - accuracy: 0.9989 - val_loss: 2.6272 - val_accuracy: 0.7483 - 20s/epoch - 49ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0129 - accuracy: 0.9980 - val_loss: 2.9549 - val_accuracy: 0.7173 - 20s/epoch - 49ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0037 - accuracy: 0.9991 - val_loss: 3.5438 - val_accuracy: 0.7435 - 19s/epoch - 46ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0072 - accuracy: 0.9982 - val_loss: 3.2474 - val_accuracy: 0.7571 - 19s/epoch - 47ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6237 - val_accuracy: 0.6840 - 19s/epoch - 47ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0024 - accuracy: 0.9995 - val_loss: 2.7246 - val_accuracy: 0.7815 - 19s/epoch - 47ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 3.0823 - val_accuracy: 0.7154 - 19s/epoch - 47ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 3.4297 - val_accuracy: 0.7321 - 19s/epoch - 47ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0114 - accuracy: 0.9968 - val_loss: 2.6975 - val_accuracy: 0.7663 - 19s/epoch - 47ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0023 - accuracy: 0.9996 - val_loss: 2.8436 - val_accuracy: 0.7679 - 19s/epoch - 47ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 8.5916e-04 - accuracy: 0.9999 - val_loss: 3.2799 - val_accuracy: 0.7088 - 19s/epoch - 47ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 7.1276e-04 - accuracy: 0.9999 - val_loss: 3.8392 - val_accuracy: 0.7083 - 19s/epoch - 46ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.5711 - val_accuracy: 0.7362 - 19s/epoch - 47ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0044 - accuracy: 0.9986 - val_loss: 3.0023 - val_accuracy: 0.6812 - 19s/epoch - 47ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 2.8040 - val_accuracy: 0.7602 - 19s/epoch - 47ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0049 - accuracy: 0.9986 - val_loss: 3.0593 - val_accuracy: 0.7433 - 19s/epoch - 46ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0075 - accuracy: 0.9984 - val_loss: 3.7019 - val_accuracy: 0.7196 - 20s/epoch - 47ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0079 - accuracy: 0.9983 - val_loss: 3.2823 - val_accuracy: 0.7565 - 19s/epoch - 47ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0231 - accuracy: 0.9958 - val_loss: 2.4870 - val_accuracy: 0.7267 - 19s/epoch - 47ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 3.7159 - val_accuracy: 0.6902 - 19s/epoch - 47ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 3.6072 - val_accuracy: 0.6756 - 19s/epoch - 47ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 3.5741 - val_accuracy: 0.6740 - 19s/epoch - 46ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 3.3093 - val_accuracy: 0.7296 - 20s/epoch - 49ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 3.4196 - val_accuracy: 0.7156 - 19s/epoch - 47ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0080 - accuracy: 0.9982 - val_loss: 2.4705 - val_accuracy: 0.6956 - 19s/epoch - 47ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0040 - accuracy: 0.9989 - val_loss: 3.6426 - val_accuracy: 0.7273 - 20s/epoch - 47ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0038 - accuracy: 0.9990 - val_loss: 3.4922 - val_accuracy: 0.7163 - 19s/epoch - 47ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0048 - accuracy: 0.9984 - val_loss: 3.0960 - val_accuracy: 0.6798 - 19s/epoch - 47ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0080 - accuracy: 0.9986 - val_loss: 2.5120 - val_accuracy: 0.7396 - 20s/epoch - 47ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0039 - accuracy: 0.9989 - val_loss: 2.8405 - val_accuracy: 0.6804 - 19s/epoch - 47ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0018 - accuracy: 0.9996 - val_loss: 3.8691 - val_accuracy: 0.7344 - 20s/epoch - 49ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0076 - accuracy: 0.9983 - val_loss: 3.1530 - val_accuracy: 0.6819 - 19s/epoch - 47ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 3.2647 - val_accuracy: 0.7258 - 19s/epoch - 47ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0015 - accuracy: 0.9994 - val_loss: 4.1185 - val_accuracy: 0.7279 - 19s/epoch - 47ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 4.0480 - val_accuracy: 0.7190 - 19s/epoch - 47ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0030 - accuracy: 0.9994 - val_loss: 3.5031 - val_accuracy: 0.6815 - 19s/epoch - 47ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0157 - accuracy: 0.9971 - val_loss: 2.3175 - val_accuracy: 0.7121 - 19s/epoch - 47ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0096 - accuracy: 0.9981 - val_loss: 2.9870 - val_accuracy: 0.6929 - 19s/epoch - 47ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0057 - accuracy: 0.9987 - val_loss: 4.1114 - val_accuracy: 0.7031 - 19s/epoch - 47ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 3.5989 - val_accuracy: 0.6621 - 20s/epoch - 48ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0014 - accuracy: 0.9997 - val_loss: 3.2306 - val_accuracy: 0.7121 - 19s/epoch - 47ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 3.8836 - val_accuracy: 0.7023 - 19s/epoch - 47ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0041 - accuracy: 0.9989 - val_loss: 2.7333 - val_accuracy: 0.7102 - 19s/epoch - 47ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 6.8336e-04 - accuracy: 0.9999 - val_loss: 5.0263 - val_accuracy: 0.6750 - 20s/epoch - 49ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0042 - accuracy: 0.9990 - val_loss: 3.6938 - val_accuracy: 0.6831 - 19s/epoch - 46ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0170 - accuracy: 0.9982 - val_loss: 3.0936 - val_accuracy: 0.6927 - 19s/epoch - 47ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0085 - accuracy: 0.9981 - val_loss: 2.7760 - val_accuracy: 0.7744 - 19s/epoch - 47ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 6.7780e-04 - accuracy: 0.9999 - val_loss: 4.2190 - val_accuracy: 0.7096 - 19s/epoch - 47ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 4.5543 - val_accuracy: 0.6985 - 19s/epoch - 46ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0107 - accuracy: 0.9974 - val_loss: 3.0717 - val_accuracy: 0.7317 - 19s/epoch - 47ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 8.1334e-04 - accuracy: 0.9997 - val_loss: 3.5073 - val_accuracy: 0.7458 - 19s/epoch - 47ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 9.4738e-04 - accuracy: 0.9998 - val_loss: 2.6662 - val_accuracy: 0.7356 - 19s/epoch - 47ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 2.7889 - val_accuracy: 0.6712 - 19s/epoch - 47ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0114 - accuracy: 0.9984 - val_loss: 2.0259 - val_accuracy: 0.7135 - 19s/epoch - 47ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0049 - accuracy: 0.9990 - val_loss: 2.1946 - val_accuracy: 0.7700 - 19s/epoch - 47ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0088 - accuracy: 0.9983 - val_loss: 1.7457 - val_accuracy: 0.7615 - 20s/epoch - 48ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0061 - accuracy: 0.9991 - val_loss: 4.1027 - val_accuracy: 0.6996 - 19s/epoch - 47ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0043 - accuracy: 0.9989 - val_loss: 2.7440 - val_accuracy: 0.7473 - 19s/epoch - 47ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 9.1771e-04 - accuracy: 0.9997 - val_loss: 3.7954 - val_accuracy: 0.7342 - 19s/epoch - 47ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 3.1347 - val_accuracy: 0.7410 - 20s/epoch - 47ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.4411 - val_accuracy: 0.7785 - 20s/epoch - 47ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0046 - accuracy: 0.9989 - val_loss: 4.1162 - val_accuracy: 0.7279 - 20s/epoch - 48ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0066 - accuracy: 0.9981 - val_loss: 2.9950 - val_accuracy: 0.7008 - 20s/epoch - 47ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0036 - accuracy: 0.9994 - val_loss: 2.8686 - val_accuracy: 0.6960 - 20s/epoch - 48ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0056 - accuracy: 0.9992 - val_loss: 3.1862 - val_accuracy: 0.7308 - 19s/epoch - 47ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 3.0501 - val_accuracy: 0.7142 - 19s/epoch - 47ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 7.5671e-04 - accuracy: 0.9999 - val_loss: 3.8791 - val_accuracy: 0.7144 - 19s/epoch - 47ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0057 - accuracy: 0.9987 - val_loss: 2.9528 - val_accuracy: 0.7410 - 19s/epoch - 47ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0042 - accuracy: 0.9991 - val_loss: 3.1103 - val_accuracy: 0.7310 - 19s/epoch - 47ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 3.1870 - val_accuracy: 0.7346 - 19s/epoch - 47ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 7.8141e-04 - accuracy: 0.9998 - val_loss: 3.4877 - val_accuracy: 0.7169 - 19s/epoch - 47ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 2.0247 - val_accuracy: 0.6969 - 19s/epoch - 47ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0040 - accuracy: 0.9991 - val_loss: 3.2210 - val_accuracy: 0.7544 - 19s/epoch - 47ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0046 - accuracy: 0.9989 - val_loss: 3.6445 - val_accuracy: 0.6837 - 19s/epoch - 47ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0067 - accuracy: 0.9981 - val_loss: 2.7873 - val_accuracy: 0.7631 - 20s/epoch - 48ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 3.9990 - val_accuracy: 0.7015 - 20s/epoch - 48ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 3.0905 - val_accuracy: 0.6573 - 19s/epoch - 47ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0031 - accuracy: 0.9992 - val_loss: 2.9994 - val_accuracy: 0.7054 - 20s/epoch - 47ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 3.9232 - val_accuracy: 0.7669 - 19s/epoch - 47ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 3.5527 - val_accuracy: 0.7437 - 19s/epoch - 47ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0031 - accuracy: 0.9992 - val_loss: 4.4373 - val_accuracy: 0.7362 - 19s/epoch - 47ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0032 - accuracy: 0.9991 - val_loss: 3.9048 - val_accuracy: 0.7519 - 19s/epoch - 47ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0023 - accuracy: 0.9996 - val_loss: 4.2858 - val_accuracy: 0.6960 - 19s/epoch - 47ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.6105 - val_accuracy: 0.7331 - 19s/epoch - 47ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 1.9867 - val_accuracy: 0.7302 - 19s/epoch - 47ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0069 - accuracy: 0.9986 - val_loss: 2.0845 - val_accuracy: 0.7565 - 19s/epoch - 47ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0051 - accuracy: 0.9989 - val_loss: 2.1189 - val_accuracy: 0.7879 - 19s/epoch - 47ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.1561 - val_accuracy: 0.7594 - 19s/epoch - 47ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9992 - val_loss: 2.1476 - val_accuracy: 0.7419 - 19s/epoch - 47ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0059 - accuracy: 0.9983 - val_loss: 2.4112 - val_accuracy: 0.7458 - 19s/epoch - 47ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 3.1346 - val_accuracy: 0.7485 - 19s/epoch - 47ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0043 - accuracy: 0.9989 - val_loss: 2.7452 - val_accuracy: 0.7040 - 20s/epoch - 48ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0031 - accuracy: 0.9993 - val_loss: 3.4624 - val_accuracy: 0.7467 - 19s/epoch - 47ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0069 - accuracy: 0.9985 - val_loss: 2.5247 - val_accuracy: 0.7135 - 20s/epoch - 49ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 3.9562 - val_accuracy: 0.7225 - 19s/epoch - 47ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0035 - accuracy: 0.9989 - val_loss: 2.6466 - val_accuracy: 0.8340 - 19s/epoch - 47ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 1.44949\n",
      "413/413 - 22s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 3.2521 - val_accuracy: 0.7590 - 22s/epoch - 54ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 3.3645 - val_accuracy: 0.7271 - 20s/epoch - 49ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0083 - accuracy: 0.9978 - val_loss: 1.8046 - val_accuracy: 0.6990 - 20s/epoch - 50ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0022 - accuracy: 0.9995 - val_loss: 3.6422 - val_accuracy: 0.6831 - 19s/epoch - 47ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0095 - accuracy: 0.9979 - val_loss: 3.2583 - val_accuracy: 0.6833 - 19s/epoch - 47ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0059 - accuracy: 0.9986 - val_loss: 3.8554 - val_accuracy: 0.6579 - 20s/epoch - 47ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 5.1477 - val_accuracy: 0.6425 - 20s/epoch - 49ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 3.9935 - val_accuracy: 0.7373 - 20s/epoch - 49ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 8.5313e-04 - accuracy: 0.9998 - val_loss: 4.2933 - val_accuracy: 0.6775 - 19s/epoch - 47ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 1.8189e-04 - accuracy: 1.0000 - val_loss: 5.5383 - val_accuracy: 0.6560 - 20s/epoch - 49ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0030 - accuracy: 0.9991 - val_loss: 3.8455 - val_accuracy: 0.6587 - 20s/epoch - 48ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0029 - accuracy: 0.9995 - val_loss: 3.7780 - val_accuracy: 0.6575 - 19s/epoch - 47ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 4.8041 - val_accuracy: 0.7108 - 19s/epoch - 47ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0016 - accuracy: 0.9997 - val_loss: 4.3398 - val_accuracy: 0.6669 - 20s/epoch - 48ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0018 - accuracy: 0.9996 - val_loss: 5.3156 - val_accuracy: 0.6415 - 19s/epoch - 47ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 5.3612 - val_accuracy: 0.7104 - 19s/epoch - 47ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0032 - accuracy: 0.9991 - val_loss: 4.4217 - val_accuracy: 0.6587 - 19s/epoch - 47ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 5.2499 - val_accuracy: 0.6513 - 19s/epoch - 47ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0066 - accuracy: 0.9988 - val_loss: 3.2606 - val_accuracy: 0.6344 - 19s/epoch - 47ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0081 - accuracy: 0.9981 - val_loss: 3.6190 - val_accuracy: 0.6844 - 19s/epoch - 47ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 5.3253 - val_accuracy: 0.7013 - 20s/epoch - 48ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0046 - accuracy: 0.9989 - val_loss: 2.8728 - val_accuracy: 0.7175 - 19s/epoch - 47ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0043 - accuracy: 0.9992 - val_loss: 4.1863 - val_accuracy: 0.7083 - 19s/epoch - 47ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 1.44949\n",
      "413/413 - 19s - loss: 0.0121 - accuracy: 0.9975 - val_loss: 3.5661 - val_accuracy: 0.7237 - 19s/epoch - 47ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 1.44949\n",
      "413/413 - 20s - loss: 0.0042 - accuracy: 0.9991 - val_loss: 4.1727 - val_accuracy: 0.6896 - 20s/epoch - 47ms/step\n",
      "179/179 [==============================] - 3s 13ms/step\n",
      "Classification accuracy: 0.519945 \n"
     ]
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.22900236e-01 2.36023977e-01]\n",
      " [9.02649343e-01 1.24495834e-01]\n",
      " [8.54743004e-01 1.82194188e-01]\n",
      " ...\n",
      " [9.99183476e-01 5.51787962e-04]\n",
      " [9.98423755e-01 1.07745233e-03]\n",
      " [9.98615742e-01 8.59286229e-04]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[1945 1355]\n",
      " [1625  775]]\n",
      "[47.72 54.48 36.38]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for DeepConvNet on New_ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)\n",
    "plot_confusion_matrix(preds_Deep, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for DeepConvNet on New_ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.91606, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 14s - loss: 0.3771 - accuracy: 0.8778 - val_loss: 0.9161 - val_accuracy: 0.7356 - 14s/epoch - 33ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0367 - accuracy: 0.9915 - val_loss: 1.0871 - val_accuracy: 0.7544 - 12s/epoch - 30ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0210 - accuracy: 0.9946 - val_loss: 1.2214 - val_accuracy: 0.7135 - 12s/epoch - 30ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0175 - accuracy: 0.9960 - val_loss: 1.4156 - val_accuracy: 0.7421 - 12s/epoch - 30ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0115 - accuracy: 0.9976 - val_loss: 1.3029 - val_accuracy: 0.7535 - 12s/epoch - 29ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0065 - accuracy: 0.9989 - val_loss: 0.9290 - val_accuracy: 0.7083 - 12s/epoch - 29ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0444 - accuracy: 0.9896 - val_loss: 1.2676 - val_accuracy: 0.7431 - 12s/epoch - 29ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0058 - accuracy: 0.9990 - val_loss: 1.3184 - val_accuracy: 0.7340 - 12s/epoch - 30ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0018 - accuracy: 0.9997 - val_loss: 1.8603 - val_accuracy: 0.7077 - 12s/epoch - 29ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 1.7535 - val_accuracy: 0.7281 - 12s/epoch - 29ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0083 - accuracy: 0.9980 - val_loss: 1.4755 - val_accuracy: 0.6890 - 12s/epoch - 29ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0388 - accuracy: 0.9904 - val_loss: 1.7733 - val_accuracy: 0.6831 - 12s/epoch - 30ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0036 - accuracy: 0.9995 - val_loss: 1.9169 - val_accuracy: 0.7090 - 12s/epoch - 30ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 1.6286 - val_accuracy: 0.7283 - 12s/epoch - 29ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0131 - accuracy: 0.9961 - val_loss: 1.3217 - val_accuracy: 0.7354 - 12s/epoch - 29ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0059 - accuracy: 0.9988 - val_loss: 2.0737 - val_accuracy: 0.7269 - 12s/epoch - 29ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 8.0070e-04 - accuracy: 0.9999 - val_loss: 1.7334 - val_accuracy: 0.7148 - 12s/epoch - 29ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.7688e-04 - accuracy: 1.0000 - val_loss: 1.8244 - val_accuracy: 0.7135 - 12s/epoch - 29ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.5802e-04 - accuracy: 1.0000 - val_loss: 2.3426 - val_accuracy: 0.7142 - 12s/epoch - 30ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0300 - accuracy: 0.9921 - val_loss: 1.9668 - val_accuracy: 0.6892 - 12s/epoch - 30ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0042 - accuracy: 0.9989 - val_loss: 1.1916 - val_accuracy: 0.7638 - 12s/epoch - 29ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.7768 - val_accuracy: 0.7250 - 12s/epoch - 30ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0077 - accuracy: 0.9987 - val_loss: 1.7679 - val_accuracy: 0.7290 - 12s/epoch - 29ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0056 - accuracy: 0.9987 - val_loss: 1.8570 - val_accuracy: 0.7492 - 12s/epoch - 30ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0022 - accuracy: 0.9995 - val_loss: 2.0041 - val_accuracy: 0.6938 - 12s/epoch - 29ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.4403e-04 - accuracy: 1.0000 - val_loss: 2.2606 - val_accuracy: 0.6827 - 12s/epoch - 29ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0642 - accuracy: 0.9870 - val_loss: 1.6224 - val_accuracy: 0.7106 - 12s/epoch - 29ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 1.8385 - val_accuracy: 0.7160 - 12s/epoch - 29ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.8573 - val_accuracy: 0.7079 - 12s/epoch - 29ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.8461e-04 - accuracy: 1.0000 - val_loss: 2.0993 - val_accuracy: 0.7004 - 12s/epoch - 29ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 6.0548e-04 - accuracy: 0.9999 - val_loss: 1.7406 - val_accuracy: 0.7292 - 12s/epoch - 29ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.5550e-04 - accuracy: 1.0000 - val_loss: 1.8115 - val_accuracy: 0.7273 - 12s/epoch - 29ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.1194e-04 - accuracy: 1.0000 - val_loss: 2.0277 - val_accuracy: 0.7104 - 12s/epoch - 29ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.2772e-04 - accuracy: 1.0000 - val_loss: 1.8112 - val_accuracy: 0.7262 - 12s/epoch - 29ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.0058e-04 - accuracy: 1.0000 - val_loss: 1.9327 - val_accuracy: 0.7433 - 12s/epoch - 30ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.3414 - val_accuracy: 0.7469 - 12s/epoch - 29ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0219 - accuracy: 0.9955 - val_loss: 1.6363 - val_accuracy: 0.7260 - 12s/epoch - 30ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 9.6485e-04 - accuracy: 0.9998 - val_loss: 1.7541 - val_accuracy: 0.7294 - 12s/epoch - 29ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.7490e-04 - accuracy: 1.0000 - val_loss: 1.7209 - val_accuracy: 0.7337 - 12s/epoch - 29ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.3187e-04 - accuracy: 0.9999 - val_loss: 1.6724 - val_accuracy: 0.7398 - 12s/epoch - 30ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.1608e-04 - accuracy: 1.0000 - val_loss: 2.0879 - val_accuracy: 0.7298 - 12s/epoch - 29ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.0807e-04 - accuracy: 0.9999 - val_loss: 1.8440 - val_accuracy: 0.7446 - 12s/epoch - 29ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 1.5621e-04 - accuracy: 1.0000 - val_loss: 1.8082 - val_accuracy: 0.7369 - 13s/epoch - 30ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 9.5772e-05 - accuracy: 1.0000 - val_loss: 1.8175 - val_accuracy: 0.7456 - 12s/epoch - 30ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.0482e-05 - accuracy: 1.0000 - val_loss: 1.8504 - val_accuracy: 0.7406 - 12s/epoch - 29ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.6304e-05 - accuracy: 1.0000 - val_loss: 1.9463 - val_accuracy: 0.7335 - 12s/epoch - 29ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0110 - accuracy: 0.9971 - val_loss: 1.4586 - val_accuracy: 0.6787 - 12s/epoch - 29ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0154 - accuracy: 0.9959 - val_loss: 1.6365 - val_accuracy: 0.7337 - 12s/epoch - 30ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 9.8258e-04 - accuracy: 0.9998 - val_loss: 1.7292 - val_accuracy: 0.7427 - 12s/epoch - 30ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 2.2517 - val_accuracy: 0.7088 - 13s/epoch - 31ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0057 - accuracy: 0.9988 - val_loss: 1.7207 - val_accuracy: 0.7765 - 12s/epoch - 30ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.0615e-04 - accuracy: 1.0000 - val_loss: 1.7768 - val_accuracy: 0.7740 - 12s/epoch - 29ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.2280e-04 - accuracy: 0.9999 - val_loss: 2.1043 - val_accuracy: 0.7258 - 12s/epoch - 30ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.0430e-04 - accuracy: 1.0000 - val_loss: 1.8429 - val_accuracy: 0.7581 - 12s/epoch - 30ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0066 - accuracy: 0.9983 - val_loss: 1.6395 - val_accuracy: 0.7206 - 12s/epoch - 30ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0090 - accuracy: 0.9983 - val_loss: 1.2528 - val_accuracy: 0.8033 - 12s/epoch - 30ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.5626e-04 - accuracy: 0.9998 - val_loss: 1.7885 - val_accuracy: 0.7365 - 12s/epoch - 30ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.7492e-04 - accuracy: 1.0000 - val_loss: 1.6454 - val_accuracy: 0.7725 - 12s/epoch - 29ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.3473e-04 - accuracy: 1.0000 - val_loss: 1.6398 - val_accuracy: 0.7956 - 12s/epoch - 29ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.3350e-04 - accuracy: 1.0000 - val_loss: 1.7773 - val_accuracy: 0.7302 - 12s/epoch - 30ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0085 - accuracy: 0.9984 - val_loss: 1.6940 - val_accuracy: 0.7790 - 12s/epoch - 29ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.9478e-04 - accuracy: 1.0000 - val_loss: 1.8274 - val_accuracy: 0.7627 - 12s/epoch - 30ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.0519e-04 - accuracy: 1.0000 - val_loss: 1.8580 - val_accuracy: 0.7708 - 12s/epoch - 30ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.0766e-04 - accuracy: 1.0000 - val_loss: 1.5515 - val_accuracy: 0.7946 - 12s/epoch - 30ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.0862e-04 - accuracy: 1.0000 - val_loss: 2.2020 - val_accuracy: 0.7460 - 12s/epoch - 30ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 6.1375e-05 - accuracy: 1.0000 - val_loss: 2.1690 - val_accuracy: 0.7550 - 12s/epoch - 29ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.8423e-05 - accuracy: 1.0000 - val_loss: 2.0317 - val_accuracy: 0.7677 - 12s/epoch - 30ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.9137e-05 - accuracy: 1.0000 - val_loss: 1.9736 - val_accuracy: 0.7862 - 12s/epoch - 30ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.4515e-05 - accuracy: 1.0000 - val_loss: 2.0592 - val_accuracy: 0.7819 - 12s/epoch - 29ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 7.9551e-05 - accuracy: 1.0000 - val_loss: 2.4877 - val_accuracy: 0.7494 - 12s/epoch - 29ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0259 - accuracy: 0.9928 - val_loss: 1.6245 - val_accuracy: 0.7227 - 12s/epoch - 30ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.5920 - val_accuracy: 0.7327 - 12s/epoch - 29ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 2.6446e-04 - accuracy: 1.0000 - val_loss: 1.7034 - val_accuracy: 0.7323 - 13s/epoch - 31ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.7533e-04 - accuracy: 1.0000 - val_loss: 1.5155 - val_accuracy: 0.7429 - 12s/epoch - 30ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.0962e-04 - accuracy: 1.0000 - val_loss: 1.4841 - val_accuracy: 0.7419 - 12s/epoch - 30ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0040 - accuracy: 0.9993 - val_loss: 2.4436 - val_accuracy: 0.6717 - 12s/epoch - 30ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0039 - accuracy: 0.9991 - val_loss: 1.5540 - val_accuracy: 0.7783 - 12s/epoch - 29ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.6896e-04 - accuracy: 1.0000 - val_loss: 1.7483 - val_accuracy: 0.7785 - 12s/epoch - 30ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.4451e-04 - accuracy: 0.9999 - val_loss: 1.6254 - val_accuracy: 0.7771 - 12s/epoch - 30ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.5723e-04 - accuracy: 0.9999 - val_loss: 1.7160 - val_accuracy: 0.7608 - 12s/epoch - 30ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.2514e-04 - accuracy: 1.0000 - val_loss: 1.7144 - val_accuracy: 0.7767 - 12s/epoch - 29ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.2144e-05 - accuracy: 1.0000 - val_loss: 1.8773 - val_accuracy: 0.7677 - 12s/epoch - 30ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 8.1848e-05 - accuracy: 1.0000 - val_loss: 1.8069 - val_accuracy: 0.7602 - 12s/epoch - 29ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.3460e-05 - accuracy: 1.0000 - val_loss: 2.0527 - val_accuracy: 0.7598 - 12s/epoch - 29ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.6497e-04 - accuracy: 1.0000 - val_loss: 1.9571 - val_accuracy: 0.7688 - 12s/epoch - 29ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0160 - accuracy: 0.9959 - val_loss: 1.6307 - val_accuracy: 0.7404 - 12s/epoch - 30ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.8353 - val_accuracy: 0.7354 - 12s/epoch - 29ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.5429e-04 - accuracy: 1.0000 - val_loss: 1.8944 - val_accuracy: 0.7292 - 12s/epoch - 29ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.1952e-04 - accuracy: 1.0000 - val_loss: 2.2333 - val_accuracy: 0.7294 - 12s/epoch - 29ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 8.3450e-05 - accuracy: 1.0000 - val_loss: 2.2694 - val_accuracy: 0.7237 - 12s/epoch - 29ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.9928e-04 - accuracy: 1.0000 - val_loss: 2.2012 - val_accuracy: 0.7231 - 12s/epoch - 29ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.2284e-04 - accuracy: 1.0000 - val_loss: 1.9636 - val_accuracy: 0.7467 - 12s/epoch - 29ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 5.0921e-05 - accuracy: 1.0000 - val_loss: 1.9784 - val_accuracy: 0.7360 - 13s/epoch - 31ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.3966e-05 - accuracy: 1.0000 - val_loss: 2.0901 - val_accuracy: 0.7342 - 12s/epoch - 30ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.3765e-05 - accuracy: 1.0000 - val_loss: 2.2293 - val_accuracy: 0.7329 - 12s/epoch - 29ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.6389e-05 - accuracy: 1.0000 - val_loss: 2.0831 - val_accuracy: 0.7450 - 12s/epoch - 29ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0079 - accuracy: 0.9978 - val_loss: 1.3597 - val_accuracy: 0.7713 - 12s/epoch - 30ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 0.0117 - accuracy: 0.9978 - val_loss: 2.3707 - val_accuracy: 0.6819 - 13s/epoch - 30ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 1.9991 - val_accuracy: 0.7144 - 12s/epoch - 30ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.6795e-04 - accuracy: 1.0000 - val_loss: 2.1286 - val_accuracy: 0.7029 - 12s/epoch - 30ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.1806e-04 - accuracy: 1.0000 - val_loss: 2.5867 - val_accuracy: 0.6787 - 12s/epoch - 29ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.3846e-04 - accuracy: 1.0000 - val_loss: 1.9303 - val_accuracy: 0.7098 - 12s/epoch - 30ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.2680e-04 - accuracy: 1.0000 - val_loss: 2.7608 - val_accuracy: 0.6923 - 12s/epoch - 30ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.1058e-04 - accuracy: 1.0000 - val_loss: 2.5306 - val_accuracy: 0.7038 - 12s/epoch - 29ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 6.3901e-05 - accuracy: 1.0000 - val_loss: 2.5299 - val_accuracy: 0.7073 - 12s/epoch - 29ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.2354e-05 - accuracy: 1.0000 - val_loss: 2.3417 - val_accuracy: 0.7240 - 12s/epoch - 29ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.9317e-05 - accuracy: 1.0000 - val_loss: 2.7784 - val_accuracy: 0.7113 - 12s/epoch - 29ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.1859e-05 - accuracy: 1.0000 - val_loss: 2.5239 - val_accuracy: 0.7173 - 12s/epoch - 29ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.8710e-05 - accuracy: 1.0000 - val_loss: 2.4966 - val_accuracy: 0.7219 - 12s/epoch - 30ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.7633e-05 - accuracy: 1.0000 - val_loss: 2.6488 - val_accuracy: 0.7115 - 14s/epoch - 34ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.4552e-05 - accuracy: 1.0000 - val_loss: 2.3500 - val_accuracy: 0.7296 - 15s/epoch - 36ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0119 - accuracy: 0.9968 - val_loss: 2.7534 - val_accuracy: 0.6821 - 12s/epoch - 30ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.9058e-04 - accuracy: 1.0000 - val_loss: 2.3824 - val_accuracy: 0.7252 - 12s/epoch - 30ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.7370e-04 - accuracy: 1.0000 - val_loss: 2.5652 - val_accuracy: 0.7202 - 12s/epoch - 30ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.0811e-04 - accuracy: 1.0000 - val_loss: 2.5987 - val_accuracy: 0.7254 - 12s/epoch - 30ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 7.1675e-05 - accuracy: 1.0000 - val_loss: 2.6360 - val_accuracy: 0.7256 - 12s/epoch - 30ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 7.1061e-05 - accuracy: 1.0000 - val_loss: 2.2731 - val_accuracy: 0.7344 - 12s/epoch - 30ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 4.2810e-05 - accuracy: 1.0000 - val_loss: 2.6878 - val_accuracy: 0.7254 - 12s/epoch - 30ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.0815e-05 - accuracy: 1.0000 - val_loss: 2.8605 - val_accuracy: 0.7175 - 12s/epoch - 30ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.6482e-05 - accuracy: 1.0000 - val_loss: 2.7506 - val_accuracy: 0.7258 - 12s/epoch - 30ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 2.1024e-05 - accuracy: 1.0000 - val_loss: 2.6859 - val_accuracy: 0.7204 - 12s/epoch - 30ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 3.7866e-05 - accuracy: 1.0000 - val_loss: 2.6760 - val_accuracy: 0.7192 - 12s/epoch - 30ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0103 - accuracy: 0.9979 - val_loss: 2.5310 - val_accuracy: 0.6704 - 12s/epoch - 30ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 2.5989 - val_accuracy: 0.6783 - 12s/epoch - 30ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0033 - accuracy: 0.9989 - val_loss: 2.1790 - val_accuracy: 0.6944 - 12s/epoch - 30ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.0451e-04 - accuracy: 0.9999 - val_loss: 2.6353 - val_accuracy: 0.6846 - 12s/epoch - 30ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.6623e-04 - accuracy: 1.0000 - val_loss: 2.8318 - val_accuracy: 0.6815 - 12s/epoch - 29ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0036 - accuracy: 0.9995 - val_loss: 1.4683 - val_accuracy: 0.7394 - 12s/epoch - 30ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 0.0142 - accuracy: 0.9965 - val_loss: 2.0624 - val_accuracy: 0.7054 - 12s/epoch - 30ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 5.2441e-04 - accuracy: 1.0000 - val_loss: 2.1839 - val_accuracy: 0.7088 - 12s/epoch - 30ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.7678e-04 - accuracy: 1.0000 - val_loss: 2.0848 - val_accuracy: 0.7200 - 12s/epoch - 30ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.91606\n",
      "413/413 - 12s - loss: 1.5231e-04 - accuracy: 1.0000 - val_loss: 2.2956 - val_accuracy: 0.7135 - 12s/epoch - 30ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.9300e-05 - accuracy: 1.0000 - val_loss: 2.3076 - val_accuracy: 0.7127 - 14s/epoch - 34ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 5.6409e-05 - accuracy: 1.0000 - val_loss: 2.6318 - val_accuracy: 0.6996 - 13s/epoch - 32ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.7486e-05 - accuracy: 1.0000 - val_loss: 2.5727 - val_accuracy: 0.7063 - 14s/epoch - 33ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 4.6647e-05 - accuracy: 1.0000 - val_loss: 2.5705 - val_accuracy: 0.7090 - 13s/epoch - 32ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 3.7194e-05 - accuracy: 1.0000 - val_loss: 2.4614 - val_accuracy: 0.7188 - 13s/epoch - 32ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 2.3353e-05 - accuracy: 1.0000 - val_loss: 2.5833 - val_accuracy: 0.7140 - 13s/epoch - 32ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 4.7704e-05 - accuracy: 1.0000 - val_loss: 2.5853 - val_accuracy: 0.7198 - 13s/epoch - 33ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 2.7807e-05 - accuracy: 1.0000 - val_loss: 2.5019 - val_accuracy: 0.7252 - 13s/epoch - 32ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 2.4280e-05 - accuracy: 1.0000 - val_loss: 2.4912 - val_accuracy: 0.7323 - 15s/epoch - 36ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.2883e-05 - accuracy: 1.0000 - val_loss: 2.5669 - val_accuracy: 0.7287 - 14s/epoch - 35ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.8697e-05 - accuracy: 1.0000 - val_loss: 2.5977 - val_accuracy: 0.7279 - 14s/epoch - 34ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0153 - accuracy: 0.9961 - val_loss: 1.6149 - val_accuracy: 0.7179 - 14s/epoch - 35ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0033 - accuracy: 0.9993 - val_loss: 1.7090 - val_accuracy: 0.7019 - 14s/epoch - 35ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0021 - accuracy: 0.9994 - val_loss: 1.8876 - val_accuracy: 0.6998 - 14s/epoch - 35ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 3.4449e-04 - accuracy: 1.0000 - val_loss: 2.0086 - val_accuracy: 0.7075 - 15s/epoch - 35ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.3063e-04 - accuracy: 1.0000 - val_loss: 2.0468 - val_accuracy: 0.7063 - 14s/epoch - 34ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.9184e-05 - accuracy: 1.0000 - val_loss: 2.1921 - val_accuracy: 0.6975 - 14s/epoch - 34ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 5.7253e-05 - accuracy: 1.0000 - val_loss: 2.3176 - val_accuracy: 0.6981 - 14s/epoch - 33ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.5842e-05 - accuracy: 1.0000 - val_loss: 2.0925 - val_accuracy: 0.7150 - 14s/epoch - 33ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.1759e-05 - accuracy: 1.0000 - val_loss: 1.8813 - val_accuracy: 0.7277 - 14s/epoch - 34ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.2457e-05 - accuracy: 1.0000 - val_loss: 2.0795 - val_accuracy: 0.7190 - 14s/epoch - 34ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.1883e-05 - accuracy: 1.0000 - val_loss: 2.3662 - val_accuracy: 0.7056 - 14s/epoch - 34ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.8576e-05 - accuracy: 1.0000 - val_loss: 2.2349 - val_accuracy: 0.7121 - 14s/epoch - 34ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.4642e-05 - accuracy: 1.0000 - val_loss: 2.4384 - val_accuracy: 0.7000 - 14s/epoch - 34ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.6535e-05 - accuracy: 1.0000 - val_loss: 1.9187 - val_accuracy: 0.7267 - 14s/epoch - 34ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0089 - accuracy: 0.9974 - val_loss: 1.7915 - val_accuracy: 0.7085 - 14s/epoch - 33ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.2558e-04 - accuracy: 0.9998 - val_loss: 2.0693 - val_accuracy: 0.6731 - 14s/epoch - 34ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0129 - accuracy: 0.9970 - val_loss: 2.0772 - val_accuracy: 0.6881 - 14s/epoch - 35ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.1115e-04 - accuracy: 0.9999 - val_loss: 1.9680 - val_accuracy: 0.7085 - 14s/epoch - 35ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.3556e-04 - accuracy: 1.0000 - val_loss: 2.0720 - val_accuracy: 0.7000 - 14s/epoch - 35ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 9.9725e-05 - accuracy: 1.0000 - val_loss: 2.2402 - val_accuracy: 0.7058 - 15s/epoch - 35ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.6185e-05 - accuracy: 1.0000 - val_loss: 2.5973 - val_accuracy: 0.6829 - 14s/epoch - 34ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 5.2043e-05 - accuracy: 1.0000 - val_loss: 2.2503 - val_accuracy: 0.6990 - 14s/epoch - 35ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 4.1624e-05 - accuracy: 1.0000 - val_loss: 2.2100 - val_accuracy: 0.7025 - 15s/epoch - 37ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.2080e-05 - accuracy: 1.0000 - val_loss: 2.6685 - val_accuracy: 0.6929 - 14s/epoch - 35ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0034 - accuracy: 0.9992 - val_loss: 2.4341 - val_accuracy: 0.6535 - 14s/epoch - 34ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0016 - accuracy: 0.9998 - val_loss: 2.1442 - val_accuracy: 0.6925 - 14s/epoch - 34ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0038 - accuracy: 0.9992 - val_loss: 2.1998 - val_accuracy: 0.6925 - 14s/epoch - 35ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.6010e-04 - accuracy: 0.9998 - val_loss: 2.1536 - val_accuracy: 0.7165 - 14s/epoch - 34ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.2265e-05 - accuracy: 1.0000 - val_loss: 2.2705 - val_accuracy: 0.7181 - 14s/epoch - 34ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.9222e-05 - accuracy: 1.0000 - val_loss: 2.3885 - val_accuracy: 0.7015 - 14s/epoch - 34ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.1401e-04 - accuracy: 0.9997 - val_loss: 1.0992 - val_accuracy: 0.7452 - 14s/epoch - 35ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 2.3391 - val_accuracy: 0.6865 - 14s/epoch - 35ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.6484e-04 - accuracy: 0.9998 - val_loss: 2.2367 - val_accuracy: 0.7046 - 14s/epoch - 33ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.5624e-05 - accuracy: 1.0000 - val_loss: 2.3511 - val_accuracy: 0.7106 - 14s/epoch - 33ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.9924e-04 - accuracy: 1.0000 - val_loss: 2.3637 - val_accuracy: 0.7100 - 15s/epoch - 37ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.91606\n",
      "413/413 - 16s - loss: 5.3157e-05 - accuracy: 1.0000 - val_loss: 2.5572 - val_accuracy: 0.6865 - 16s/epoch - 38ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 2.7079e-05 - accuracy: 1.0000 - val_loss: 2.5608 - val_accuracy: 0.6981 - 15s/epoch - 37ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 2.7327e-05 - accuracy: 1.0000 - val_loss: 2.6443 - val_accuracy: 0.6988 - 15s/epoch - 37ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.8817e-05 - accuracy: 1.0000 - val_loss: 2.5804 - val_accuracy: 0.7102 - 15s/epoch - 37ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 2.1521e-05 - accuracy: 1.0000 - val_loss: 2.6515 - val_accuracy: 0.7085 - 15s/epoch - 36ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.6530e-05 - accuracy: 1.0000 - val_loss: 2.4376 - val_accuracy: 0.7212 - 15s/epoch - 36ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0110 - accuracy: 0.9979 - val_loss: 2.3133 - val_accuracy: 0.6756 - 14s/epoch - 33ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0026 - accuracy: 0.9996 - val_loss: 2.2703 - val_accuracy: 0.6775 - 14s/epoch - 34ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.5223e-04 - accuracy: 1.0000 - val_loss: 2.1585 - val_accuracy: 0.6810 - 14s/epoch - 33ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.0301e-04 - accuracy: 1.0000 - val_loss: 2.5675 - val_accuracy: 0.6742 - 15s/epoch - 36ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.8855e-04 - accuracy: 0.9999 - val_loss: 2.2267 - val_accuracy: 0.6942 - 14s/epoch - 34ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.0614e-05 - accuracy: 1.0000 - val_loss: 2.5386 - val_accuracy: 0.6773 - 14s/epoch - 34ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.5522e-05 - accuracy: 1.0000 - val_loss: 2.5152 - val_accuracy: 0.6825 - 14s/epoch - 34ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.5186e-05 - accuracy: 1.0000 - val_loss: 2.2312 - val_accuracy: 0.6942 - 14s/epoch - 34ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.1311e-05 - accuracy: 1.0000 - val_loss: 2.2965 - val_accuracy: 0.6965 - 14s/epoch - 34ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.7540e-05 - accuracy: 1.0000 - val_loss: 2.3788 - val_accuracy: 0.6927 - 14s/epoch - 35ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.7401e-05 - accuracy: 1.0000 - val_loss: 2.6214 - val_accuracy: 0.6850 - 14s/epoch - 35ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.4836e-05 - accuracy: 1.0000 - val_loss: 2.7285 - val_accuracy: 0.6831 - 14s/epoch - 35ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.2875e-05 - accuracy: 1.0000 - val_loss: 2.7553 - val_accuracy: 0.6823 - 14s/epoch - 35ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 8.9614e-06 - accuracy: 1.0000 - val_loss: 2.7205 - val_accuracy: 0.6862 - 15s/epoch - 36ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0046 - accuracy: 0.9983 - val_loss: 1.8261 - val_accuracy: 0.6871 - 14s/epoch - 34ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0028 - accuracy: 0.9995 - val_loss: 2.4385 - val_accuracy: 0.6660 - 14s/epoch - 34ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.0013e-04 - accuracy: 1.0000 - val_loss: 2.3764 - val_accuracy: 0.6731 - 14s/epoch - 34ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.2025e-05 - accuracy: 1.0000 - val_loss: 2.3352 - val_accuracy: 0.6812 - 14s/epoch - 35ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.1321e-05 - accuracy: 1.0000 - val_loss: 2.5486 - val_accuracy: 0.6804 - 14s/epoch - 35ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0060 - accuracy: 0.9987 - val_loss: 1.3508 - val_accuracy: 0.6750 - 14s/epoch - 35ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0064 - accuracy: 0.9986 - val_loss: 2.5288 - val_accuracy: 0.6417 - 14s/epoch - 35ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.1857e-04 - accuracy: 1.0000 - val_loss: 2.6707 - val_accuracy: 0.6444 - 14s/epoch - 35ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.4670e-04 - accuracy: 1.0000 - val_loss: 2.7992 - val_accuracy: 0.6485 - 15s/epoch - 35ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 7.6945e-05 - accuracy: 1.0000 - val_loss: 2.9503 - val_accuracy: 0.6492 - 15s/epoch - 36ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.5205e-04 - accuracy: 0.9998 - val_loss: 2.6979 - val_accuracy: 0.6590 - 14s/epoch - 35ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.1791e-04 - accuracy: 1.0000 - val_loss: 2.9854 - val_accuracy: 0.6556 - 14s/epoch - 34ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.4381e-05 - accuracy: 1.0000 - val_loss: 2.8258 - val_accuracy: 0.6656 - 14s/epoch - 34ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.8453e-05 - accuracy: 1.0000 - val_loss: 2.9119 - val_accuracy: 0.6642 - 14s/epoch - 34ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.2928e-05 - accuracy: 1.0000 - val_loss: 2.4057 - val_accuracy: 0.6785 - 14s/epoch - 35ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.5444e-05 - accuracy: 1.0000 - val_loss: 2.4839 - val_accuracy: 0.6783 - 14s/epoch - 34ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.6899e-04 - accuracy: 0.9998 - val_loss: 1.5656 - val_accuracy: 0.6925 - 14s/epoch - 33ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 0.0150 - accuracy: 0.9964 - val_loss: 2.4245 - val_accuracy: 0.6552 - 13s/epoch - 32ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.91606\n",
      "413/413 - 13s - loss: 0.0027 - accuracy: 0.9996 - val_loss: 2.2355 - val_accuracy: 0.6890 - 13s/epoch - 33ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 5.1194e-04 - accuracy: 0.9999 - val_loss: 2.4655 - val_accuracy: 0.6744 - 14s/epoch - 34ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.9524e-05 - accuracy: 1.0000 - val_loss: 2.5432 - val_accuracy: 0.6773 - 14s/epoch - 34ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.1843e-05 - accuracy: 1.0000 - val_loss: 2.5508 - val_accuracy: 0.6871 - 14s/epoch - 33ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.3778e-05 - accuracy: 1.0000 - val_loss: 2.6502 - val_accuracy: 0.6831 - 14s/epoch - 34ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 5.4546e-05 - accuracy: 1.0000 - val_loss: 2.5624 - val_accuracy: 0.6796 - 14s/epoch - 33ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.0589e-05 - accuracy: 1.0000 - val_loss: 2.7047 - val_accuracy: 0.6794 - 14s/epoch - 34ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.2081e-05 - accuracy: 1.0000 - val_loss: 2.7193 - val_accuracy: 0.6888 - 14s/epoch - 33ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.5855e-05 - accuracy: 1.0000 - val_loss: 2.7372 - val_accuracy: 0.6965 - 14s/epoch - 34ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.9916e-05 - accuracy: 1.0000 - val_loss: 2.3304 - val_accuracy: 0.7081 - 14s/epoch - 34ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.7039e-05 - accuracy: 1.0000 - val_loss: 2.8451 - val_accuracy: 0.6990 - 14s/epoch - 34ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0081 - accuracy: 0.9977 - val_loss: 2.4267 - val_accuracy: 0.6858 - 14s/epoch - 34ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.4754e-04 - accuracy: 0.9999 - val_loss: 2.7395 - val_accuracy: 0.6821 - 14s/epoch - 34ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.7668e-05 - accuracy: 1.0000 - val_loss: 2.2366 - val_accuracy: 0.7054 - 14s/epoch - 33ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.5224e-05 - accuracy: 1.0000 - val_loss: 2.4283 - val_accuracy: 0.6975 - 14s/epoch - 34ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.8664e-05 - accuracy: 1.0000 - val_loss: 2.1568 - val_accuracy: 0.7210 - 14s/epoch - 35ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 0.0026 - accuracy: 0.9994 - val_loss: 1.6343 - val_accuracy: 0.7304 - 15s/epoch - 37ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 0.0024 - accuracy: 0.9995 - val_loss: 2.5600 - val_accuracy: 0.6683 - 15s/epoch - 35ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.5892e-04 - accuracy: 1.0000 - val_loss: 2.6768 - val_accuracy: 0.6779 - 14s/epoch - 34ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 9.9931e-05 - accuracy: 1.0000 - val_loss: 2.5721 - val_accuracy: 0.6733 - 14s/epoch - 34ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.5301e-05 - accuracy: 1.0000 - val_loss: 2.4232 - val_accuracy: 0.6885 - 14s/epoch - 33ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 5.6903e-05 - accuracy: 1.0000 - val_loss: 2.1233 - val_accuracy: 0.7050 - 14s/epoch - 33ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.6618e-05 - accuracy: 1.0000 - val_loss: 2.4372 - val_accuracy: 0.6971 - 14s/epoch - 34ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.1007e-05 - accuracy: 1.0000 - val_loss: 2.4019 - val_accuracy: 0.6973 - 14s/epoch - 35ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.5237e-05 - accuracy: 1.0000 - val_loss: 2.6707 - val_accuracy: 0.6921 - 14s/epoch - 35ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.0553e-05 - accuracy: 1.0000 - val_loss: 2.6224 - val_accuracy: 0.7029 - 14s/epoch - 34ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.4759e-05 - accuracy: 1.0000 - val_loss: 2.6743 - val_accuracy: 0.7002 - 14s/epoch - 34ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.4878e-06 - accuracy: 1.0000 - val_loss: 2.6971 - val_accuracy: 0.6990 - 14s/epoch - 35ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0123 - accuracy: 0.9970 - val_loss: 1.8946 - val_accuracy: 0.7269 - 14s/epoch - 35ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.1720e-04 - accuracy: 1.0000 - val_loss: 2.2295 - val_accuracy: 0.7100 - 14s/epoch - 35ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.5129e-04 - accuracy: 1.0000 - val_loss: 2.4324 - val_accuracy: 0.6892 - 14s/epoch - 35ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.0814e-05 - accuracy: 1.0000 - val_loss: 2.2676 - val_accuracy: 0.6985 - 14s/epoch - 35ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.1607e-05 - accuracy: 1.0000 - val_loss: 2.2238 - val_accuracy: 0.7079 - 14s/epoch - 35ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 4.4189e-05 - accuracy: 1.0000 - val_loss: 2.6019 - val_accuracy: 0.6833 - 15s/epoch - 36ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 0.0063 - accuracy: 0.9984 - val_loss: 1.9317 - val_accuracy: 0.7296 - 15s/epoch - 35ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.0354e-04 - accuracy: 1.0000 - val_loss: 2.2297 - val_accuracy: 0.7344 - 14s/epoch - 35ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.5379e-04 - accuracy: 1.0000 - val_loss: 2.1793 - val_accuracy: 0.7367 - 14s/epoch - 35ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.9038e-04 - accuracy: 0.9999 - val_loss: 2.4630 - val_accuracy: 0.6956 - 14s/epoch - 35ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.3955e-05 - accuracy: 1.0000 - val_loss: 2.9276 - val_accuracy: 0.6938 - 14s/epoch - 35ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.7010e-05 - accuracy: 1.0000 - val_loss: 2.6652 - val_accuracy: 0.6973 - 14s/epoch - 35ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.6494e-05 - accuracy: 1.0000 - val_loss: 2.7366 - val_accuracy: 0.6896 - 14s/epoch - 35ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.0978e-04 - accuracy: 1.0000 - val_loss: 2.0744 - val_accuracy: 0.7152 - 14s/epoch - 35ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.0225e-04 - accuracy: 1.0000 - val_loss: 2.4110 - val_accuracy: 0.7113 - 14s/epoch - 35ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 2.4663e-05 - accuracy: 1.0000 - val_loss: 2.4939 - val_accuracy: 0.7094 - 15s/epoch - 35ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.0399e-05 - accuracy: 1.0000 - val_loss: 2.7119 - val_accuracy: 0.6898 - 14s/epoch - 35ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.8643e-05 - accuracy: 1.0000 - val_loss: 2.4643 - val_accuracy: 0.7033 - 15s/epoch - 35ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0143 - accuracy: 0.9961 - val_loss: 2.4424 - val_accuracy: 0.6833 - 14s/epoch - 34ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 2.6488 - val_accuracy: 0.6852 - 14s/epoch - 35ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.4120e-04 - accuracy: 1.0000 - val_loss: 2.5455 - val_accuracy: 0.6869 - 14s/epoch - 34ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.3102e-04 - accuracy: 1.0000 - val_loss: 2.5863 - val_accuracy: 0.6840 - 14s/epoch - 34ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 5.0047e-05 - accuracy: 1.0000 - val_loss: 2.4160 - val_accuracy: 0.6931 - 14s/epoch - 35ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.7341e-05 - accuracy: 1.0000 - val_loss: 2.7804 - val_accuracy: 0.6848 - 14s/epoch - 34ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.7118e-05 - accuracy: 1.0000 - val_loss: 2.2282 - val_accuracy: 0.7058 - 14s/epoch - 35ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.6419e-05 - accuracy: 1.0000 - val_loss: 2.2961 - val_accuracy: 0.7048 - 14s/epoch - 34ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.7278e-05 - accuracy: 1.0000 - val_loss: 2.4097 - val_accuracy: 0.7044 - 14s/epoch - 35ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 2.3465e-05 - accuracy: 1.0000 - val_loss: 2.4796 - val_accuracy: 0.6994 - 15s/epoch - 36ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.91606\n",
      "413/413 - 16s - loss: 2.8049e-05 - accuracy: 1.0000 - val_loss: 2.1027 - val_accuracy: 0.7344 - 16s/epoch - 39ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.1750e-05 - accuracy: 1.0000 - val_loss: 2.3455 - val_accuracy: 0.7387 - 14s/epoch - 35ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.2521e-05 - accuracy: 1.0000 - val_loss: 2.3173 - val_accuracy: 0.7383 - 14s/epoch - 34ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0080 - accuracy: 0.9982 - val_loss: 2.2795 - val_accuracy: 0.6850 - 14s/epoch - 34ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.1158e-04 - accuracy: 1.0000 - val_loss: 2.1832 - val_accuracy: 0.6971 - 14s/epoch - 33ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 6.0439e-05 - accuracy: 1.0000 - val_loss: 2.5382 - val_accuracy: 0.6846 - 14s/epoch - 34ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.8025e-05 - accuracy: 1.0000 - val_loss: 2.0966 - val_accuracy: 0.7085 - 14s/epoch - 35ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.9119e-05 - accuracy: 1.0000 - val_loss: 2.3198 - val_accuracy: 0.7004 - 14s/epoch - 35ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 1.8635 - val_accuracy: 0.7033 - 15s/epoch - 35ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.3376e-04 - accuracy: 1.0000 - val_loss: 2.2825 - val_accuracy: 0.6983 - 14s/epoch - 35ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 7.6031e-05 - accuracy: 1.0000 - val_loss: 2.4549 - val_accuracy: 0.6900 - 14s/epoch - 35ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.0269e-04 - accuracy: 1.0000 - val_loss: 2.1772 - val_accuracy: 0.7200 - 14s/epoch - 35ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.7259e-05 - accuracy: 1.0000 - val_loss: 2.4860 - val_accuracy: 0.7004 - 14s/epoch - 35ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.6509e-05 - accuracy: 1.0000 - val_loss: 2.2285 - val_accuracy: 0.7135 - 14s/epoch - 35ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.2997e-05 - accuracy: 1.0000 - val_loss: 2.5810 - val_accuracy: 0.6917 - 15s/epoch - 35ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.91606\n",
      "413/413 - 15s - loss: 1.4838e-05 - accuracy: 1.0000 - val_loss: 2.5151 - val_accuracy: 0.6969 - 15s/epoch - 35ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 4.9416e-05 - accuracy: 1.0000 - val_loss: 2.7555 - val_accuracy: 0.6923 - 14s/epoch - 35ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0098 - accuracy: 0.9975 - val_loss: 2.3549 - val_accuracy: 0.6515 - 14s/epoch - 34ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 8.5390e-04 - accuracy: 0.9998 - val_loss: 2.1775 - val_accuracy: 0.7077 - 14s/epoch - 35ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.7952e-04 - accuracy: 1.0000 - val_loss: 2.6046 - val_accuracy: 0.6681 - 14s/epoch - 35ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 5.9426e-05 - accuracy: 1.0000 - val_loss: 2.5122 - val_accuracy: 0.6842 - 14s/epoch - 34ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.6500e-05 - accuracy: 1.0000 - val_loss: 2.5977 - val_accuracy: 0.6760 - 14s/epoch - 35ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.8652e-05 - accuracy: 1.0000 - val_loss: 2.7638 - val_accuracy: 0.6727 - 14s/epoch - 35ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 2.5026e-05 - accuracy: 1.0000 - val_loss: 2.8570 - val_accuracy: 0.6660 - 14s/epoch - 35ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.8825e-05 - accuracy: 1.0000 - val_loss: 2.7152 - val_accuracy: 0.6771 - 14s/epoch - 35ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 1.3935e-05 - accuracy: 1.0000 - val_loss: 2.6372 - val_accuracy: 0.6837 - 14s/epoch - 35ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 3.1523e-05 - accuracy: 1.0000 - val_loss: 2.5021 - val_accuracy: 0.6996 - 14s/epoch - 35ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.91606\n",
      "413/413 - 14s - loss: 0.0085 - accuracy: 0.9977 - val_loss: 1.8747 - val_accuracy: 0.7040 - 14s/epoch - 35ms/step\n",
      "179/179 [==============================] - 2s 10ms/step\n",
      "Classification accuracy: 0.515873 \n"
     ]
    }
   ],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7180625  0.20691341]\n",
      " [0.41300786 0.28644618]\n",
      " [0.81810886 0.07328607]\n",
      " ...\n",
      " [0.8570372  0.09127623]\n",
      " [0.6757814  0.37289137]\n",
      " [0.5810559  0.29089224]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[1813 1487]\n",
      " [1610  790]]\n",
      "[45.67 52.97 34.69]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for ShallowConvNet on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)\n",
    "\n",
    "plot_confusion_matrix(preds_Shallow, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for ShallowConvNet on ICA data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6df28382c6e70c1c1d3c02fa7b17b6f3b6fcf9f5d22d2410beebd122bfaf45e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
