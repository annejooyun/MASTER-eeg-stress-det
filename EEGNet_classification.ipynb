{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from load_data import load_data\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification\n",
    "import utils.variables as v\n",
    "\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P010_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_002\n",
      "ERROR:root:1) Failed to read data for recording P020_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P023_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs: \n",
      " ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n",
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n",
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n",
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n",
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n",
      "(44, 8, 38400)\n",
      "(13200, 8, 128)\n",
      "(13200, 1)\n",
      "(5700, 8, 128)\n",
      "(5700, 1)\n",
      "(4800, 8, 128)\n",
      "(4800, 1)\n",
      "Shape of train data set: (13200, 8, 128)\n",
      "Shape of train labels set: (13200, 1)\n",
      "Shape of validation data set: (4800, 8, 128)\n",
      "Shape of validation labels set: (4800, 1)\n",
      "Shape of test data set: (5700, 8, 128)\n",
      "Shape of test labels set: (5700, 1)\n"
     ]
    }
   ],
   "source": [
    "data_type = 'new_ica'\n",
    "label_type = 'stai'\n",
    "\n",
    "train_data, test_data, val_data, train_labels, test_labels, val_labels = load_data(data_type, label_type, epoched = True, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "new_ica\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.73389, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 12s - loss: 0.9681 - accuracy: 0.4318 - val_loss: 0.7339 - val_accuracy: 0.3125 - 12s/epoch - 58ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.73389\n",
      "207/207 - 7s - loss: 0.9640 - accuracy: 0.4318 - val_loss: 0.7606 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.73389\n",
      "207/207 - 7s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7595 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.73389\n",
      "207/207 - 8s - loss: 0.9631 - accuracy: 0.4317 - val_loss: 0.8592 - val_accuracy: 0.3125 - 8s/epoch - 39ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.73389\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.9926 - val_accuracy: 0.3125 - 7s/epoch - 31ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.73389\n",
      "207/207 - 10s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8358 - val_accuracy: 0.3125 - 10s/epoch - 47ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 0.73389 to 0.66395, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 7s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.6640 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 0.66395 to 0.66080, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 8s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6608 - val_accuracy: 0.6875 - 8s/epoch - 41ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7960 - val_accuracy: 0.3125 - 9s/epoch - 43ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7630 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7833 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7659 - val_accuracy: 0.3125 - 8s/epoch - 40ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6950 - val_accuracy: 0.3137 - 7s/epoch - 32ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.8223 - val_accuracy: 0.3125 - 7s/epoch - 35ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8130 - val_accuracy: 0.3125 - 9s/epoch - 46ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7424 - val_accuracy: 0.3125 - 8s/epoch - 41ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7830 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8495 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7920 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7719 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7283 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.8062 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7768 - val_accuracy: 0.3125 - 8s/epoch - 38ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.66080\n",
      "207/207 - 10s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8312 - val_accuracy: 0.3125 - 10s/epoch - 47ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7984 - val_accuracy: 0.3125 - 9s/epoch - 43ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8005 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7903 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7605 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8203 - val_accuracy: 0.3125 - 8s/epoch - 37ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8418 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.66080\n",
      "207/207 - 10s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7718 - val_accuracy: 0.3125 - 10s/epoch - 49ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7989 - val_accuracy: 0.3125 - 8s/epoch - 39ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7882 - val_accuracy: 0.3125 - 8s/epoch - 40ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8077 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8745 - val_accuracy: 0.3125 - 9s/epoch - 43ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.8096 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.66080\n",
      "207/207 - 10s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8080 - val_accuracy: 0.3125 - 10s/epoch - 46ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8252 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7963 - val_accuracy: 0.3125 - 8s/epoch - 41ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7910 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8154 - val_accuracy: 0.3125 - 9s/epoch - 43ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7902 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.66080\n",
      "207/207 - 10s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8456 - val_accuracy: 0.3125 - 10s/epoch - 47ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.66080\n",
      "207/207 - 10s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7460 - val_accuracy: 0.3125 - 10s/epoch - 47ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7783 - val_accuracy: 0.3125 - 9s/epoch - 43ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8039 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7815 - val_accuracy: 0.3125 - 9s/epoch - 44ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7943 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7784 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7936 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.66080\n",
      "207/207 - 10s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7892 - val_accuracy: 0.3125 - 10s/epoch - 47ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7986 - val_accuracy: 0.3125 - 9s/epoch - 45ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7829 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7902 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.66080\n",
      "207/207 - 10s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7828 - val_accuracy: 0.3125 - 10s/epoch - 50ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7857 - val_accuracy: 0.3125 - 9s/epoch - 43ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7983 - val_accuracy: 0.3125 - 8s/epoch - 39ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7980 - val_accuracy: 0.3125 - 7s/epoch - 34ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7927 - val_accuracy: 0.3125 - 8s/epoch - 38ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7757 - val_accuracy: 0.3125 - 8s/epoch - 37ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.9039 - val_accuracy: 0.3125 - 9s/epoch - 44ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7880 - val_accuracy: 0.3125 - 9s/epoch - 43ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7721 - val_accuracy: 0.3125 - 7s/epoch - 36ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8309 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7737 - val_accuracy: 0.3125 - 8s/epoch - 39ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8064 - val_accuracy: 0.3125 - 8s/epoch - 39ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8103 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7996 - val_accuracy: 0.3125 - 7s/epoch - 34ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.9477 - val_accuracy: 0.3125 - 8s/epoch - 40ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7556 - val_accuracy: 0.3125 - 7s/epoch - 34ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7591 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8245 - val_accuracy: 0.3125 - 7s/epoch - 35ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.9523 - val_accuracy: 0.3125 - 7s/epoch - 34ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.6894 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7739 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7946 - val_accuracy: 0.3125 - 4s/epoch - 18ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7980 - val_accuracy: 0.3125 - 4s/epoch - 19ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9617 - accuracy: 0.4318 - val_loss: 0.7791 - val_accuracy: 0.3125 - 4s/epoch - 19ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7935 - val_accuracy: 0.3125 - 4s/epoch - 19ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.8142 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7853 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8354 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7929 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8186 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8313 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7779 - val_accuracy: 0.3125 - 4s/epoch - 20ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.66080\n",
      "207/207 - 4s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7845 - val_accuracy: 0.3125 - 4s/epoch - 21ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7791 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7668 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7964 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8027 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7858 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7786 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9617 - accuracy: 0.4318 - val_loss: 0.9087 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9621 - accuracy: 0.4321 - val_loss: 0.9625 - val_accuracy: 0.3125 - 9s/epoch - 44ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7791 - val_accuracy: 0.3125 - 8s/epoch - 39ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8039 - val_accuracy: 0.3125 - 7s/epoch - 34ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8079 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7977 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7965 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8130 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8122 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7656 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7912 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7650 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7796 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7905 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7954 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8016 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7949 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9617 - accuracy: 0.4318 - val_loss: 0.8038 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8631 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7732 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8247 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.8100 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8038 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7780 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8052 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7740 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7998 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8416 - val_accuracy: 0.3125 - 5s/epoch - 26ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.66080\n",
      "207/207 - 5s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8145 - val_accuracy: 0.3125 - 5s/epoch - 25ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7932 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7917 - val_accuracy: 0.3125 - 6s/epoch - 28ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7922 - val_accuracy: 0.3125 - 6s/epoch - 28ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7948 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.8184 - val_accuracy: 0.3125 - 6s/epoch - 28ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8187 - val_accuracy: 0.3125 - 6s/epoch - 27ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7984 - val_accuracy: 0.3125 - 6s/epoch - 28ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7844 - val_accuracy: 0.3125 - 6s/epoch - 28ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7725 - val_accuracy: 0.3125 - 6s/epoch - 28ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8064 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8013 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7907 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7805 - val_accuracy: 0.3125 - 7s/epoch - 35ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7895 - val_accuracy: 0.3125 - 8s/epoch - 37ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7688 - val_accuracy: 0.3125 - 8s/epoch - 38ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8011 - val_accuracy: 0.3125 - 8s/epoch - 40ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7853 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7917 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7727 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8009 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7946 - val_accuracy: 0.3125 - 8s/epoch - 41ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7980 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7821 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7943 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7890 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7957 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7995 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7936 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7992 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8039 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7973 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.8009 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7982 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8006 - val_accuracy: 0.3125 - 8s/epoch - 41ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7962 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7979 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7995 - val_accuracy: 0.3125 - 9s/epoch - 41ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8043 - val_accuracy: 0.3125 - 8s/epoch - 41ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7917 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7864 - val_accuracy: 0.3125 - 8s/epoch - 38ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7892 - val_accuracy: 0.3125 - 8s/epoch - 38ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9617 - accuracy: 0.4318 - val_loss: 0.7882 - val_accuracy: 0.3125 - 7s/epoch - 36ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7965 - val_accuracy: 0.3125 - 7s/epoch - 35ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8366 - val_accuracy: 0.3125 - 7s/epoch - 36ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7936 - val_accuracy: 0.3125 - 7s/epoch - 34ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7871 - val_accuracy: 0.3125 - 7s/epoch - 35ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7863 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7899 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.8052 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7976 - val_accuracy: 0.3125 - 7s/epoch - 31ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.8008 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7875 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8000 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7976 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8007 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7910 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7888 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7803 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7862 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7945 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7911 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7977 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7851 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7900 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7918 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7861 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7944 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7860 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7945 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7948 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7954 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7970 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7931 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7940 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7890 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7858 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7984 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7957 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7885 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7792 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.8060 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7820 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9617 - accuracy: 0.4318 - val_loss: 0.8133 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.8048 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7884 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7924 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7902 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7957 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7914 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7946 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7468 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7761 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7913 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7904 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7939 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7887 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7953 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7931 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7960 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7943 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7921 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7957 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7883 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7977 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7949 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7947 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7929 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7935 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7937 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7933 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7942 - val_accuracy: 0.3125 - 9s/epoch - 44ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7932 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7899 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7897 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7918 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7881 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7942 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7891 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7984 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7956 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7927 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7929 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7941 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7979 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7942 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7933 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7947 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8261 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8205 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.8022 - val_accuracy: 0.3125 - 7s/epoch - 35ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.66080\n",
      "207/207 - 9s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7790 - val_accuracy: 0.3125 - 9s/epoch - 42ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7931 - val_accuracy: 0.3125 - 8s/epoch - 37ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.66080\n",
      "207/207 - 8s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7943 - val_accuracy: 0.3125 - 8s/epoch - 39ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.8049 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7895 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7948 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7912 - val_accuracy: 0.3125 - 7s/epoch - 32ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.66080\n",
      "207/207 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7856 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7950 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7920 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7920 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7920 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7903 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9616 - accuracy: 0.4318 - val_loss: 0.7975 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8002 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7944 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7953 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7938 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7897 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7978 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7887 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7877 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7918 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7944 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7895 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7908 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7938 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7940 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7906 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7908 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8016 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7983 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7897 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7955 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7888 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9617 - accuracy: 0.4318 - val_loss: 0.7788 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7916 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7915 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7953 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7889 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7908 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8010 - val_accuracy: 0.3125 - 6s/epoch - 28ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7917 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8010 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8039 - val_accuracy: 0.3125 - 6s/epoch - 29ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7980 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.7990 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.66080\n",
      "207/207 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.7906 - val_accuracy: 0.3125 - 6s/epoch - 30ms/step\n",
      "179/179 [==============================] - 1s 6ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    }
   ],
   "source": [
    "probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[3300    0]\n",
      " [2400    0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for EEGNet on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)\n",
    "\n",
    "plot_confusion_matrix(preds_EEGNet, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for EEGNet on ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.82909, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 39s - loss: 1.1461 - accuracy: 0.4356 - val_loss: 0.8291 - val_accuracy: 0.3125 - 39s/epoch - 95ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 0.82909 to 0.75849, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 37s - loss: 1.0273 - accuracy: 0.4330 - val_loss: 0.7585 - val_accuracy: 0.3162 - 37s/epoch - 90ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 0.75849 to 0.71273, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 37s - loss: 1.0155 - accuracy: 0.4318 - val_loss: 0.7127 - val_accuracy: 0.6850 - 37s/epoch - 89ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.71273\n",
      "413/413 - 37s - loss: 1.0013 - accuracy: 0.4326 - val_loss: 0.7859 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.71273\n",
      "413/413 - 38s - loss: 1.0116 - accuracy: 0.4370 - val_loss: 1.7612 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.71273\n",
      "413/413 - 38s - loss: 1.0116 - accuracy: 0.4315 - val_loss: 0.8590 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 0.71273 to 0.70777, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 37s - loss: 1.0074 - accuracy: 0.4338 - val_loss: 0.7078 - val_accuracy: 0.6875 - 37s/epoch - 90ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.70777\n",
      "413/413 - 37s - loss: 1.0095 - accuracy: 0.4319 - val_loss: 0.7089 - val_accuracy: 0.6862 - 37s/epoch - 89ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.70777\n",
      "413/413 - 37s - loss: 1.0253 - accuracy: 0.4331 - val_loss: 0.7614 - val_accuracy: 0.3200 - 37s/epoch - 90ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 0.70777 to 0.65269, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 37s - loss: 1.0047 - accuracy: 0.4322 - val_loss: 0.6527 - val_accuracy: 0.6875 - 37s/epoch - 90ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.65269\n",
      "413/413 - 41s - loss: 1.0075 - accuracy: 0.4318 - val_loss: 1.0049 - val_accuracy: 0.3125 - 41s/epoch - 99ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.65269\n",
      "413/413 - 39s - loss: 0.9990 - accuracy: 0.4316 - val_loss: 1.0311 - val_accuracy: 0.3125 - 39s/epoch - 94ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.65269\n",
      "413/413 - 40s - loss: 0.9947 - accuracy: 0.4319 - val_loss: 0.8021 - val_accuracy: 0.3125 - 40s/epoch - 98ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.65269\n",
      "413/413 - 38s - loss: 1.0015 - accuracy: 0.4316 - val_loss: 0.8772 - val_accuracy: 0.3137 - 38s/epoch - 91ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.65269\n",
      "413/413 - 36s - loss: 1.0022 - accuracy: 0.4317 - val_loss: 0.8891 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.65269\n",
      "413/413 - 38s - loss: 0.9939 - accuracy: 0.4320 - val_loss: 0.8465 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.65269\n",
      "413/413 - 37s - loss: 1.0024 - accuracy: 0.4327 - val_loss: 0.8099 - val_accuracy: 0.3137 - 37s/epoch - 89ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.65269\n",
      "413/413 - 36s - loss: 0.9967 - accuracy: 0.4318 - val_loss: 0.7871 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.65269\n",
      "413/413 - 36s - loss: 0.9965 - accuracy: 0.4323 - val_loss: 0.9093 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.65269\n",
      "413/413 - 37s - loss: 0.9840 - accuracy: 0.4317 - val_loss: 0.7867 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.65269\n",
      "413/413 - 37s - loss: 0.9872 - accuracy: 0.4319 - val_loss: 0.8629 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.65269\n",
      "413/413 - 37s - loss: 0.9831 - accuracy: 0.4319 - val_loss: 0.7327 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.65269 to 0.64767, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 36s - loss: 0.9909 - accuracy: 0.4317 - val_loss: 0.6477 - val_accuracy: 0.6875 - 36s/epoch - 88ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.64767\n",
      "413/413 - 36s - loss: 0.9870 - accuracy: 0.4315 - val_loss: 0.7901 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9830 - accuracy: 0.4318 - val_loss: 0.8392 - val_accuracy: 0.3125 - 37s/epoch - 91ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.64767\n",
      "413/413 - 38s - loss: 0.9853 - accuracy: 0.4318 - val_loss: 0.9346 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9881 - accuracy: 0.4320 - val_loss: 0.8073 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.64767\n",
      "413/413 - 36s - loss: 0.9821 - accuracy: 0.4317 - val_loss: 0.7793 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.64767\n",
      "413/413 - 35s - loss: 0.9739 - accuracy: 0.4318 - val_loss: 0.8133 - val_accuracy: 0.3125 - 35s/epoch - 85ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9731 - accuracy: 0.4318 - val_loss: 0.8007 - val_accuracy: 0.3125 - 37s/epoch - 88ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.64767\n",
      "413/413 - 36s - loss: 0.9760 - accuracy: 0.4318 - val_loss: 0.9584 - val_accuracy: 0.3125 - 36s/epoch - 87ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.64767\n",
      "413/413 - 35s - loss: 0.9835 - accuracy: 0.4316 - val_loss: 0.7526 - val_accuracy: 0.3125 - 35s/epoch - 84ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.64767\n",
      "413/413 - 38s - loss: 0.9730 - accuracy: 0.4318 - val_loss: 0.8124 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9696 - accuracy: 0.4318 - val_loss: 1.0648 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9778 - accuracy: 0.4318 - val_loss: 0.8146 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9841 - accuracy: 0.4317 - val_loss: 0.8504 - val_accuracy: 0.3125 - 37s/epoch - 91ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.64767\n",
      "413/413 - 40s - loss: 0.9722 - accuracy: 0.4318 - val_loss: 0.8782 - val_accuracy: 0.3125 - 40s/epoch - 96ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9737 - accuracy: 0.4317 - val_loss: 0.8054 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.64767\n",
      "413/413 - 37s - loss: 0.9778 - accuracy: 0.4319 - val_loss: 0.8349 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.64767\n",
      "413/413 - 40s - loss: 0.9681 - accuracy: 0.4318 - val_loss: 0.8087 - val_accuracy: 0.3125 - 40s/epoch - 96ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.64767\n",
      "413/413 - 38s - loss: 0.9654 - accuracy: 0.4318 - val_loss: 0.7887 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss improved from 0.64767 to 0.63768, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 38s - loss: 0.9688 - accuracy: 0.4317 - val_loss: 0.6377 - val_accuracy: 0.6875 - 38s/epoch - 92ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9791 - accuracy: 0.4317 - val_loss: 0.7669 - val_accuracy: 0.3125 - 37s/epoch - 91ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9775 - accuracy: 0.4319 - val_loss: 0.6982 - val_accuracy: 0.6862 - 36s/epoch - 88ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9727 - accuracy: 0.4318 - val_loss: 0.7898 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9678 - accuracy: 0.4319 - val_loss: 0.8024 - val_accuracy: 0.3125 - 37s/epoch - 91ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9671 - accuracy: 0.4318 - val_loss: 0.8039 - val_accuracy: 0.3125 - 35s/epoch - 84ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9669 - accuracy: 0.4318 - val_loss: 0.8032 - val_accuracy: 0.3125 - 36s/epoch - 87ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9664 - accuracy: 0.4318 - val_loss: 0.7618 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9661 - accuracy: 0.4318 - val_loss: 0.7909 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9686 - accuracy: 0.4318 - val_loss: 0.7885 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9722 - accuracy: 0.4318 - val_loss: 0.8023 - val_accuracy: 0.3125 - 36s/epoch - 87ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9668 - accuracy: 0.4318 - val_loss: 0.8069 - val_accuracy: 0.3125 - 36s/epoch - 87ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9660 - accuracy: 0.4319 - val_loss: 0.8019 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9660 - accuracy: 0.4318 - val_loss: 0.7893 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9653 - accuracy: 0.4319 - val_loss: 0.7699 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9689 - accuracy: 0.4318 - val_loss: 0.8093 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9670 - accuracy: 0.4318 - val_loss: 0.8076 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.8225 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9677 - accuracy: 0.4318 - val_loss: 0.8029 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9673 - accuracy: 0.4318 - val_loss: 0.7966 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9650 - accuracy: 0.4318 - val_loss: 0.7779 - val_accuracy: 0.3125 - 39s/epoch - 95ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9650 - accuracy: 0.4318 - val_loss: 0.7991 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9657 - accuracy: 0.4318 - val_loss: 0.8169 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9676 - accuracy: 0.4320 - val_loss: 0.8095 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9721 - accuracy: 0.4318 - val_loss: 0.7939 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9708 - accuracy: 0.4318 - val_loss: 0.8094 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9673 - accuracy: 0.4318 - val_loss: 0.7985 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9669 - accuracy: 0.4318 - val_loss: 0.8004 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9651 - accuracy: 0.4318 - val_loss: 0.7996 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9679 - accuracy: 0.4319 - val_loss: 0.8315 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9674 - accuracy: 0.4318 - val_loss: 0.7949 - val_accuracy: 0.3125 - 37s/epoch - 91ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9666 - accuracy: 0.4318 - val_loss: 0.7621 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9716 - accuracy: 0.4318 - val_loss: 0.8107 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9685 - accuracy: 0.4318 - val_loss: 0.8094 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9709 - accuracy: 0.4318 - val_loss: 0.8095 - val_accuracy: 0.3125 - 35s/epoch - 85ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9719 - accuracy: 0.4318 - val_loss: 0.8362 - val_accuracy: 0.3125 - 35s/epoch - 85ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9707 - accuracy: 0.4318 - val_loss: 0.8133 - val_accuracy: 0.3125 - 35s/epoch - 84ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.63768\n",
      "413/413 - 33s - loss: 0.9688 - accuracy: 0.4318 - val_loss: 0.8041 - val_accuracy: 0.3125 - 33s/epoch - 81ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9684 - accuracy: 0.4318 - val_loss: 0.7985 - val_accuracy: 0.3125 - 35s/epoch - 84ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9708 - accuracy: 0.4318 - val_loss: 0.8234 - val_accuracy: 0.3125 - 37s/epoch - 91ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9721 - accuracy: 0.4318 - val_loss: 0.7773 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.63768\n",
      "413/413 - 41s - loss: 0.9689 - accuracy: 0.4318 - val_loss: 0.8239 - val_accuracy: 0.3125 - 41s/epoch - 100ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9690 - accuracy: 0.4318 - val_loss: 0.8416 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9695 - accuracy: 0.4318 - val_loss: 0.8040 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.63768\n",
      "413/413 - 41s - loss: 0.9665 - accuracy: 0.4318 - val_loss: 0.7882 - val_accuracy: 0.3125 - 41s/epoch - 100ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9672 - accuracy: 0.4318 - val_loss: 0.7925 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9676 - accuracy: 0.4318 - val_loss: 0.8242 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9677 - accuracy: 0.4318 - val_loss: 0.8234 - val_accuracy: 0.3125 - 39s/epoch - 94ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9667 - accuracy: 0.4318 - val_loss: 0.7839 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9687 - accuracy: 0.4318 - val_loss: 0.7885 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9724 - accuracy: 0.4318 - val_loss: 0.7946 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9732 - accuracy: 0.4318 - val_loss: 0.8145 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9704 - accuracy: 0.4318 - val_loss: 0.8150 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9685 - accuracy: 0.4318 - val_loss: 0.8045 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9703 - accuracy: 0.4318 - val_loss: 0.7885 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9704 - accuracy: 0.4318 - val_loss: 0.8227 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9705 - accuracy: 0.4318 - val_loss: 0.8037 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9686 - accuracy: 0.4318 - val_loss: 0.7640 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.8268 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9699 - accuracy: 0.4318 - val_loss: 0.8203 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9709 - accuracy: 0.4318 - val_loss: 0.7817 - val_accuracy: 0.3125 - 39s/epoch - 93ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9691 - accuracy: 0.4318 - val_loss: 0.7978 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9670 - accuracy: 0.4318 - val_loss: 0.8201 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9680 - accuracy: 0.4318 - val_loss: 0.8050 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9672 - accuracy: 0.4318 - val_loss: 0.7731 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9670 - accuracy: 0.4318 - val_loss: 0.7900 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9679 - accuracy: 0.4318 - val_loss: 0.7987 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9689 - accuracy: 0.4318 - val_loss: 0.8446 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.63768\n",
      "413/413 - 41s - loss: 0.9704 - accuracy: 0.4318 - val_loss: 0.7883 - val_accuracy: 0.3125 - 41s/epoch - 98ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9711 - accuracy: 0.4318 - val_loss: 0.8270 - val_accuracy: 0.3125 - 36s/epoch - 86ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.63768\n",
      "413/413 - 34s - loss: 0.9674 - accuracy: 0.4318 - val_loss: 0.8072 - val_accuracy: 0.3125 - 34s/epoch - 81ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.63768\n",
      "413/413 - 33s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.7948 - val_accuracy: 0.3125 - 33s/epoch - 79ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.63768\n",
      "413/413 - 33s - loss: 0.9671 - accuracy: 0.4318 - val_loss: 0.8023 - val_accuracy: 0.3125 - 33s/epoch - 80ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9660 - accuracy: 0.4318 - val_loss: 0.7960 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9662 - accuracy: 0.4318 - val_loss: 0.7254 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9667 - accuracy: 0.4318 - val_loss: 0.8013 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9675 - accuracy: 0.4318 - val_loss: 0.8098 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9665 - accuracy: 0.4318 - val_loss: 0.7931 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9664 - accuracy: 0.4318 - val_loss: 0.8139 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9657 - accuracy: 0.4318 - val_loss: 0.8231 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9673 - accuracy: 0.4318 - val_loss: 0.8113 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9699 - accuracy: 0.4318 - val_loss: 0.7886 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9687 - accuracy: 0.4318 - val_loss: 0.8680 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9707 - accuracy: 0.4318 - val_loss: 0.8068 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9708 - accuracy: 0.4318 - val_loss: 0.8006 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9685 - accuracy: 0.4318 - val_loss: 0.8004 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9672 - accuracy: 0.4318 - val_loss: 0.8110 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9683 - accuracy: 0.4318 - val_loss: 0.8009 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9679 - accuracy: 0.4318 - val_loss: 0.7849 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9689 - accuracy: 0.4318 - val_loss: 0.7935 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9684 - accuracy: 0.4318 - val_loss: 0.7846 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9686 - accuracy: 0.4318 - val_loss: 0.8022 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.63768\n",
      "413/413 - 42s - loss: 0.9705 - accuracy: 0.4318 - val_loss: 0.8196 - val_accuracy: 0.3125 - 42s/epoch - 101ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9729 - accuracy: 0.4318 - val_loss: 0.7977 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9703 - accuracy: 0.4318 - val_loss: 0.7999 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9680 - accuracy: 0.4318 - val_loss: 0.8120 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9678 - accuracy: 0.4318 - val_loss: 0.7918 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9692 - accuracy: 0.4318 - val_loss: 0.7797 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9683 - accuracy: 0.4319 - val_loss: 0.8022 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9671 - accuracy: 0.4318 - val_loss: 0.8232 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9692 - accuracy: 0.4318 - val_loss: 0.7822 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9700 - accuracy: 0.4318 - val_loss: 0.8037 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9703 - accuracy: 0.4318 - val_loss: 0.8166 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9694 - accuracy: 0.4318 - val_loss: 0.8147 - val_accuracy: 0.3125 - 39s/epoch - 94ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9705 - accuracy: 0.4316 - val_loss: 0.7645 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9701 - accuracy: 0.4318 - val_loss: 0.7926 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.8217 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9676 - accuracy: 0.4318 - val_loss: 0.7805 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9676 - accuracy: 0.4318 - val_loss: 0.7888 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9668 - accuracy: 0.4318 - val_loss: 0.8061 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9671 - accuracy: 0.4318 - val_loss: 0.7891 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.8540 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9697 - accuracy: 0.4318 - val_loss: 0.8073 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9699 - accuracy: 0.4318 - val_loss: 0.8217 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9703 - accuracy: 0.4318 - val_loss: 0.8304 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9700 - accuracy: 0.4318 - val_loss: 0.8883 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.63768\n",
      "413/413 - 42s - loss: 0.9681 - accuracy: 0.4318 - val_loss: 0.9063 - val_accuracy: 0.3125 - 42s/epoch - 101ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9697 - accuracy: 0.4318 - val_loss: 0.8029 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9669 - accuracy: 0.4318 - val_loss: 0.7971 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9662 - accuracy: 0.4318 - val_loss: 0.8024 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9681 - accuracy: 0.4318 - val_loss: 0.8008 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9706 - accuracy: 0.4318 - val_loss: 0.7534 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9707 - accuracy: 0.4318 - val_loss: 0.8005 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9708 - accuracy: 0.4318 - val_loss: 0.7900 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9675 - accuracy: 0.4318 - val_loss: 0.7695 - val_accuracy: 0.3125 - 39s/epoch - 94ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9674 - accuracy: 0.4318 - val_loss: 0.7982 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9697 - accuracy: 0.4318 - val_loss: 0.8183 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9716 - accuracy: 0.4318 - val_loss: 0.8277 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9718 - accuracy: 0.4318 - val_loss: 0.8153 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.8040 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9688 - accuracy: 0.4318 - val_loss: 0.7893 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9720 - accuracy: 0.4318 - val_loss: 0.8097 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9685 - accuracy: 0.4317 - val_loss: 0.7601 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9681 - accuracy: 0.4318 - val_loss: 0.8015 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9667 - accuracy: 0.4318 - val_loss: 0.7536 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9686 - accuracy: 0.4318 - val_loss: 0.7893 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9681 - accuracy: 0.4318 - val_loss: 0.7727 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9660 - accuracy: 0.4318 - val_loss: 0.7877 - val_accuracy: 0.3125 - 39s/epoch - 94ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9662 - accuracy: 0.4318 - val_loss: 0.7957 - val_accuracy: 0.3125 - 39s/epoch - 95ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.63768\n",
      "413/413 - 44s - loss: 0.9687 - accuracy: 0.4318 - val_loss: 0.7876 - val_accuracy: 0.3125 - 44s/epoch - 107ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.63768\n",
      "413/413 - 41s - loss: 0.9684 - accuracy: 0.4318 - val_loss: 0.7978 - val_accuracy: 0.3125 - 41s/epoch - 99ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9691 - accuracy: 0.4325 - val_loss: 0.7354 - val_accuracy: 0.3654 - 38s/epoch - 92ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9696 - accuracy: 0.4318 - val_loss: 0.8113 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9691 - accuracy: 0.4319 - val_loss: 0.8203 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9676 - accuracy: 0.4318 - val_loss: 0.7665 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9665 - accuracy: 0.4318 - val_loss: 0.8350 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9664 - accuracy: 0.4318 - val_loss: 0.7936 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4319 - val_loss: 0.7687 - val_accuracy: 0.3587 - 38s/epoch - 92ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9697 - accuracy: 0.4318 - val_loss: 0.7982 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9688 - accuracy: 0.4317 - val_loss: 0.7989 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9678 - accuracy: 0.4318 - val_loss: 0.8112 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9664 - accuracy: 0.4318 - val_loss: 0.8038 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9665 - accuracy: 0.4318 - val_loss: 0.8082 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9668 - accuracy: 0.4318 - val_loss: 0.7954 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9659 - accuracy: 0.4318 - val_loss: 0.7803 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9651 - accuracy: 0.4318 - val_loss: 0.7966 - val_accuracy: 0.3125 - 39s/epoch - 93ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9653 - accuracy: 0.4318 - val_loss: 0.7855 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9664 - accuracy: 0.4318 - val_loss: 0.7982 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9650 - accuracy: 0.4318 - val_loss: 0.8080 - val_accuracy: 0.3137 - 38s/epoch - 91ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9670 - accuracy: 0.4318 - val_loss: 0.7933 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9677 - accuracy: 0.4318 - val_loss: 0.8536 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9673 - accuracy: 0.4317 - val_loss: 0.7888 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9669 - accuracy: 0.4318 - val_loss: 0.8021 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.63768\n",
      "413/413 - 42s - loss: 0.9678 - accuracy: 0.4317 - val_loss: 0.7105 - val_accuracy: 0.3944 - 42s/epoch - 103ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.63768\n",
      "413/413 - 43s - loss: 0.9690 - accuracy: 0.4317 - val_loss: 0.8005 - val_accuracy: 0.3125 - 43s/epoch - 103ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.63768\n",
      "413/413 - 44s - loss: 0.9688 - accuracy: 0.4317 - val_loss: 0.8080 - val_accuracy: 0.3125 - 44s/epoch - 106ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.63768\n",
      "413/413 - 40s - loss: 0.9679 - accuracy: 0.4318 - val_loss: 0.7607 - val_accuracy: 0.3125 - 40s/epoch - 97ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9686 - accuracy: 0.4318 - val_loss: 0.7959 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9676 - accuracy: 0.4318 - val_loss: 0.7954 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.8016 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9678 - accuracy: 0.4318 - val_loss: 0.8175 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9684 - accuracy: 0.4318 - val_loss: 0.7800 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9664 - accuracy: 0.4318 - val_loss: 0.8135 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9672 - accuracy: 0.4318 - val_loss: 0.8070 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.7687 - val_accuracy: 0.3137 - 38s/epoch - 92ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9695 - accuracy: 0.4318 - val_loss: 0.8120 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9671 - accuracy: 0.4318 - val_loss: 0.8032 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9676 - accuracy: 0.4318 - val_loss: 0.7816 - val_accuracy: 0.3125 - 36s/epoch - 87ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9692 - accuracy: 0.4318 - val_loss: 0.8123 - val_accuracy: 0.3125 - 35s/epoch - 85ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9689 - accuracy: 0.4317 - val_loss: 0.7915 - val_accuracy: 0.3125 - 36s/epoch - 87ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9708 - accuracy: 0.4322 - val_loss: 0.7586 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9710 - accuracy: 0.4317 - val_loss: 0.8035 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9700 - accuracy: 0.4318 - val_loss: 0.8054 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9696 - accuracy: 0.4319 - val_loss: 0.8045 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9672 - accuracy: 0.4318 - val_loss: 0.8002 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9675 - accuracy: 0.4318 - val_loss: 0.7956 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.63768\n",
      "413/413 - 40s - loss: 0.9680 - accuracy: 0.4318 - val_loss: 0.8068 - val_accuracy: 0.3125 - 40s/epoch - 96ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.63768\n",
      "413/413 - 40s - loss: 0.9674 - accuracy: 0.4318 - val_loss: 0.7998 - val_accuracy: 0.3125 - 40s/epoch - 97ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9677 - accuracy: 0.4319 - val_loss: 0.8064 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9695 - accuracy: 0.4320 - val_loss: 0.8397 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9669 - accuracy: 0.4318 - val_loss: 0.7831 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9675 - accuracy: 0.4318 - val_loss: 0.8015 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9687 - accuracy: 0.4318 - val_loss: 0.8039 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9663 - accuracy: 0.4318 - val_loss: 0.7838 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9690 - accuracy: 0.4317 - val_loss: 0.8355 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9702 - accuracy: 0.4318 - val_loss: 0.8410 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9700 - accuracy: 0.4318 - val_loss: 0.7991 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9679 - accuracy: 0.4318 - val_loss: 0.8194 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9681 - accuracy: 0.4318 - val_loss: 0.8276 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9686 - accuracy: 0.4317 - val_loss: 0.8603 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9686 - accuracy: 0.4318 - val_loss: 0.7532 - val_accuracy: 0.3150 - 38s/epoch - 91ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9699 - accuracy: 0.4317 - val_loss: 0.7960 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9709 - accuracy: 0.4318 - val_loss: 0.8241 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9707 - accuracy: 0.4319 - val_loss: 0.7534 - val_accuracy: 0.3200 - 38s/epoch - 92ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9695 - accuracy: 0.4319 - val_loss: 0.7941 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9702 - accuracy: 0.4318 - val_loss: 0.7924 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9679 - accuracy: 0.4318 - val_loss: 0.7725 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9693 - accuracy: 0.4318 - val_loss: 0.8058 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9687 - accuracy: 0.4318 - val_loss: 0.7870 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9685 - accuracy: 0.4318 - val_loss: 0.7972 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.63768\n",
      "413/413 - 40s - loss: 0.9666 - accuracy: 0.4318 - val_loss: 0.7856 - val_accuracy: 0.3125 - 40s/epoch - 98ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9670 - accuracy: 0.4318 - val_loss: 0.7975 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9658 - accuracy: 0.4318 - val_loss: 0.8027 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9667 - accuracy: 0.4318 - val_loss: 0.8071 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9677 - accuracy: 0.4318 - val_loss: 0.8053 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9671 - accuracy: 0.4318 - val_loss: 0.8052 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9662 - accuracy: 0.4318 - val_loss: 0.7984 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9661 - accuracy: 0.4318 - val_loss: 0.7893 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9654 - accuracy: 0.4322 - val_loss: 0.8621 - val_accuracy: 0.3125 - 39s/epoch - 94ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9710 - accuracy: 0.4317 - val_loss: 0.8262 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9708 - accuracy: 0.4317 - val_loss: 0.8155 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9694 - accuracy: 0.4318 - val_loss: 0.7936 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9722 - accuracy: 0.4320 - val_loss: 0.8079 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9698 - accuracy: 0.4318 - val_loss: 0.7982 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9699 - accuracy: 0.4317 - val_loss: 0.8046 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9674 - accuracy: 0.4318 - val_loss: 0.7928 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9685 - accuracy: 0.4317 - val_loss: 0.8269 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9668 - accuracy: 0.4318 - val_loss: 0.8139 - val_accuracy: 0.3125 - 39s/epoch - 94ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.63768\n",
      "413/413 - 43s - loss: 0.9656 - accuracy: 0.4318 - val_loss: 0.7985 - val_accuracy: 0.3125 - 43s/epoch - 104ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.63768\n",
      "413/413 - 43s - loss: 0.9668 - accuracy: 0.4318 - val_loss: 0.7727 - val_accuracy: 0.3125 - 43s/epoch - 104ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.63768\n",
      "413/413 - 43s - loss: 0.9662 - accuracy: 0.4318 - val_loss: 0.7630 - val_accuracy: 0.3125 - 43s/epoch - 104ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.63768\n",
      "413/413 - 44s - loss: 0.9695 - accuracy: 0.4319 - val_loss: 0.8075 - val_accuracy: 0.3137 - 44s/epoch - 106ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9688 - accuracy: 0.4317 - val_loss: 0.7784 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.63768\n",
      "413/413 - 42s - loss: 0.9666 - accuracy: 0.4317 - val_loss: 0.8085 - val_accuracy: 0.3125 - 42s/epoch - 102ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.63768\n",
      "413/413 - 41s - loss: 0.9690 - accuracy: 0.4318 - val_loss: 0.8148 - val_accuracy: 0.3125 - 41s/epoch - 99ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9689 - accuracy: 0.4319 - val_loss: 0.7968 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9679 - accuracy: 0.4318 - val_loss: 0.8236 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.8023 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9682 - accuracy: 0.4318 - val_loss: 0.7835 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9692 - accuracy: 0.4318 - val_loss: 0.7956 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.63768\n",
      "413/413 - 34s - loss: 0.9688 - accuracy: 0.4318 - val_loss: 0.7998 - val_accuracy: 0.3125 - 34s/epoch - 83ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9673 - accuracy: 0.4318 - val_loss: 0.7984 - val_accuracy: 0.3125 - 35s/epoch - 85ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9658 - accuracy: 0.4318 - val_loss: 0.7876 - val_accuracy: 0.3125 - 39s/epoch - 93ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.63768\n",
      "413/413 - 38s - loss: 0.9661 - accuracy: 0.4318 - val_loss: 0.8109 - val_accuracy: 0.3125 - 38s/epoch - 91ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9675 - accuracy: 0.4318 - val_loss: 0.8390 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9683 - accuracy: 0.4318 - val_loss: 0.8014 - val_accuracy: 0.3125 - 37s/epoch - 89ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9660 - accuracy: 0.4318 - val_loss: 0.7951 - val_accuracy: 0.3125 - 37s/epoch - 90ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.63768\n",
      "413/413 - 37s - loss: 0.9663 - accuracy: 0.4318 - val_loss: 0.7963 - val_accuracy: 0.3125 - 37s/epoch - 91ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.63768\n",
      "413/413 - 36s - loss: 0.9662 - accuracy: 0.4318 - val_loss: 0.8283 - val_accuracy: 0.3125 - 36s/epoch - 88ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.63768\n",
      "413/413 - 34s - loss: 0.9691 - accuracy: 0.4318 - val_loss: 0.7961 - val_accuracy: 0.3125 - 34s/epoch - 82ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.63768\n",
      "413/413 - 32s - loss: 0.9680 - accuracy: 0.4319 - val_loss: 0.8029 - val_accuracy: 0.3125 - 32s/epoch - 78ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.63768\n",
      "413/413 - 33s - loss: 0.9700 - accuracy: 0.4318 - val_loss: 0.7769 - val_accuracy: 0.3125 - 33s/epoch - 81ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9709 - accuracy: 0.4318 - val_loss: 0.7875 - val_accuracy: 0.3125 - 35s/epoch - 84ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9697 - accuracy: 0.4318 - val_loss: 0.7629 - val_accuracy: 0.3125 - 35s/epoch - 85ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9711 - accuracy: 0.4318 - val_loss: 0.7937 - val_accuracy: 0.3125 - 35s/epoch - 85ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.63768\n",
      "413/413 - 34s - loss: 0.9692 - accuracy: 0.4318 - val_loss: 0.8349 - val_accuracy: 0.3125 - 34s/epoch - 83ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.63768\n",
      "413/413 - 34s - loss: 0.9689 - accuracy: 0.4318 - val_loss: 0.8010 - val_accuracy: 0.3125 - 34s/epoch - 83ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.63768\n",
      "413/413 - 35s - loss: 0.9674 - accuracy: 0.4318 - val_loss: 0.7784 - val_accuracy: 0.3125 - 35s/epoch - 86ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.63768\n",
      "413/413 - 39s - loss: 0.9705 - accuracy: 0.4319 - val_loss: 0.8453 - val_accuracy: 0.3125 - 39s/epoch - 95ms/step\n",
      "179/179 [==============================] - 4s 23ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63754857 0.36245146]\n",
      " [0.62820315 0.37179688]\n",
      " [0.6293789  0.3706212 ]\n",
      " ...\n",
      " [0.6241371  0.3758629 ]\n",
      " [0.63444257 0.36555746]\n",
      " [0.6286932  0.3713068 ]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[3300    0]\n",
      " [2400    0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for TSGL on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)\n",
    "\n",
    "plot_confusion_matrix(preds_TSGL, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for TSGL on ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.72523, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 11s - loss: 1.0591 - accuracy: 0.4670 - val_loss: 0.7252 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.72523\n",
      "413/413 - 9s - loss: 1.0192 - accuracy: 0.4661 - val_loss: 2.1803 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.72523\n",
      "413/413 - 9s - loss: 1.0172 - accuracy: 0.4695 - val_loss: 2.0179 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.72523\n",
      "413/413 - 10s - loss: 1.0149 - accuracy: 0.4568 - val_loss: 3.4360 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.72523\n",
      "413/413 - 10s - loss: 1.0122 - accuracy: 0.4570 - val_loss: 2.2777 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.72523\n",
      "413/413 - 10s - loss: 1.0017 - accuracy: 0.4611 - val_loss: 1.3204 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 0.72523 to 0.70989, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 10s - loss: 0.9986 - accuracy: 0.4591 - val_loss: 0.7099 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 0.70989 to 0.62131, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 10s - loss: 0.9945 - accuracy: 0.4583 - val_loss: 0.6213 - val_accuracy: 0.6875 - 10s/epoch - 25ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9927 - accuracy: 0.4532 - val_loss: 0.6233 - val_accuracy: 0.6875 - 10s/epoch - 25ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9928 - accuracy: 0.4542 - val_loss: 1.1599 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9886 - accuracy: 0.4494 - val_loss: 0.9651 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9840 - accuracy: 0.4565 - val_loss: 2.0388 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9836 - accuracy: 0.4573 - val_loss: 0.6243 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9854 - accuracy: 0.4481 - val_loss: 0.6934 - val_accuracy: 0.4521 - 10s/epoch - 24ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9825 - accuracy: 0.4470 - val_loss: 0.6398 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9818 - accuracy: 0.4448 - val_loss: 0.7184 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9798 - accuracy: 0.4477 - val_loss: 0.8219 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9792 - accuracy: 0.4459 - val_loss: 0.7531 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9803 - accuracy: 0.4433 - val_loss: 0.6233 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9799 - accuracy: 0.4385 - val_loss: 0.6222 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9773 - accuracy: 0.4392 - val_loss: 1.0036 - val_accuracy: 0.3125 - 10s/epoch - 23ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9761 - accuracy: 0.4402 - val_loss: 0.8115 - val_accuracy: 0.3125 - 10s/epoch - 23ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.62131\n",
      "413/413 - 9s - loss: 0.9771 - accuracy: 0.4429 - val_loss: 1.2802 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.62131\n",
      "413/413 - 9s - loss: 0.9746 - accuracy: 0.4413 - val_loss: 1.4294 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.62131\n",
      "413/413 - 10s - loss: 0.9757 - accuracy: 0.4439 - val_loss: 0.6224 - val_accuracy: 0.6875 - 10s/epoch - 23ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.62131\n",
      "413/413 - 9s - loss: 0.9738 - accuracy: 0.4402 - val_loss: 0.7979 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.62131\n",
      "413/413 - 9s - loss: 0.9734 - accuracy: 0.4374 - val_loss: 0.7799 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.62131\n",
      "413/413 - 9s - loss: 0.9769 - accuracy: 0.4400 - val_loss: 0.9013 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.62131 to 0.62115, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 9s - loss: 0.9709 - accuracy: 0.4413 - val_loss: 0.6212 - val_accuracy: 0.6875 - 9s/epoch - 22ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9731 - accuracy: 0.4372 - val_loss: 0.6510 - val_accuracy: 0.6875 - 10s/epoch - 23ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9737 - accuracy: 0.4350 - val_loss: 1.0224 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9728 - accuracy: 0.4361 - val_loss: 0.8070 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9729 - accuracy: 0.4373 - val_loss: 0.7184 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9712 - accuracy: 0.4383 - val_loss: 1.0405 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9707 - accuracy: 0.4356 - val_loss: 0.8730 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9710 - accuracy: 0.4341 - val_loss: 0.9027 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9705 - accuracy: 0.4347 - val_loss: 0.8338 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9713 - accuracy: 0.4335 - val_loss: 0.6358 - val_accuracy: 0.6875 - 9s/epoch - 22ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9716 - accuracy: 0.4329 - val_loss: 0.6227 - val_accuracy: 0.6875 - 9s/epoch - 22ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9686 - accuracy: 0.4326 - val_loss: 0.7458 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9692 - accuracy: 0.4339 - val_loss: 0.7649 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9672 - accuracy: 0.4336 - val_loss: 1.1417 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9693 - accuracy: 0.4312 - val_loss: 0.6512 - val_accuracy: 0.6875 - 9s/epoch - 22ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9678 - accuracy: 0.4315 - val_loss: 0.7234 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9683 - accuracy: 0.4323 - val_loss: 0.7583 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9666 - accuracy: 0.4324 - val_loss: 0.8048 - val_accuracy: 0.3125 - 10s/epoch - 23ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9665 - accuracy: 0.4336 - val_loss: 0.6576 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9673 - accuracy: 0.4324 - val_loss: 1.0108 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9664 - accuracy: 0.4324 - val_loss: 0.7486 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9646 - accuracy: 0.4324 - val_loss: 0.8248 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9663 - accuracy: 0.4316 - val_loss: 0.7315 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9659 - accuracy: 0.4317 - val_loss: 0.6327 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9648 - accuracy: 0.4317 - val_loss: 0.8037 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9646 - accuracy: 0.4320 - val_loss: 0.7819 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9655 - accuracy: 0.4318 - val_loss: 0.7945 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9643 - accuracy: 0.4320 - val_loss: 0.8337 - val_accuracy: 0.3125 - 10s/epoch - 23ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9647 - accuracy: 0.4319 - val_loss: 0.8091 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9638 - accuracy: 0.4318 - val_loss: 0.8193 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9644 - accuracy: 0.4319 - val_loss: 0.7813 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7844 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7383 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9633 - accuracy: 0.4321 - val_loss: 0.7641 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7945 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7706 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7328 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9643 - accuracy: 0.4318 - val_loss: 0.7613 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.8122 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7654 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8192 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.7843 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.8179 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7612 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.8341 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.8015 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7856 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8296 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8881 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.8122 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.8291 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7852 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.9678 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7901 - val_accuracy: 0.3125 - 10s/epoch - 23ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7995 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.7894 - val_accuracy: 0.3125 - 11s/epoch - 25ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.8616 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7996 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.8993 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.8199 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8420 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.62115\n",
      "413/413 - 15s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.8209 - val_accuracy: 0.3125 - 15s/epoch - 36ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.62115\n",
      "413/413 - 14s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6305 - val_accuracy: 0.6875 - 14s/epoch - 33ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.62115\n",
      "413/413 - 13s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7212 - val_accuracy: 0.3125 - 13s/epoch - 31ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.62115\n",
      "413/413 - 13s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 1.0868 - val_accuracy: 0.3125 - 13s/epoch - 30ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.62115\n",
      "413/413 - 13s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7038 - val_accuracy: 0.3125 - 13s/epoch - 31ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7448 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9638 - accuracy: 0.4318 - val_loss: 0.8019 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8172 - val_accuracy: 0.3125 - 11s/epoch - 25ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8310 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.8220 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7789 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8441 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.6699 - val_accuracy: 0.6875 - 10s/epoch - 25ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7015 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.7840 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8328 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.8008 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7623 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.8020 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9624 - accuracy: 0.4319 - val_loss: 0.8107 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7845 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9626 - accuracy: 0.4317 - val_loss: 0.7718 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.8050 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.8331 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8000 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7307 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9639 - accuracy: 0.4318 - val_loss: 0.7498 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8059 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7636 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.6833 - val_accuracy: 0.6875 - 12s/epoch - 30ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7883 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8367 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.8574 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9638 - accuracy: 0.4318 - val_loss: 0.7811 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.8408 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7889 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7282 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7552 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8111 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7876 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.9448 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8063 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7399 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.7208 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8389 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8360 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7792 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7523 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7910 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7898 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7389 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.8043 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.6750 - val_accuracy: 0.6875 - 10s/epoch - 25ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8532 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.7423 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8568 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.9274 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.8248 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7974 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7810 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8819 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8131 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7699 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.8295 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7795 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7975 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.8905 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7675 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7796 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.8188 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7942 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8079 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7859 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9625 - accuracy: 0.4320 - val_loss: 0.8177 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4317 - val_loss: 0.8197 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7459 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7778 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.8299 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8063 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.62115\n",
      "413/413 - 15s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8874 - val_accuracy: 0.3125 - 15s/epoch - 36ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.7830 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7633 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.8648 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8161 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7935 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7810 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7710 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7962 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7775 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7713 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.7676 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.8210 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.8466 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7780 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.62115\n",
      "413/413 - 13s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.8216 - val_accuracy: 0.3125 - 13s/epoch - 32ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8392 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.7985 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.7225 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7987 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7447 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.7907 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7945 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9638 - accuracy: 0.4318 - val_loss: 0.7770 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7360 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7936 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7875 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7874 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.8293 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.7802 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7805 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8176 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8057 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8134 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7556 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7999 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.7891 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7973 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8783 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.8026 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7666 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7708 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7935 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8301 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7713 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7929 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7706 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7907 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8012 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7647 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.7830 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7633 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7507 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.7913 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7841 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.7941 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8541 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8260 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9639 - accuracy: 0.4318 - val_loss: 0.7911 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.8006 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7153 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7476 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7963 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7090 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7966 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7851 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.8158 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4319 - val_loss: 0.7284 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.8111 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7598 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7749 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7616 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.8368 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8041 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7049 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7838 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7799 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7705 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.7916 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8250 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7602 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.7767 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7947 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7546 - val_accuracy: 0.3125 - 12s/epoch - 28ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9627 - accuracy: 0.4319 - val_loss: 0.7793 - val_accuracy: 0.3125 - 12s/epoch - 29ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9638 - accuracy: 0.4318 - val_loss: 0.7250 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7903 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.62115\n",
      "413/413 - 12s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.8160 - val_accuracy: 0.3125 - 12s/epoch - 30ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7257 - val_accuracy: 0.3125 - 11s/epoch - 28ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7689 - val_accuracy: 0.3125 - 11s/epoch - 26ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.8098 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.7836 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7561 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8110 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7619 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7789 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7960 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7830 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7712 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.7961 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.7957 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.8658 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8481 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.8303 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9636 - accuracy: 0.4318 - val_loss: 0.8499 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.62115\n",
      "413/413 - 11s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7641 - val_accuracy: 0.3125 - 11s/epoch - 27ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7821 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8040 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.6929 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4317 - val_loss: 0.7459 - val_accuracy: 0.3125 - 10s/epoch - 23ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.7765 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8212 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.8511 - val_accuracy: 0.3125 - 9s/epoch - 22ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.62115\n",
      "413/413 - 9s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.8080 - val_accuracy: 0.3125 - 9s/epoch - 23ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9635 - accuracy: 0.4318 - val_loss: 0.6828 - val_accuracy: 0.6875 - 10s/epoch - 23ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.6461 - val_accuracy: 0.6875 - 10s/epoch - 24ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.8095 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9632 - accuracy: 0.4319 - val_loss: 0.8751 - val_accuracy: 0.3125 - 10s/epoch - 23ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.8005 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7766 - val_accuracy: 0.3125 - 10s/epoch - 24ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8276 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9634 - accuracy: 0.4318 - val_loss: 0.6895 - val_accuracy: 0.6875 - 10s/epoch - 25ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7187 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7140 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7552 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.8059 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.7664 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4318 - val_loss: 0.7816 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9633 - accuracy: 0.4318 - val_loss: 0.7429 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9637 - accuracy: 0.4318 - val_loss: 0.7452 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9632 - accuracy: 0.4318 - val_loss: 0.8001 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.62115\n",
      "413/413 - 10s - loss: 0.9631 - accuracy: 0.4317 - val_loss: 0.8193 - val_accuracy: 0.3125 - 10s/epoch - 25ms/step\n",
      "179/179 [==============================] - 1s 6ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6172634  0.42878726]\n",
      " [0.6169782  0.42953387]\n",
      " [0.6170041  0.4297525 ]\n",
      " ...\n",
      " [0.61698014 0.4297645 ]\n",
      " [0.6171377  0.4294967 ]\n",
      " [0.61699086 0.4296414 ]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[3300    0]\n",
      " [2400    0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for DeepConvNet on New_ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)\n",
    "plot_confusion_matrix(preds_Deep, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for DeepConvNet on New_ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.51594, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 8s - loss: 1.8804 - accuracy: 0.4880 - val_loss: 1.5159 - val_accuracy: 0.3125 - 8s/epoch - 18ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 1.51594 to 0.86213, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 7s - loss: 1.1960 - accuracy: 0.4706 - val_loss: 0.8621 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 0.86213 to 0.62793, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 6s - loss: 1.1498 - accuracy: 0.4707 - val_loss: 0.6279 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.1692 - accuracy: 0.4764 - val_loss: 0.8783 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.1639 - accuracy: 0.4774 - val_loss: 0.7388 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.1084 - accuracy: 0.4742 - val_loss: 0.6489 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.1235 - accuracy: 0.4745 - val_loss: 0.7789 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.1222 - accuracy: 0.4737 - val_loss: 0.6627 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.1002 - accuracy: 0.4748 - val_loss: 0.9450 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.0952 - accuracy: 0.4689 - val_loss: 0.7157 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.0705 - accuracy: 0.4763 - val_loss: 1.8073 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.0764 - accuracy: 0.4738 - val_loss: 1.0450 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.62793\n",
      "413/413 - 7s - loss: 1.0617 - accuracy: 0.4618 - val_loss: 0.8280 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.0543 - accuracy: 0.4708 - val_loss: 0.9966 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.0510 - accuracy: 0.4724 - val_loss: 0.7941 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.0333 - accuracy: 0.4642 - val_loss: 1.2260 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.62793\n",
      "413/413 - 6s - loss: 1.0450 - accuracy: 0.4632 - val_loss: 0.7761 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.62793\n",
      "413/413 - 7s - loss: 1.0452 - accuracy: 0.4570 - val_loss: 0.7695 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.62793\n",
      "413/413 - 7s - loss: 1.0199 - accuracy: 0.4627 - val_loss: 0.7965 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.62793 to 0.62130, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 6s - loss: 1.0177 - accuracy: 0.4610 - val_loss: 0.6213 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 1.0176 - accuracy: 0.4555 - val_loss: 1.0496 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 1.0009 - accuracy: 0.4558 - val_loss: 1.0806 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 1.0078 - accuracy: 0.4576 - val_loss: 0.6531 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9996 - accuracy: 0.4594 - val_loss: 0.6895 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9910 - accuracy: 0.4542 - val_loss: 1.8156 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9964 - accuracy: 0.4538 - val_loss: 0.6274 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9873 - accuracy: 0.4492 - val_loss: 0.7891 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9853 - accuracy: 0.4477 - val_loss: 0.6230 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9808 - accuracy: 0.4484 - val_loss: 0.6433 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9756 - accuracy: 0.4455 - val_loss: 0.7306 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9720 - accuracy: 0.4428 - val_loss: 0.6325 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9690 - accuracy: 0.4438 - val_loss: 1.6661 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9677 - accuracy: 0.4336 - val_loss: 0.7092 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9677 - accuracy: 0.4318 - val_loss: 1.1347 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9684 - accuracy: 0.4314 - val_loss: 1.0547 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9655 - accuracy: 0.4321 - val_loss: 0.8704 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9643 - accuracy: 0.4320 - val_loss: 0.6934 - val_accuracy: 0.4625 - 6s/epoch - 14ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9643 - accuracy: 0.4318 - val_loss: 0.6623 - val_accuracy: 0.6875 - 6s/epoch - 14ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9644 - accuracy: 0.4318 - val_loss: 0.7723 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 0.6537 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7891 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.1490 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.7075 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.6303 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9629 - accuracy: 0.4318 - val_loss: 0.7760 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 1.3426 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 1.1086 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6227 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 1.3930 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9628 - accuracy: 0.4318 - val_loss: 0.9013 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 1.4866 - val_accuracy: 0.6875 - 7s/epoch - 17ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.62130\n",
      "413/413 - 8s - loss: 0.9630 - accuracy: 0.4318 - val_loss: 1.4546 - val_accuracy: 0.6875 - 8s/epoch - 20ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.0184 - val_accuracy: 0.6875 - 7s/epoch - 17ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.9481 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.1972 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.6328 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 1.3204 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.6794 - val_accuracy: 0.6590 - 6s/epoch - 15ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.6225 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8118 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.1183 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.3766 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.3506 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.1163 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.62130\n",
      "413/413 - 7s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7386 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.4182 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4319 - val_loss: 0.7552 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4319 - val_loss: 1.0712 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6263 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6271 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.6230 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.4028 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.2437 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7031 - val_accuracy: 0.3300 - 6s/epoch - 15ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.6741 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7013 - val_accuracy: 0.3162 - 6s/epoch - 15ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.6372 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.3966 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6293 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6479 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7874 - val_accuracy: 0.3425 - 6s/epoch - 15ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6245 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8171 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 1.9258 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.4630 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.3419 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.3881 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6451 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6619 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.3699 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7915 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 2.3849 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6240 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8709 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6227 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6225 - val_accuracy: 0.6875 - 6s/epoch - 14ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8319 - val_accuracy: 0.6875 - 6s/epoch - 14ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8077 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.9920 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7882 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6503 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.6966 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.62130\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7537 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 0.62130 to 0.62127, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6213 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.9033 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 1.1432 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.0381 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6215 - val_accuracy: 0.6875 - 6s/epoch - 14ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.0236 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.9764 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4319 - val_loss: 1.6840 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7801 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8119 - val_accuracy: 0.6875 - 6s/epoch - 14ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7022 - val_accuracy: 0.4096 - 6s/epoch - 14ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.6277 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8532 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6317 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6476 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 1.1728 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 2.0992 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.9793 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7458 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 1.0001 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6645 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6758 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.6613 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6894 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6533 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6285 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6594 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.6632 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7183 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.7065 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8602 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6224 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6247 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7574 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6348 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9617 - accuracy: 0.4318 - val_loss: 0.9277 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6868 - val_accuracy: 0.6862 - 6s/epoch - 15ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6897 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.2851 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 2.2473 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.4504 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.6313 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8072 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6474 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.8142 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8132 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.8852 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7701 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6348 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.6547 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.9407 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8051 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8138 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7240 - val_accuracy: 0.3150 - 6s/epoch - 15ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 2.0950 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.8078 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.9177 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.2024 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.2276 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8540 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6270 - val_accuracy: 0.6875 - 6s/epoch - 14ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8450 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.4026 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.6217 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7738 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.1782 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7192 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7898 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.3748 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.8409 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.6248 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8473 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.0092 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.1101 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8019 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.9297 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.9419 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.8385 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.6360 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.8221 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8317 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.0550 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.9968 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7381 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8726 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.8603 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7311 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.9639 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7263 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.1978 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.6217 - val_accuracy: 0.6875 - 7s/epoch - 17ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.62127\n",
      "413/413 - 9s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.6230 - val_accuracy: 0.6875 - 9s/epoch - 21ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6573 - val_accuracy: 0.6875 - 7s/epoch - 16ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8347 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.0396 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.7725 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.6238 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7549 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.0107 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.1261 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7902 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7707 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7017 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9619 - accuracy: 0.4318 - val_loss: 0.6402 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7663 - val_accuracy: 0.3354 - 6s/epoch - 15ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7211 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7573 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8475 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 1.0322 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7769 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6965 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7199 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7456 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7596 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7242 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8957 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7667 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7776 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 1.0853 - val_accuracy: 0.3125 - 6s/epoch - 14ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 1.0070 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7422 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7543 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.6939 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6589 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6994 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.9197 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7435 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.9415 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7774 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7213 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7716 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9627 - accuracy: 0.4318 - val_loss: 0.7183 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7778 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7953 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.8120 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9618 - accuracy: 0.4318 - val_loss: 0.6634 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7691 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7819 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7505 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8446 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.62127\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7878 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8178 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7905 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7057 - val_accuracy: 0.3125 - 7s/epoch - 17ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8025 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7667 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.62127\n",
      "413/413 - 7s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.9325 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss improved from 0.62127 to 0.62110, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6211 - val_accuracy: 0.6875 - 6s/epoch - 16ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8150 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8285 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.7154 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8118 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6297 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.9059 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7201 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8988 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8265 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7758 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 1.7426 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7111 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.8186 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7470 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6418 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8328 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8351 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7074 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.62110\n",
      "413/413 - 7s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.8070 - val_accuracy: 0.3125 - 7s/epoch - 16ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7255 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7476 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7583 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.9249 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9620 - accuracy: 0.4318 - val_loss: 0.7135 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9616 - accuracy: 0.4318 - val_loss: 0.8667 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.8950 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6680 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7510 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7145 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.7586 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9621 - accuracy: 0.4318 - val_loss: 0.7885 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7337 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8503 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.9426 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7323 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8444 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6488 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9626 - accuracy: 0.4318 - val_loss: 0.8086 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7261 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.8658 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.6856 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.8154 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7834 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9625 - accuracy: 0.4318 - val_loss: 0.7677 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 0.7520 - val_accuracy: 0.3125 - 6s/epoch - 16ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9624 - accuracy: 0.4318 - val_loss: 1.6000 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7572 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9622 - accuracy: 0.4318 - val_loss: 0.7965 - val_accuracy: 0.3125 - 6s/epoch - 15ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.62110\n",
      "413/413 - 6s - loss: 0.9623 - accuracy: 0.4318 - val_loss: 0.6513 - val_accuracy: 0.6875 - 6s/epoch - 15ms/step\n",
      "179/179 [==============================] - 1s 4ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    }
   ],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59571314 0.40428677]\n",
      " [0.595711   0.40428892]\n",
      " [0.59599423 0.40400556]\n",
      " ...\n",
      " [0.59600323 0.40399668]\n",
      " [0.595964   0.40403587]\n",
      " [0.5957991  0.4042009 ]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[3300    0]\n",
      " [2400    0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for ShallowConvNet on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)\n",
    "\n",
    "plot_confusion_matrix(preds_Shallow, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for ShallowConvNet on ICA data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6df28382c6e70c1c1d3c02fa7b17b6f3b6fcf9f5d22d2410beebd122bfaf45e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
