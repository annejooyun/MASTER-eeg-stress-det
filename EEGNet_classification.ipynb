{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from load_data import load_data\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification\n",
    "import utils.variables as v\n",
    "\n",
    "from pyriemann.utils.viz import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs: \n",
      " ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n",
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n",
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n",
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n",
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n",
      "(44, 8, 75000)\n",
      "(13200, 8, 250)\n",
      "(13200, 1)\n",
      "(5700, 8, 250)\n",
      "(5700, 1)\n",
      "(4800, 8, 250)\n",
      "(4800, 1)\n",
      "Shape of train data set: (13200, 8, 250)\n",
      "Shape of train labels set: (13200, 1)\n",
      "Shape of validation data set: (4800, 8, 250)\n",
      "Shape of validation labels set: (4800, 1)\n",
      "Shape of test data set: (5700, 8, 250)\n",
      "Shape of test labels set: (5700, 1)\n"
     ]
    }
   ],
   "source": [
    "data_type = 'ica'\n",
    "label_type = 'stai'\n",
    "\n",
    "train_data, test_data, val_data, train_labels, test_labels, val_labels = load_data(data_type, label_type, epoched = True, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "ica\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.65654, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 8s - loss: 0.5846 - accuracy: 0.6800 - val_loss: 0.6565 - val_accuracy: 0.6900 - 8s/epoch - 40ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.65654\n",
      "207/207 - 7s - loss: 0.3587 - accuracy: 0.8564 - val_loss: 0.7098 - val_accuracy: 0.6538 - 7s/epoch - 34ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.65654\n",
      "207/207 - 8s - loss: 0.2485 - accuracy: 0.9183 - val_loss: 0.9208 - val_accuracy: 0.6692 - 8s/epoch - 38ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.65654\n",
      "207/207 - 8s - loss: 0.2106 - accuracy: 0.9359 - val_loss: 1.0450 - val_accuracy: 0.6562 - 8s/epoch - 37ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.1702 - accuracy: 0.9501 - val_loss: 1.1265 - val_accuracy: 0.6602 - 9s/epoch - 43ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.1541 - accuracy: 0.9558 - val_loss: 1.3150 - val_accuracy: 0.6690 - 9s/epoch - 44ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.1302 - accuracy: 0.9651 - val_loss: 1.4531 - val_accuracy: 0.6652 - 9s/epoch - 45ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.1136 - accuracy: 0.9688 - val_loss: 1.5145 - val_accuracy: 0.6660 - 9s/epoch - 45ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.1139 - accuracy: 0.9684 - val_loss: 1.3101 - val_accuracy: 0.6658 - 9s/epoch - 45ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0946 - accuracy: 0.9766 - val_loss: 1.4741 - val_accuracy: 0.6642 - 9s/epoch - 45ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0965 - accuracy: 0.9730 - val_loss: 1.5495 - val_accuracy: 0.6629 - 10s/epoch - 46ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0970 - accuracy: 0.9729 - val_loss: 1.4187 - val_accuracy: 0.6646 - 10s/epoch - 50ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0950 - accuracy: 0.9740 - val_loss: 1.2512 - val_accuracy: 0.6698 - 10s/epoch - 49ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0775 - accuracy: 0.9794 - val_loss: 1.3045 - val_accuracy: 0.6706 - 10s/epoch - 48ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0677 - accuracy: 0.9820 - val_loss: 1.5348 - val_accuracy: 0.6677 - 12s/epoch - 57ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0662 - accuracy: 0.9829 - val_loss: 1.4805 - val_accuracy: 0.6671 - 10s/epoch - 50ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0716 - accuracy: 0.9819 - val_loss: 1.3987 - val_accuracy: 0.6750 - 12s/epoch - 58ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0606 - accuracy: 0.9825 - val_loss: 1.4069 - val_accuracy: 0.6881 - 10s/epoch - 47ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0565 - accuracy: 0.9853 - val_loss: 1.3954 - val_accuracy: 0.6892 - 10s/epoch - 46ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0677 - accuracy: 0.9819 - val_loss: 1.0818 - val_accuracy: 0.7538 - 10s/epoch - 46ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0489 - accuracy: 0.9865 - val_loss: 1.3968 - val_accuracy: 0.7358 - 10s/epoch - 46ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0521 - accuracy: 0.9851 - val_loss: 1.3596 - val_accuracy: 0.7504 - 10s/epoch - 46ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0445 - accuracy: 0.9881 - val_loss: 1.4479 - val_accuracy: 0.7412 - 9s/epoch - 46ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0520 - accuracy: 0.9863 - val_loss: 1.3971 - val_accuracy: 0.7800 - 9s/epoch - 46ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0453 - accuracy: 0.9873 - val_loss: 1.4665 - val_accuracy: 0.7785 - 10s/epoch - 47ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0546 - accuracy: 0.9855 - val_loss: 1.5147 - val_accuracy: 0.7492 - 9s/epoch - 46ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0334 - accuracy: 0.9909 - val_loss: 1.5121 - val_accuracy: 0.7890 - 9s/epoch - 46ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0448 - accuracy: 0.9886 - val_loss: 1.4797 - val_accuracy: 0.7940 - 9s/epoch - 46ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0351 - accuracy: 0.9910 - val_loss: 1.5746 - val_accuracy: 0.7956 - 10s/epoch - 46ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0518 - accuracy: 0.9864 - val_loss: 1.4825 - val_accuracy: 0.7921 - 10s/epoch - 48ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0347 - accuracy: 0.9905 - val_loss: 1.5617 - val_accuracy: 0.7862 - 10s/epoch - 50ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0379 - accuracy: 0.9908 - val_loss: 1.6705 - val_accuracy: 0.7887 - 11s/epoch - 52ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0467 - accuracy: 0.9865 - val_loss: 1.4876 - val_accuracy: 0.7846 - 11s/epoch - 53ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0361 - accuracy: 0.9912 - val_loss: 1.7616 - val_accuracy: 0.7725 - 11s/epoch - 53ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0373 - accuracy: 0.9908 - val_loss: 1.6566 - val_accuracy: 0.7681 - 11s/epoch - 53ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0307 - accuracy: 0.9905 - val_loss: 1.6394 - val_accuracy: 0.7775 - 11s/epoch - 53ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0316 - accuracy: 0.9911 - val_loss: 1.6839 - val_accuracy: 0.7794 - 11s/epoch - 53ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0359 - accuracy: 0.9901 - val_loss: 1.7118 - val_accuracy: 0.7865 - 11s/epoch - 53ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0243 - accuracy: 0.9936 - val_loss: 1.9254 - val_accuracy: 0.7792 - 11s/epoch - 53ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0274 - accuracy: 0.9930 - val_loss: 1.5359 - val_accuracy: 0.7933 - 11s/epoch - 53ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0318 - accuracy: 0.9908 - val_loss: 1.5526 - val_accuracy: 0.7850 - 12s/epoch - 56ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.65654\n",
      "207/207 - 13s - loss: 0.0297 - accuracy: 0.9911 - val_loss: 1.9277 - val_accuracy: 0.7844 - 13s/epoch - 62ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.65654\n",
      "207/207 - 13s - loss: 0.0271 - accuracy: 0.9930 - val_loss: 1.9396 - val_accuracy: 0.7794 - 13s/epoch - 62ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.65654\n",
      "207/207 - 14s - loss: 0.0213 - accuracy: 0.9952 - val_loss: 1.8840 - val_accuracy: 0.7846 - 14s/epoch - 68ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.65654\n",
      "207/207 - 15s - loss: 0.0263 - accuracy: 0.9923 - val_loss: 1.9467 - val_accuracy: 0.7837 - 15s/epoch - 71ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0495 - accuracy: 0.9866 - val_loss: 1.6430 - val_accuracy: 0.7792 - 12s/epoch - 57ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0218 - accuracy: 0.9941 - val_loss: 1.9643 - val_accuracy: 0.7692 - 12s/epoch - 56ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.65654\n",
      "207/207 - 13s - loss: 0.1366 - accuracy: 0.9533 - val_loss: 1.4805 - val_accuracy: 0.7760 - 13s/epoch - 61ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0267 - accuracy: 0.9927 - val_loss: 1.8086 - val_accuracy: 0.7765 - 12s/epoch - 57ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0251 - accuracy: 0.9927 - val_loss: 2.0754 - val_accuracy: 0.7542 - 12s/epoch - 59ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0225 - accuracy: 0.9939 - val_loss: 2.0469 - val_accuracy: 0.7671 - 12s/epoch - 57ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0226 - accuracy: 0.9942 - val_loss: 1.9771 - val_accuracy: 0.7556 - 12s/epoch - 60ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0249 - accuracy: 0.9934 - val_loss: 2.0431 - val_accuracy: 0.7567 - 12s/epoch - 59ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0266 - accuracy: 0.9934 - val_loss: 2.0729 - val_accuracy: 0.7460 - 11s/epoch - 54ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0243 - accuracy: 0.9936 - val_loss: 2.0954 - val_accuracy: 0.7369 - 11s/epoch - 55ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0259 - accuracy: 0.9927 - val_loss: 2.0326 - val_accuracy: 0.7217 - 11s/epoch - 54ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0242 - accuracy: 0.9928 - val_loss: 2.2244 - val_accuracy: 0.7146 - 11s/epoch - 53ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0154 - accuracy: 0.9965 - val_loss: 2.2104 - val_accuracy: 0.7410 - 11s/epoch - 53ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0244 - accuracy: 0.9936 - val_loss: 2.2658 - val_accuracy: 0.7369 - 11s/epoch - 54ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0162 - accuracy: 0.9950 - val_loss: 2.3279 - val_accuracy: 0.7265 - 11s/epoch - 53ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0150 - accuracy: 0.9962 - val_loss: 2.3796 - val_accuracy: 0.7365 - 11s/epoch - 53ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0241 - accuracy: 0.9934 - val_loss: 2.2177 - val_accuracy: 0.7387 - 11s/epoch - 53ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0209 - accuracy: 0.9936 - val_loss: 2.3645 - val_accuracy: 0.7440 - 11s/epoch - 53ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0164 - accuracy: 0.9945 - val_loss: 2.2858 - val_accuracy: 0.7362 - 11s/epoch - 52ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0154 - accuracy: 0.9956 - val_loss: 2.3647 - val_accuracy: 0.7377 - 10s/epoch - 50ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0206 - accuracy: 0.9953 - val_loss: 2.4636 - val_accuracy: 0.7192 - 10s/epoch - 50ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0185 - accuracy: 0.9950 - val_loss: 2.3932 - val_accuracy: 0.7225 - 11s/epoch - 51ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0212 - accuracy: 0.9945 - val_loss: 2.2008 - val_accuracy: 0.7538 - 10s/epoch - 50ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0222 - accuracy: 0.9936 - val_loss: 2.4174 - val_accuracy: 0.7040 - 11s/epoch - 53ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0346 - accuracy: 0.9914 - val_loss: 2.3949 - val_accuracy: 0.6967 - 10s/epoch - 47ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0186 - accuracy: 0.9951 - val_loss: 2.4242 - val_accuracy: 0.7021 - 10s/epoch - 47ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0139 - accuracy: 0.9961 - val_loss: 2.6613 - val_accuracy: 0.6981 - 9s/epoch - 45ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0194 - accuracy: 0.9943 - val_loss: 2.3196 - val_accuracy: 0.7444 - 9s/epoch - 45ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0156 - accuracy: 0.9960 - val_loss: 2.5321 - val_accuracy: 0.7150 - 9s/epoch - 45ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0260 - accuracy: 0.9939 - val_loss: 2.7770 - val_accuracy: 0.7027 - 9s/epoch - 45ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0180 - accuracy: 0.9945 - val_loss: 2.5973 - val_accuracy: 0.6981 - 9s/epoch - 45ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0176 - accuracy: 0.9948 - val_loss: 2.7126 - val_accuracy: 0.6975 - 10s/epoch - 50ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0135 - accuracy: 0.9962 - val_loss: 2.8050 - val_accuracy: 0.7002 - 10s/epoch - 47ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0217 - accuracy: 0.9944 - val_loss: 2.4211 - val_accuracy: 0.7050 - 10s/epoch - 48ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.65654\n",
      "207/207 - 9s - loss: 0.0120 - accuracy: 0.9967 - val_loss: 2.7935 - val_accuracy: 0.7054 - 9s/epoch - 45ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0219 - accuracy: 0.9939 - val_loss: 2.9220 - val_accuracy: 0.6998 - 10s/epoch - 46ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0187 - accuracy: 0.9951 - val_loss: 2.8733 - val_accuracy: 0.6923 - 10s/epoch - 48ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0199 - accuracy: 0.9948 - val_loss: 2.3309 - val_accuracy: 0.7185 - 10s/epoch - 49ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0238 - accuracy: 0.9928 - val_loss: 2.6034 - val_accuracy: 0.6981 - 10s/epoch - 50ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0155 - accuracy: 0.9955 - val_loss: 2.6252 - val_accuracy: 0.7015 - 11s/epoch - 52ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0127 - accuracy: 0.9968 - val_loss: 2.9001 - val_accuracy: 0.6983 - 11s/epoch - 52ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0132 - accuracy: 0.9968 - val_loss: 2.9508 - val_accuracy: 0.6977 - 11s/epoch - 53ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0099 - accuracy: 0.9976 - val_loss: 3.1862 - val_accuracy: 0.6992 - 11s/epoch - 52ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0193 - accuracy: 0.9958 - val_loss: 2.6206 - val_accuracy: 0.7010 - 11s/epoch - 52ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0135 - accuracy: 0.9970 - val_loss: 2.7475 - val_accuracy: 0.6996 - 11s/epoch - 53ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0196 - accuracy: 0.9945 - val_loss: 2.6245 - val_accuracy: 0.6977 - 11s/epoch - 52ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0197 - accuracy: 0.9949 - val_loss: 2.6831 - val_accuracy: 0.6971 - 11s/epoch - 52ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0138 - accuracy: 0.9955 - val_loss: 2.8381 - val_accuracy: 0.6973 - 11s/epoch - 52ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0118 - accuracy: 0.9968 - val_loss: 2.9461 - val_accuracy: 0.6960 - 11s/epoch - 54ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0111 - accuracy: 0.9967 - val_loss: 2.8227 - val_accuracy: 0.7231 - 11s/epoch - 55ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0146 - accuracy: 0.9956 - val_loss: 2.8483 - val_accuracy: 0.7006 - 11s/epoch - 52ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0093 - accuracy: 0.9980 - val_loss: 2.7516 - val_accuracy: 0.7025 - 11s/epoch - 55ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0205 - accuracy: 0.9950 - val_loss: 2.5890 - val_accuracy: 0.7000 - 11s/epoch - 52ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0161 - accuracy: 0.9964 - val_loss: 2.5826 - val_accuracy: 0.7073 - 11s/epoch - 53ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0130 - accuracy: 0.9961 - val_loss: 2.5155 - val_accuracy: 0.7377 - 11s/epoch - 52ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0105 - accuracy: 0.9977 - val_loss: 2.3411 - val_accuracy: 0.7446 - 11s/epoch - 52ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0146 - accuracy: 0.9957 - val_loss: 2.6590 - val_accuracy: 0.7148 - 11s/epoch - 52ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0093 - accuracy: 0.9974 - val_loss: 2.9462 - val_accuracy: 0.7021 - 11s/epoch - 52ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0081 - accuracy: 0.9978 - val_loss: 2.7981 - val_accuracy: 0.7065 - 11s/epoch - 52ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0215 - accuracy: 0.9951 - val_loss: 2.3858 - val_accuracy: 0.7165 - 11s/epoch - 54ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0092 - accuracy: 0.9978 - val_loss: 2.7799 - val_accuracy: 0.7010 - 11s/epoch - 53ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0110 - accuracy: 0.9973 - val_loss: 2.9640 - val_accuracy: 0.6944 - 11s/epoch - 53ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0094 - accuracy: 0.9976 - val_loss: 2.8521 - val_accuracy: 0.6979 - 10s/epoch - 51ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0145 - accuracy: 0.9962 - val_loss: 2.7054 - val_accuracy: 0.6881 - 11s/epoch - 52ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0103 - accuracy: 0.9970 - val_loss: 2.8223 - val_accuracy: 0.6990 - 11s/epoch - 52ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0163 - accuracy: 0.9955 - val_loss: 2.8754 - val_accuracy: 0.7004 - 11s/epoch - 52ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0066 - accuracy: 0.9987 - val_loss: 2.9438 - val_accuracy: 0.7015 - 10s/epoch - 50ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0143 - accuracy: 0.9964 - val_loss: 2.9794 - val_accuracy: 0.6940 - 10s/epoch - 49ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0129 - accuracy: 0.9964 - val_loss: 2.7943 - val_accuracy: 0.7023 - 10s/epoch - 51ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0113 - accuracy: 0.9972 - val_loss: 2.7399 - val_accuracy: 0.7008 - 10s/epoch - 49ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0102 - accuracy: 0.9977 - val_loss: 2.9764 - val_accuracy: 0.6983 - 10s/epoch - 50ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0110 - accuracy: 0.9971 - val_loss: 2.7224 - val_accuracy: 0.6983 - 11s/epoch - 52ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0096 - accuracy: 0.9974 - val_loss: 2.8698 - val_accuracy: 0.6960 - 11s/epoch - 52ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0126 - accuracy: 0.9964 - val_loss: 2.7581 - val_accuracy: 0.6900 - 11s/epoch - 53ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0115 - accuracy: 0.9966 - val_loss: 2.8309 - val_accuracy: 0.6921 - 11s/epoch - 51ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0170 - accuracy: 0.9963 - val_loss: 3.1022 - val_accuracy: 0.6869 - 11s/epoch - 51ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0107 - accuracy: 0.9970 - val_loss: 3.3482 - val_accuracy: 0.6865 - 11s/epoch - 53ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0122 - accuracy: 0.9980 - val_loss: 3.1827 - val_accuracy: 0.6908 - 11s/epoch - 52ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0071 - accuracy: 0.9984 - val_loss: 3.5314 - val_accuracy: 0.6877 - 11s/epoch - 52ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0110 - accuracy: 0.9973 - val_loss: 3.1714 - val_accuracy: 0.7008 - 11s/epoch - 52ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0087 - accuracy: 0.9975 - val_loss: 3.3114 - val_accuracy: 0.6917 - 11s/epoch - 52ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0119 - accuracy: 0.9963 - val_loss: 2.9065 - val_accuracy: 0.6931 - 11s/epoch - 52ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0057 - accuracy: 0.9993 - val_loss: 3.1519 - val_accuracy: 0.6973 - 11s/epoch - 52ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0072 - accuracy: 0.9982 - val_loss: 2.8181 - val_accuracy: 0.6944 - 11s/epoch - 52ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0091 - accuracy: 0.9983 - val_loss: 3.4255 - val_accuracy: 0.6935 - 10s/epoch - 50ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0066 - accuracy: 0.9983 - val_loss: 3.4158 - val_accuracy: 0.6992 - 10s/epoch - 50ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0220 - accuracy: 0.9947 - val_loss: 2.6779 - val_accuracy: 0.6983 - 10s/epoch - 49ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0065 - accuracy: 0.9986 - val_loss: 3.2709 - val_accuracy: 0.6965 - 10s/epoch - 48ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0131 - accuracy: 0.9963 - val_loss: 3.2426 - val_accuracy: 0.6956 - 10s/epoch - 48ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0061 - accuracy: 0.9987 - val_loss: 3.1518 - val_accuracy: 0.6988 - 10s/epoch - 47ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0215 - accuracy: 0.9958 - val_loss: 2.8502 - val_accuracy: 0.6965 - 10s/epoch - 48ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 3.2939 - val_accuracy: 0.6915 - 10s/epoch - 47ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0075 - accuracy: 0.9981 - val_loss: 3.1979 - val_accuracy: 0.6946 - 10s/epoch - 47ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0102 - accuracy: 0.9974 - val_loss: 2.9438 - val_accuracy: 0.6931 - 10s/epoch - 47ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0086 - accuracy: 0.9978 - val_loss: 3.2836 - val_accuracy: 0.6938 - 10s/epoch - 47ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0055 - accuracy: 0.9986 - val_loss: 3.3027 - val_accuracy: 0.6954 - 10s/epoch - 48ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0086 - accuracy: 0.9973 - val_loss: 3.4244 - val_accuracy: 0.6906 - 10s/epoch - 50ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0107 - accuracy: 0.9970 - val_loss: 3.4240 - val_accuracy: 0.6894 - 11s/epoch - 52ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0075 - accuracy: 0.9984 - val_loss: 3.4273 - val_accuracy: 0.6915 - 11s/epoch - 52ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0058 - accuracy: 0.9986 - val_loss: 3.5508 - val_accuracy: 0.6948 - 12s/epoch - 56ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0176 - accuracy: 0.9959 - val_loss: 2.9984 - val_accuracy: 0.6935 - 11s/epoch - 53ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0249 - accuracy: 0.9938 - val_loss: 2.4579 - val_accuracy: 0.7019 - 11s/epoch - 53ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0063 - accuracy: 0.9983 - val_loss: 2.9891 - val_accuracy: 0.7000 - 11s/epoch - 53ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0050 - accuracy: 0.9989 - val_loss: 3.3625 - val_accuracy: 0.6950 - 11s/epoch - 53ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0077 - accuracy: 0.9982 - val_loss: 3.1144 - val_accuracy: 0.6948 - 11s/epoch - 53ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0050 - accuracy: 0.9989 - val_loss: 3.4317 - val_accuracy: 0.6985 - 11s/epoch - 53ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0077 - accuracy: 0.9979 - val_loss: 2.9747 - val_accuracy: 0.6992 - 11s/epoch - 53ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0169 - accuracy: 0.9957 - val_loss: 2.6175 - val_accuracy: 0.7002 - 11s/epoch - 53ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0074 - accuracy: 0.9986 - val_loss: 3.3372 - val_accuracy: 0.6917 - 11s/epoch - 52ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0102 - accuracy: 0.9977 - val_loss: 3.2222 - val_accuracy: 0.6898 - 11s/epoch - 53ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0062 - accuracy: 0.9984 - val_loss: 3.4814 - val_accuracy: 0.6898 - 11s/epoch - 53ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0062 - accuracy: 0.9980 - val_loss: 3.9804 - val_accuracy: 0.6877 - 11s/epoch - 52ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0052 - accuracy: 0.9990 - val_loss: 3.2073 - val_accuracy: 0.6967 - 11s/epoch - 53ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0094 - accuracy: 0.9989 - val_loss: 3.4685 - val_accuracy: 0.6902 - 11s/epoch - 53ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0188 - accuracy: 0.9946 - val_loss: 3.0321 - val_accuracy: 0.6981 - 11s/epoch - 53ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0073 - accuracy: 0.9984 - val_loss: 3.1405 - val_accuracy: 0.6942 - 11s/epoch - 52ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0059 - accuracy: 0.9985 - val_loss: 3.4968 - val_accuracy: 0.6940 - 11s/epoch - 53ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0064 - accuracy: 0.9982 - val_loss: 3.5297 - val_accuracy: 0.6933 - 11s/epoch - 53ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0159 - accuracy: 0.9952 - val_loss: 3.2568 - val_accuracy: 0.6888 - 11s/epoch - 52ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0089 - accuracy: 0.9983 - val_loss: 3.2200 - val_accuracy: 0.6925 - 11s/epoch - 53ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0087 - accuracy: 0.9977 - val_loss: 3.7045 - val_accuracy: 0.6875 - 11s/epoch - 53ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0088 - accuracy: 0.9983 - val_loss: 3.8782 - val_accuracy: 0.6877 - 11s/epoch - 52ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0052 - accuracy: 0.9986 - val_loss: 3.3873 - val_accuracy: 0.6908 - 11s/epoch - 52ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0065 - accuracy: 0.9983 - val_loss: 3.7746 - val_accuracy: 0.6885 - 11s/epoch - 53ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0138 - accuracy: 0.9961 - val_loss: 2.9349 - val_accuracy: 0.6904 - 11s/epoch - 53ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0061 - accuracy: 0.9986 - val_loss: 3.1536 - val_accuracy: 0.6950 - 11s/epoch - 53ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0061 - accuracy: 0.9983 - val_loss: 3.6887 - val_accuracy: 0.6883 - 11s/epoch - 55ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 3.8568 - val_accuracy: 0.6890 - 11s/epoch - 53ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0051 - accuracy: 0.9988 - val_loss: 3.5793 - val_accuracy: 0.6890 - 11s/epoch - 52ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0054 - accuracy: 0.9986 - val_loss: 3.3722 - val_accuracy: 0.6900 - 11s/epoch - 52ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0096 - accuracy: 0.9975 - val_loss: 3.3260 - val_accuracy: 0.6904 - 11s/epoch - 53ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0079 - accuracy: 0.9989 - val_loss: 3.4039 - val_accuracy: 0.6890 - 11s/epoch - 53ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0039 - accuracy: 0.9994 - val_loss: 3.6231 - val_accuracy: 0.6894 - 11s/epoch - 53ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0041 - accuracy: 0.9989 - val_loss: 3.5449 - val_accuracy: 0.6952 - 11s/epoch - 53ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 3.8005 - val_accuracy: 0.6958 - 11s/epoch - 53ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0132 - accuracy: 0.9971 - val_loss: 3.1782 - val_accuracy: 0.6927 - 11s/epoch - 52ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0056 - accuracy: 0.9987 - val_loss: 3.7774 - val_accuracy: 0.6900 - 10s/epoch - 51ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0044 - accuracy: 0.9989 - val_loss: 3.4394 - val_accuracy: 0.6898 - 11s/epoch - 52ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0048 - accuracy: 0.9990 - val_loss: 3.7312 - val_accuracy: 0.6898 - 11s/epoch - 51ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0058 - accuracy: 0.9989 - val_loss: 3.6988 - val_accuracy: 0.6894 - 10s/epoch - 48ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 3.6227 - val_accuracy: 0.6892 - 10s/epoch - 48ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0186 - accuracy: 0.9957 - val_loss: 3.3240 - val_accuracy: 0.6927 - 10s/epoch - 48ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0070 - accuracy: 0.9982 - val_loss: 3.1636 - val_accuracy: 0.6963 - 10s/epoch - 49ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0057 - accuracy: 0.9986 - val_loss: 3.5542 - val_accuracy: 0.6965 - 10s/epoch - 48ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0056 - accuracy: 0.9987 - val_loss: 3.6120 - val_accuracy: 0.6956 - 10s/epoch - 50ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0198 - accuracy: 0.9955 - val_loss: 3.1097 - val_accuracy: 0.6873 - 10s/epoch - 48ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0076 - accuracy: 0.9979 - val_loss: 3.7110 - val_accuracy: 0.6892 - 10s/epoch - 49ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0051 - accuracy: 0.9989 - val_loss: 3.5362 - val_accuracy: 0.6921 - 10s/epoch - 50ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0038 - accuracy: 0.9992 - val_loss: 3.3107 - val_accuracy: 0.6942 - 10s/epoch - 47ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0050 - accuracy: 0.9986 - val_loss: 3.4419 - val_accuracy: 0.6902 - 10s/epoch - 46ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 3.8003 - val_accuracy: 0.6913 - 10s/epoch - 47ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 3.8774 - val_accuracy: 0.6917 - 10s/epoch - 47ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0130 - accuracy: 0.9970 - val_loss: 3.3954 - val_accuracy: 0.6933 - 10s/epoch - 48ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 3.2208 - val_accuracy: 0.6888 - 10s/epoch - 49ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0081 - accuracy: 0.9975 - val_loss: 3.3489 - val_accuracy: 0.6933 - 11s/epoch - 52ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0050 - accuracy: 0.9986 - val_loss: 3.2070 - val_accuracy: 0.6935 - 11s/epoch - 53ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0052 - accuracy: 0.9986 - val_loss: 3.4601 - val_accuracy: 0.6944 - 11s/epoch - 52ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0051 - accuracy: 0.9986 - val_loss: 3.6121 - val_accuracy: 0.6910 - 11s/epoch - 53ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0106 - accuracy: 0.9970 - val_loss: 3.1395 - val_accuracy: 0.6885 - 11s/epoch - 53ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0064 - accuracy: 0.9985 - val_loss: 3.6474 - val_accuracy: 0.6908 - 11s/epoch - 52ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0089 - accuracy: 0.9973 - val_loss: 3.4122 - val_accuracy: 0.6956 - 11s/epoch - 52ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0033 - accuracy: 0.9993 - val_loss: 3.7450 - val_accuracy: 0.6931 - 11s/epoch - 53ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0037 - accuracy: 0.9991 - val_loss: 3.4086 - val_accuracy: 0.6950 - 11s/epoch - 52ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0183 - accuracy: 0.9956 - val_loss: 2.9535 - val_accuracy: 0.6965 - 11s/epoch - 52ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0050 - accuracy: 0.9989 - val_loss: 3.4455 - val_accuracy: 0.6919 - 11s/epoch - 52ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 3.7712 - val_accuracy: 0.6894 - 11s/epoch - 53ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0075 - accuracy: 0.9981 - val_loss: 2.8798 - val_accuracy: 0.6942 - 11s/epoch - 52ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0075 - accuracy: 0.9982 - val_loss: 3.0704 - val_accuracy: 0.6935 - 11s/epoch - 52ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0062 - accuracy: 0.9983 - val_loss: 3.8346 - val_accuracy: 0.6902 - 11s/epoch - 53ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0108 - accuracy: 0.9973 - val_loss: 3.1872 - val_accuracy: 0.6940 - 11s/epoch - 52ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0041 - accuracy: 0.9995 - val_loss: 3.2863 - val_accuracy: 0.6944 - 11s/epoch - 52ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0043 - accuracy: 0.9986 - val_loss: 3.7273 - val_accuracy: 0.6935 - 11s/epoch - 52ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0167 - accuracy: 0.9965 - val_loss: 2.9772 - val_accuracy: 0.6950 - 11s/epoch - 53ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0055 - accuracy: 0.9985 - val_loss: 3.2076 - val_accuracy: 0.6969 - 11s/epoch - 52ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0111 - accuracy: 0.9979 - val_loss: 3.5014 - val_accuracy: 0.6944 - 11s/epoch - 54ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 3.6157 - val_accuracy: 0.6975 - 11s/epoch - 53ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0097 - accuracy: 0.9980 - val_loss: 3.1402 - val_accuracy: 0.6919 - 11s/epoch - 53ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0054 - accuracy: 0.9983 - val_loss: 3.2860 - val_accuracy: 0.6917 - 11s/epoch - 52ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0113 - accuracy: 0.9974 - val_loss: 3.1611 - val_accuracy: 0.6913 - 11s/epoch - 53ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0041 - accuracy: 0.9990 - val_loss: 3.0621 - val_accuracy: 0.6935 - 11s/epoch - 53ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0133 - accuracy: 0.9964 - val_loss: 3.6145 - val_accuracy: 0.6823 - 11s/epoch - 51ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 3.6474 - val_accuracy: 0.6896 - 10s/epoch - 50ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0029 - accuracy: 0.9993 - val_loss: 3.4734 - val_accuracy: 0.6898 - 11s/epoch - 51ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0051 - accuracy: 0.9986 - val_loss: 3.5450 - val_accuracy: 0.6925 - 11s/epoch - 53ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0071 - accuracy: 0.9980 - val_loss: 3.0844 - val_accuracy: 0.6929 - 11s/epoch - 52ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0058 - accuracy: 0.9990 - val_loss: 3.6724 - val_accuracy: 0.6894 - 11s/epoch - 52ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0132 - accuracy: 0.9970 - val_loss: 3.1941 - val_accuracy: 0.6898 - 11s/epoch - 52ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0045 - accuracy: 0.9991 - val_loss: 3.3153 - val_accuracy: 0.6940 - 10s/epoch - 50ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0026 - accuracy: 0.9994 - val_loss: 3.5305 - val_accuracy: 0.6931 - 11s/epoch - 52ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0037 - accuracy: 0.9991 - val_loss: 3.6881 - val_accuracy: 0.6927 - 11s/epoch - 52ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0025 - accuracy: 0.9992 - val_loss: 3.8722 - val_accuracy: 0.6921 - 11s/epoch - 53ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0127 - accuracy: 0.9986 - val_loss: 2.7936 - val_accuracy: 0.6956 - 11s/epoch - 52ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0054 - accuracy: 0.9985 - val_loss: 3.3283 - val_accuracy: 0.6921 - 11s/epoch - 52ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0039 - accuracy: 0.9993 - val_loss: 3.7889 - val_accuracy: 0.6900 - 11s/epoch - 52ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0085 - accuracy: 0.9983 - val_loss: 3.4557 - val_accuracy: 0.6929 - 11s/epoch - 53ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0023 - accuracy: 0.9998 - val_loss: 3.9425 - val_accuracy: 0.6908 - 11s/epoch - 52ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0030 - accuracy: 0.9993 - val_loss: 3.6565 - val_accuracy: 0.6931 - 11s/epoch - 52ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0101 - accuracy: 0.9989 - val_loss: 3.4786 - val_accuracy: 0.6900 - 11s/epoch - 53ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0070 - accuracy: 0.9987 - val_loss: 3.6137 - val_accuracy: 0.6883 - 11s/epoch - 52ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0076 - accuracy: 0.9977 - val_loss: 3.0908 - val_accuracy: 0.6890 - 12s/epoch - 60ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.65654\n",
      "207/207 - 12s - loss: 0.0036 - accuracy: 0.9991 - val_loss: 3.3715 - val_accuracy: 0.6910 - 12s/epoch - 60ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.65654\n",
      "207/207 - 13s - loss: 0.0030 - accuracy: 0.9993 - val_loss: 3.4713 - val_accuracy: 0.6879 - 13s/epoch - 63ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.65654\n",
      "207/207 - 15s - loss: 0.0035 - accuracy: 0.9991 - val_loss: 3.3808 - val_accuracy: 0.6900 - 15s/epoch - 72ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.65654\n",
      "207/207 - 14s - loss: 0.0026 - accuracy: 0.9996 - val_loss: 3.7047 - val_accuracy: 0.6894 - 14s/epoch - 68ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.65654\n",
      "207/207 - 15s - loss: 0.0143 - accuracy: 0.9970 - val_loss: 3.6341 - val_accuracy: 0.6879 - 15s/epoch - 71ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0056 - accuracy: 0.9986 - val_loss: 3.1237 - val_accuracy: 0.6921 - 11s/epoch - 55ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0052 - accuracy: 0.9983 - val_loss: 3.4526 - val_accuracy: 0.6894 - 11s/epoch - 52ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0035 - accuracy: 0.9990 - val_loss: 3.3547 - val_accuracy: 0.6915 - 11s/epoch - 52ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0055 - accuracy: 0.9986 - val_loss: 3.8377 - val_accuracy: 0.6890 - 11s/epoch - 53ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0041 - accuracy: 0.9990 - val_loss: 3.5720 - val_accuracy: 0.6910 - 11s/epoch - 53ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 3.4663 - val_accuracy: 0.6902 - 11s/epoch - 52ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0069 - accuracy: 0.9980 - val_loss: 4.2861 - val_accuracy: 0.6862 - 11s/epoch - 52ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0100 - accuracy: 0.9973 - val_loss: 2.9437 - val_accuracy: 0.6867 - 11s/epoch - 53ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0059 - accuracy: 0.9982 - val_loss: 3.3391 - val_accuracy: 0.6879 - 11s/epoch - 52ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0030 - accuracy: 0.9995 - val_loss: 3.6045 - val_accuracy: 0.6908 - 11s/epoch - 52ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0016 - accuracy: 0.9998 - val_loss: 3.9476 - val_accuracy: 0.6871 - 11s/epoch - 52ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0023 - accuracy: 0.9997 - val_loss: 3.9522 - val_accuracy: 0.6885 - 11s/epoch - 52ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0073 - accuracy: 0.9984 - val_loss: 3.8869 - val_accuracy: 0.6892 - 11s/epoch - 52ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0031 - accuracy: 0.9995 - val_loss: 4.0568 - val_accuracy: 0.6892 - 10s/epoch - 50ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0069 - accuracy: 0.9981 - val_loss: 3.7059 - val_accuracy: 0.6879 - 11s/epoch - 52ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0087 - accuracy: 0.9976 - val_loss: 3.9385 - val_accuracy: 0.6846 - 11s/epoch - 52ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0098 - accuracy: 0.9974 - val_loss: 3.7226 - val_accuracy: 0.6846 - 11s/epoch - 52ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 3.9689 - val_accuracy: 0.6844 - 11s/epoch - 52ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0044 - accuracy: 0.9986 - val_loss: 3.7030 - val_accuracy: 0.6854 - 11s/epoch - 52ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0056 - accuracy: 0.9991 - val_loss: 3.5651 - val_accuracy: 0.6796 - 10s/epoch - 50ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0064 - accuracy: 0.9983 - val_loss: 3.6183 - val_accuracy: 0.6860 - 11s/epoch - 52ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0025 - accuracy: 0.9995 - val_loss: 3.7606 - val_accuracy: 0.6873 - 10s/epoch - 50ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0127 - accuracy: 0.9974 - val_loss: 3.2295 - val_accuracy: 0.6804 - 10s/epoch - 48ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0047 - accuracy: 0.9988 - val_loss: 3.7642 - val_accuracy: 0.6844 - 10s/epoch - 48ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0037 - accuracy: 0.9989 - val_loss: 3.8185 - val_accuracy: 0.6877 - 10s/epoch - 49ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0037 - accuracy: 0.9990 - val_loss: 3.7896 - val_accuracy: 0.6879 - 11s/epoch - 52ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0098 - accuracy: 0.9979 - val_loss: 3.6322 - val_accuracy: 0.6873 - 11s/epoch - 53ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0034 - accuracy: 0.9994 - val_loss: 3.4383 - val_accuracy: 0.6913 - 11s/epoch - 52ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.65654\n",
      "207/207 - 10s - loss: 0.0030 - accuracy: 0.9990 - val_loss: 3.5295 - val_accuracy: 0.6944 - 10s/epoch - 51ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0020 - accuracy: 0.9997 - val_loss: 3.9785 - val_accuracy: 0.6898 - 11s/epoch - 51ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0186 - accuracy: 0.9955 - val_loss: 3.1004 - val_accuracy: 0.6885 - 11s/epoch - 52ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 3.2525 - val_accuracy: 0.6906 - 11s/epoch - 52ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0026 - accuracy: 0.9995 - val_loss: 3.3578 - val_accuracy: 0.6906 - 11s/epoch - 52ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0036 - accuracy: 0.9989 - val_loss: 3.6062 - val_accuracy: 0.6904 - 11s/epoch - 52ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0027 - accuracy: 0.9995 - val_loss: 3.4098 - val_accuracy: 0.6938 - 11s/epoch - 52ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0057 - accuracy: 0.9985 - val_loss: 3.0479 - val_accuracy: 0.6898 - 11s/epoch - 52ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0047 - accuracy: 0.9986 - val_loss: 3.9385 - val_accuracy: 0.6852 - 11s/epoch - 52ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0060 - accuracy: 0.9986 - val_loss: 3.7723 - val_accuracy: 0.6890 - 11s/epoch - 53ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0064 - accuracy: 0.9980 - val_loss: 3.6760 - val_accuracy: 0.6852 - 11s/epoch - 53ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0046 - accuracy: 0.9988 - val_loss: 3.8354 - val_accuracy: 0.6873 - 11s/epoch - 52ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0024 - accuracy: 0.9998 - val_loss: 3.9647 - val_accuracy: 0.6865 - 11s/epoch - 52ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0028 - accuracy: 0.9991 - val_loss: 4.0107 - val_accuracy: 0.6888 - 11s/epoch - 52ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0105 - accuracy: 0.9968 - val_loss: 3.5210 - val_accuracy: 0.6944 - 11s/epoch - 53ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0044 - accuracy: 0.9989 - val_loss: 3.5148 - val_accuracy: 0.6977 - 11s/epoch - 53ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0021 - accuracy: 0.9996 - val_loss: 3.6199 - val_accuracy: 0.6935 - 11s/epoch - 52ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0040 - accuracy: 0.9989 - val_loss: 3.6943 - val_accuracy: 0.6904 - 11s/epoch - 52ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 3.6297 - val_accuracy: 0.6881 - 11s/epoch - 53ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0033 - accuracy: 0.9991 - val_loss: 3.7434 - val_accuracy: 0.6883 - 11s/epoch - 52ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0056 - accuracy: 0.9981 - val_loss: 3.5803 - val_accuracy: 0.6856 - 11s/epoch - 52ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.65654\n",
      "207/207 - 11s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 3.8920 - val_accuracy: 0.6856 - 11s/epoch - 53ms/step\n",
      "179/179 [==============================] - 1s 7ms/step\n",
      "Classification accuracy: 0.555208 \n"
     ]
    }
   ],
   "source": [
    "probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[2476  824]\n",
      " [2367   33]]\n",
      "[44.02 51.13  3.85]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for EEGNet on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)\n",
    "\n",
    "plot_confusion_matrix(preds_EEGNet, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for EEGNet on ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.99589, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 68s - loss: 0.9240 - accuracy: 0.7464 - val_loss: 0.9959 - val_accuracy: 0.6892 - 68s/epoch - 165ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.99589\n",
      "413/413 - 66s - loss: 0.5167 - accuracy: 0.9061 - val_loss: 1.2900 - val_accuracy: 0.6848 - 66s/epoch - 160ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.99589\n",
      "413/413 - 65s - loss: 0.3682 - accuracy: 0.9383 - val_loss: 1.5275 - val_accuracy: 0.6625 - 65s/epoch - 157ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 0.99589 to 0.99567, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 64s - loss: 0.3479 - accuracy: 0.9420 - val_loss: 0.9957 - val_accuracy: 0.6627 - 64s/epoch - 156ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.2862 - accuracy: 0.9564 - val_loss: 1.5871 - val_accuracy: 0.6515 - 65s/epoch - 158ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.2637 - accuracy: 0.9611 - val_loss: 2.1896 - val_accuracy: 0.6450 - 65s/epoch - 158ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.2769 - accuracy: 0.9579 - val_loss: 1.3895 - val_accuracy: 0.6165 - 66s/epoch - 159ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.2467 - accuracy: 0.9639 - val_loss: 1.2378 - val_accuracy: 0.6565 - 66s/epoch - 159ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.2372 - accuracy: 0.9657 - val_loss: 1.1657 - val_accuracy: 0.6619 - 66s/epoch - 159ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.2381 - accuracy: 0.9654 - val_loss: 1.6285 - val_accuracy: 0.6481 - 66s/epoch - 160ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.2175 - accuracy: 0.9695 - val_loss: 1.1985 - val_accuracy: 0.6729 - 66s/epoch - 161ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.2048 - accuracy: 0.9710 - val_loss: 1.2415 - val_accuracy: 0.6938 - 65s/epoch - 158ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1876 - accuracy: 0.9739 - val_loss: 1.6300 - val_accuracy: 0.7192 - 65s/epoch - 157ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1616 - accuracy: 0.9788 - val_loss: 1.7427 - val_accuracy: 0.6642 - 65s/epoch - 158ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1698 - accuracy: 0.9747 - val_loss: 1.9997 - val_accuracy: 0.7258 - 65s/epoch - 158ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1435 - accuracy: 0.9798 - val_loss: 2.3517 - val_accuracy: 0.6960 - 65s/epoch - 158ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1291 - accuracy: 0.9821 - val_loss: 2.2359 - val_accuracy: 0.7279 - 65s/epoch - 157ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1426 - accuracy: 0.9780 - val_loss: 1.5969 - val_accuracy: 0.7490 - 65s/epoch - 157ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.1388 - accuracy: 0.9785 - val_loss: 1.6317 - val_accuracy: 0.7040 - 67s/epoch - 163ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1209 - accuracy: 0.9836 - val_loss: 1.9458 - val_accuracy: 0.6742 - 65s/epoch - 158ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1413 - accuracy: 0.9769 - val_loss: 1.4711 - val_accuracy: 0.6869 - 65s/epoch - 158ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1261 - accuracy: 0.9823 - val_loss: 1.2264 - val_accuracy: 0.7002 - 65s/epoch - 158ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1104 - accuracy: 0.9852 - val_loss: 1.9800 - val_accuracy: 0.7352 - 65s/epoch - 158ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1188 - accuracy: 0.9807 - val_loss: 2.7215 - val_accuracy: 0.6865 - 65s/epoch - 158ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1144 - accuracy: 0.9834 - val_loss: 1.1833 - val_accuracy: 0.7652 - 65s/epoch - 158ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1036 - accuracy: 0.9858 - val_loss: 1.6347 - val_accuracy: 0.7502 - 65s/epoch - 158ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.1005 - accuracy: 0.9865 - val_loss: 1.5766 - val_accuracy: 0.7358 - 65s/epoch - 158ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0974 - accuracy: 0.9857 - val_loss: 1.7303 - val_accuracy: 0.7437 - 65s/epoch - 158ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0929 - accuracy: 0.9863 - val_loss: 1.3532 - val_accuracy: 0.7602 - 65s/epoch - 158ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.1129 - accuracy: 0.9823 - val_loss: 1.1760 - val_accuracy: 0.6842 - 66s/epoch - 159ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0783 - accuracy: 0.9901 - val_loss: 3.0904 - val_accuracy: 0.6969 - 65s/epoch - 158ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0959 - accuracy: 0.9858 - val_loss: 1.5982 - val_accuracy: 0.7635 - 65s/epoch - 159ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0983 - accuracy: 0.9833 - val_loss: 1.5380 - val_accuracy: 0.7292 - 67s/epoch - 162ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0852 - accuracy: 0.9899 - val_loss: 2.5630 - val_accuracy: 0.7283 - 64s/epoch - 155ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.99567\n",
      "413/413 - 57s - loss: 0.0919 - accuracy: 0.9872 - val_loss: 1.6760 - val_accuracy: 0.7396 - 57s/epoch - 139ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.99567\n",
      "413/413 - 56s - loss: 0.0818 - accuracy: 0.9882 - val_loss: 1.1519 - val_accuracy: 0.6623 - 56s/epoch - 135ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.99567\n",
      "413/413 - 57s - loss: 0.0797 - accuracy: 0.9894 - val_loss: 1.5245 - val_accuracy: 0.7592 - 57s/epoch - 138ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.99567\n",
      "413/413 - 59s - loss: 0.0841 - accuracy: 0.9886 - val_loss: 2.3650 - val_accuracy: 0.6944 - 59s/epoch - 142ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.1046 - accuracy: 0.9835 - val_loss: 1.6892 - val_accuracy: 0.7098 - 64s/epoch - 156ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0704 - accuracy: 0.9919 - val_loss: 2.7879 - val_accuracy: 0.7008 - 64s/epoch - 155ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0806 - accuracy: 0.9893 - val_loss: 1.2177 - val_accuracy: 0.7033 - 61s/epoch - 147ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.99567\n",
      "413/413 - 56s - loss: 0.0893 - accuracy: 0.9865 - val_loss: 1.4968 - val_accuracy: 0.7448 - 56s/epoch - 137ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.99567\n",
      "413/413 - 58s - loss: 0.0718 - accuracy: 0.9914 - val_loss: 2.0830 - val_accuracy: 0.6569 - 58s/epoch - 141ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0769 - accuracy: 0.9902 - val_loss: 1.6914 - val_accuracy: 0.7440 - 65s/epoch - 159ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0744 - accuracy: 0.9896 - val_loss: 1.5469 - val_accuracy: 0.7619 - 65s/epoch - 157ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0703 - accuracy: 0.9914 - val_loss: 1.6295 - val_accuracy: 0.7285 - 68s/epoch - 164ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0739 - accuracy: 0.9895 - val_loss: 2.3146 - val_accuracy: 0.7013 - 64s/epoch - 155ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.99567\n",
      "413/413 - 58s - loss: 0.0717 - accuracy: 0.9906 - val_loss: 1.8122 - val_accuracy: 0.7369 - 58s/epoch - 140ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0758 - accuracy: 0.9894 - val_loss: 1.9507 - val_accuracy: 0.7419 - 61s/epoch - 147ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0669 - accuracy: 0.9916 - val_loss: 1.1802 - val_accuracy: 0.7246 - 66s/epoch - 161ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0646 - accuracy: 0.9928 - val_loss: 2.4259 - val_accuracy: 0.6990 - 66s/epoch - 160ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0673 - accuracy: 0.9916 - val_loss: 1.9911 - val_accuracy: 0.7281 - 68s/epoch - 164ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0739 - accuracy: 0.9891 - val_loss: 2.0697 - val_accuracy: 0.7258 - 66s/epoch - 160ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0793 - accuracy: 0.9884 - val_loss: 1.7867 - val_accuracy: 0.7150 - 66s/epoch - 160ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0582 - accuracy: 0.9950 - val_loss: 1.8501 - val_accuracy: 0.7360 - 66s/epoch - 160ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0726 - accuracy: 0.9904 - val_loss: 1.4458 - val_accuracy: 0.7454 - 66s/epoch - 159ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0624 - accuracy: 0.9931 - val_loss: 2.0070 - val_accuracy: 0.6948 - 67s/epoch - 163ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0689 - accuracy: 0.9908 - val_loss: 2.0074 - val_accuracy: 0.7575 - 67s/epoch - 161ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0619 - accuracy: 0.9920 - val_loss: 1.4367 - val_accuracy: 0.6940 - 66s/epoch - 161ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0704 - accuracy: 0.9889 - val_loss: 1.5955 - val_accuracy: 0.7579 - 66s/epoch - 160ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0638 - accuracy: 0.9925 - val_loss: 1.6537 - val_accuracy: 0.7467 - 66s/epoch - 161ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0602 - accuracy: 0.9931 - val_loss: 2.1320 - val_accuracy: 0.6942 - 67s/epoch - 161ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0707 - accuracy: 0.9902 - val_loss: 1.3167 - val_accuracy: 0.7502 - 67s/epoch - 162ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0586 - accuracy: 0.9940 - val_loss: 1.9604 - val_accuracy: 0.7312 - 66s/epoch - 160ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0616 - accuracy: 0.9934 - val_loss: 2.6576 - val_accuracy: 0.6960 - 68s/epoch - 164ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0688 - accuracy: 0.9907 - val_loss: 1.9487 - val_accuracy: 0.6829 - 66s/epoch - 161ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0634 - accuracy: 0.9923 - val_loss: 1.7534 - val_accuracy: 0.7040 - 66s/epoch - 160ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0663 - accuracy: 0.9908 - val_loss: 1.4781 - val_accuracy: 0.7329 - 66s/epoch - 160ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0717 - accuracy: 0.9903 - val_loss: 1.8258 - val_accuracy: 0.6952 - 65s/epoch - 158ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0683 - accuracy: 0.9917 - val_loss: 1.9998 - val_accuracy: 0.7258 - 68s/epoch - 164ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0483 - accuracy: 0.9964 - val_loss: 2.6283 - val_accuracy: 0.6979 - 63s/epoch - 152ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.99567\n",
      "413/413 - 58s - loss: 0.0691 - accuracy: 0.9904 - val_loss: 2.4225 - val_accuracy: 0.6985 - 58s/epoch - 140ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.99567\n",
      "413/413 - 59s - loss: 0.0548 - accuracy: 0.9932 - val_loss: 2.0989 - val_accuracy: 0.7279 - 59s/epoch - 142ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0585 - accuracy: 0.9932 - val_loss: 1.4796 - val_accuracy: 0.7260 - 62s/epoch - 151ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.99567\n",
      "413/413 - 69s - loss: 0.0609 - accuracy: 0.9929 - val_loss: 2.6250 - val_accuracy: 0.7125 - 69s/epoch - 166ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0502 - accuracy: 0.9948 - val_loss: 2.1617 - val_accuracy: 0.7002 - 66s/epoch - 160ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0580 - accuracy: 0.9930 - val_loss: 2.1206 - val_accuracy: 0.7244 - 66s/epoch - 160ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0579 - accuracy: 0.9926 - val_loss: 2.0635 - val_accuracy: 0.7202 - 66s/epoch - 159ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0501 - accuracy: 0.9939 - val_loss: 1.6532 - val_accuracy: 0.6996 - 65s/epoch - 158ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0634 - accuracy: 0.9927 - val_loss: 2.0128 - val_accuracy: 0.7008 - 65s/epoch - 158ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.99567\n",
      "413/413 - 59s - loss: 0.0594 - accuracy: 0.9923 - val_loss: 2.5197 - val_accuracy: 0.6933 - 59s/epoch - 142ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0541 - accuracy: 0.9943 - val_loss: 1.7769 - val_accuracy: 0.6729 - 62s/epoch - 149ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0543 - accuracy: 0.9930 - val_loss: 2.2509 - val_accuracy: 0.7319 - 66s/epoch - 160ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0625 - accuracy: 0.9915 - val_loss: 1.8853 - val_accuracy: 0.6892 - 68s/epoch - 164ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0659 - accuracy: 0.9925 - val_loss: 2.1646 - val_accuracy: 0.7096 - 64s/epoch - 156ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0535 - accuracy: 0.9932 - val_loss: 1.7962 - val_accuracy: 0.7321 - 60s/epoch - 144ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0503 - accuracy: 0.9946 - val_loss: 1.9150 - val_accuracy: 0.7531 - 66s/epoch - 161ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0526 - accuracy: 0.9936 - val_loss: 1.8762 - val_accuracy: 0.7215 - 67s/epoch - 161ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0558 - accuracy: 0.9934 - val_loss: 1.3601 - val_accuracy: 0.6954 - 68s/epoch - 164ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0590 - accuracy: 0.9934 - val_loss: 2.5193 - val_accuracy: 0.6733 - 65s/epoch - 158ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0726 - accuracy: 0.9890 - val_loss: 2.4440 - val_accuracy: 0.7196 - 65s/epoch - 158ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0517 - accuracy: 0.9945 - val_loss: 2.4003 - val_accuracy: 0.7496 - 63s/epoch - 153ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0409 - accuracy: 0.9960 - val_loss: 2.0737 - val_accuracy: 0.7631 - 67s/epoch - 162ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0544 - accuracy: 0.9937 - val_loss: 2.1158 - val_accuracy: 0.7327 - 66s/epoch - 160ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0550 - accuracy: 0.9933 - val_loss: 1.8225 - val_accuracy: 0.6929 - 65s/epoch - 158ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0500 - accuracy: 0.9953 - val_loss: 1.5870 - val_accuracy: 0.7583 - 64s/epoch - 155ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0530 - accuracy: 0.9942 - val_loss: 1.3246 - val_accuracy: 0.7538 - 63s/epoch - 153ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0569 - accuracy: 0.9942 - val_loss: 1.6162 - val_accuracy: 0.6913 - 66s/epoch - 159ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0545 - accuracy: 0.9939 - val_loss: 2.1598 - val_accuracy: 0.7040 - 65s/epoch - 158ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0570 - accuracy: 0.9926 - val_loss: 2.1502 - val_accuracy: 0.7273 - 64s/epoch - 156ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0590 - accuracy: 0.9928 - val_loss: 2.5882 - val_accuracy: 0.6973 - 65s/epoch - 157ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0583 - accuracy: 0.9931 - val_loss: 2.9662 - val_accuracy: 0.6994 - 64s/epoch - 156ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0483 - accuracy: 0.9952 - val_loss: 1.9211 - val_accuracy: 0.6931 - 65s/epoch - 157ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0455 - accuracy: 0.9951 - val_loss: 2.6456 - val_accuracy: 0.6929 - 65s/epoch - 158ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0549 - accuracy: 0.9940 - val_loss: 1.2996 - val_accuracy: 0.7081 - 61s/epoch - 147ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.99567\n",
      "413/413 - 59s - loss: 0.0592 - accuracy: 0.9929 - val_loss: 2.1184 - val_accuracy: 0.7079 - 59s/epoch - 143ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0538 - accuracy: 0.9933 - val_loss: 1.6524 - val_accuracy: 0.7231 - 66s/epoch - 160ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0515 - accuracy: 0.9939 - val_loss: 2.0622 - val_accuracy: 0.6829 - 67s/epoch - 163ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0405 - accuracy: 0.9961 - val_loss: 5.4126 - val_accuracy: 0.6896 - 65s/epoch - 158ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0640 - accuracy: 0.9914 - val_loss: 1.8025 - val_accuracy: 0.7467 - 66s/epoch - 160ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0529 - accuracy: 0.9952 - val_loss: 1.3118 - val_accuracy: 0.6890 - 66s/epoch - 159ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0451 - accuracy: 0.9955 - val_loss: 2.3106 - val_accuracy: 0.6938 - 65s/epoch - 157ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.99567\n",
      "413/413 - 69s - loss: 0.0596 - accuracy: 0.9927 - val_loss: 2.2504 - val_accuracy: 0.6948 - 69s/epoch - 167ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0562 - accuracy: 0.9944 - val_loss: 1.4560 - val_accuracy: 0.7517 - 68s/epoch - 165ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0479 - accuracy: 0.9942 - val_loss: 2.1437 - val_accuracy: 0.7177 - 66s/epoch - 159ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0560 - accuracy: 0.9948 - val_loss: 2.0511 - val_accuracy: 0.7013 - 65s/epoch - 158ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0513 - accuracy: 0.9936 - val_loss: 2.9821 - val_accuracy: 0.6796 - 66s/epoch - 159ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0462 - accuracy: 0.9953 - val_loss: 2.8582 - val_accuracy: 0.6363 - 65s/epoch - 157ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0526 - accuracy: 0.9930 - val_loss: 1.4328 - val_accuracy: 0.7344 - 65s/epoch - 158ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0489 - accuracy: 0.9949 - val_loss: 2.3507 - val_accuracy: 0.7008 - 63s/epoch - 154ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0490 - accuracy: 0.9948 - val_loss: 2.5908 - val_accuracy: 0.6871 - 65s/epoch - 158ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0475 - accuracy: 0.9943 - val_loss: 1.6880 - val_accuracy: 0.7096 - 66s/epoch - 159ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0570 - accuracy: 0.9935 - val_loss: 1.8121 - val_accuracy: 0.7077 - 65s/epoch - 157ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0427 - accuracy: 0.9961 - val_loss: 1.7388 - val_accuracy: 0.7458 - 66s/epoch - 159ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0514 - accuracy: 0.9945 - val_loss: 1.6250 - val_accuracy: 0.7402 - 64s/epoch - 156ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0526 - accuracy: 0.9942 - val_loss: 2.2853 - val_accuracy: 0.7067 - 64s/epoch - 155ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0385 - accuracy: 0.9962 - val_loss: 1.8930 - val_accuracy: 0.7352 - 60s/epoch - 144ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0531 - accuracy: 0.9931 - val_loss: 2.1497 - val_accuracy: 0.6996 - 67s/epoch - 163ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.99567\n",
      "413/413 - 70s - loss: 0.0467 - accuracy: 0.9945 - val_loss: 1.4087 - val_accuracy: 0.7519 - 70s/epoch - 170ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.99567\n",
      "413/413 - 73s - loss: 0.0561 - accuracy: 0.9949 - val_loss: 2.3164 - val_accuracy: 0.6988 - 73s/epoch - 176ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.99567\n",
      "413/413 - 71s - loss: 0.0477 - accuracy: 0.9945 - val_loss: 2.1090 - val_accuracy: 0.7190 - 71s/epoch - 172ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0584 - accuracy: 0.9926 - val_loss: 2.8755 - val_accuracy: 0.6933 - 66s/epoch - 161ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0462 - accuracy: 0.9957 - val_loss: 2.0371 - val_accuracy: 0.7133 - 66s/epoch - 161ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0487 - accuracy: 0.9942 - val_loss: 1.6401 - val_accuracy: 0.7277 - 66s/epoch - 159ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0528 - accuracy: 0.9956 - val_loss: 2.3396 - val_accuracy: 0.7015 - 64s/epoch - 154ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0364 - accuracy: 0.9973 - val_loss: 2.5650 - val_accuracy: 0.6898 - 67s/epoch - 162ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0560 - accuracy: 0.9930 - val_loss: 2.7437 - val_accuracy: 0.6435 - 63s/epoch - 153ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0527 - accuracy: 0.9929 - val_loss: 1.8916 - val_accuracy: 0.6975 - 63s/epoch - 153ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0534 - accuracy: 0.9947 - val_loss: 2.2686 - val_accuracy: 0.6894 - 66s/epoch - 161ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0455 - accuracy: 0.9954 - val_loss: 2.4416 - val_accuracy: 0.6996 - 66s/epoch - 161ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0484 - accuracy: 0.9945 - val_loss: 2.4013 - val_accuracy: 0.6977 - 67s/epoch - 162ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0435 - accuracy: 0.9953 - val_loss: 2.4741 - val_accuracy: 0.6992 - 66s/epoch - 161ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0617 - accuracy: 0.9909 - val_loss: 1.9678 - val_accuracy: 0.6773 - 66s/epoch - 159ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0439 - accuracy: 0.9961 - val_loss: 3.2084 - val_accuracy: 0.6938 - 66s/epoch - 160ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0414 - accuracy: 0.9960 - val_loss: 2.8088 - val_accuracy: 0.6758 - 65s/epoch - 158ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0359 - accuracy: 0.9973 - val_loss: 2.0987 - val_accuracy: 0.6927 - 66s/epoch - 160ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0374 - accuracy: 0.9961 - val_loss: 3.0784 - val_accuracy: 0.6888 - 66s/epoch - 159ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0502 - accuracy: 0.9955 - val_loss: 2.6931 - val_accuracy: 0.6375 - 66s/epoch - 160ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0508 - accuracy: 0.9946 - val_loss: 2.2170 - val_accuracy: 0.7040 - 65s/epoch - 158ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0610 - accuracy: 0.9914 - val_loss: 3.4140 - val_accuracy: 0.6919 - 65s/epoch - 158ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0527 - accuracy: 0.9937 - val_loss: 2.0703 - val_accuracy: 0.6888 - 66s/epoch - 160ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0399 - accuracy: 0.9969 - val_loss: 2.3279 - val_accuracy: 0.7298 - 66s/epoch - 160ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0464 - accuracy: 0.9949 - val_loss: 2.9302 - val_accuracy: 0.6892 - 66s/epoch - 160ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0402 - accuracy: 0.9954 - val_loss: 2.3514 - val_accuracy: 0.6735 - 66s/epoch - 159ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0533 - accuracy: 0.9930 - val_loss: 1.7363 - val_accuracy: 0.7763 - 65s/epoch - 158ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0435 - accuracy: 0.9958 - val_loss: 2.8624 - val_accuracy: 0.6438 - 65s/epoch - 159ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0417 - accuracy: 0.9965 - val_loss: 1.9874 - val_accuracy: 0.6933 - 66s/epoch - 160ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0452 - accuracy: 0.9953 - val_loss: 2.0084 - val_accuracy: 0.6765 - 66s/epoch - 159ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0556 - accuracy: 0.9935 - val_loss: 2.7360 - val_accuracy: 0.6323 - 65s/epoch - 159ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0473 - accuracy: 0.9955 - val_loss: 2.9878 - val_accuracy: 0.6562 - 65s/epoch - 157ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0447 - accuracy: 0.9959 - val_loss: 2.6383 - val_accuracy: 0.6517 - 65s/epoch - 157ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0336 - accuracy: 0.9973 - val_loss: 3.2504 - val_accuracy: 0.6896 - 67s/epoch - 162ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0559 - accuracy: 0.9933 - val_loss: 3.0206 - val_accuracy: 0.6910 - 65s/epoch - 158ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0467 - accuracy: 0.9949 - val_loss: 2.1415 - val_accuracy: 0.6510 - 61s/epoch - 148ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0453 - accuracy: 0.9954 - val_loss: 1.6472 - val_accuracy: 0.7321 - 66s/epoch - 159ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0471 - accuracy: 0.9945 - val_loss: 3.2102 - val_accuracy: 0.6427 - 66s/epoch - 159ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0444 - accuracy: 0.9961 - val_loss: 3.3352 - val_accuracy: 0.6940 - 67s/epoch - 163ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.99567\n",
      "413/413 - 70s - loss: 0.0485 - accuracy: 0.9945 - val_loss: 1.5467 - val_accuracy: 0.7552 - 70s/epoch - 169ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.99567\n",
      "413/413 - 77s - loss: 0.0422 - accuracy: 0.9964 - val_loss: 1.9151 - val_accuracy: 0.7319 - 77s/epoch - 187ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0445 - accuracy: 0.9964 - val_loss: 1.9864 - val_accuracy: 0.7417 - 67s/epoch - 162ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0586 - accuracy: 0.9931 - val_loss: 1.6864 - val_accuracy: 0.7698 - 67s/epoch - 162ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0493 - accuracy: 0.9951 - val_loss: 2.3812 - val_accuracy: 0.7558 - 65s/epoch - 159ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0330 - accuracy: 0.9978 - val_loss: 2.7311 - val_accuracy: 0.6996 - 65s/epoch - 158ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0447 - accuracy: 0.9948 - val_loss: 3.6135 - val_accuracy: 0.6994 - 65s/epoch - 158ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0524 - accuracy: 0.9933 - val_loss: 2.4054 - val_accuracy: 0.6815 - 63s/epoch - 151ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0423 - accuracy: 0.9961 - val_loss: 2.1689 - val_accuracy: 0.7227 - 60s/epoch - 145ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0466 - accuracy: 0.9952 - val_loss: 2.7471 - val_accuracy: 0.7046 - 66s/epoch - 160ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0462 - accuracy: 0.9953 - val_loss: 2.5463 - val_accuracy: 0.7569 - 66s/epoch - 160ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0427 - accuracy: 0.9956 - val_loss: 3.2276 - val_accuracy: 0.7015 - 66s/epoch - 159ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0395 - accuracy: 0.9966 - val_loss: 2.4829 - val_accuracy: 0.7058 - 67s/epoch - 162ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0443 - accuracy: 0.9953 - val_loss: 2.4492 - val_accuracy: 0.7150 - 63s/epoch - 153ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0449 - accuracy: 0.9955 - val_loss: 2.2359 - val_accuracy: 0.6942 - 65s/epoch - 156ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0420 - accuracy: 0.9961 - val_loss: 1.6130 - val_accuracy: 0.6981 - 62s/epoch - 149ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0416 - accuracy: 0.9964 - val_loss: 2.2817 - val_accuracy: 0.7106 - 63s/epoch - 152ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0380 - accuracy: 0.9959 - val_loss: 2.2400 - val_accuracy: 0.7710 - 68s/epoch - 165ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0497 - accuracy: 0.9955 - val_loss: 1.1251 - val_accuracy: 0.7552 - 67s/epoch - 163ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0534 - accuracy: 0.9934 - val_loss: 1.8804 - val_accuracy: 0.7485 - 66s/epoch - 159ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.99567\n",
      "413/413 - 74s - loss: 0.0457 - accuracy: 0.9956 - val_loss: 1.4918 - val_accuracy: 0.7563 - 74s/epoch - 178ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0497 - accuracy: 0.9946 - val_loss: 2.0643 - val_accuracy: 0.7096 - 64s/epoch - 156ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0374 - accuracy: 0.9963 - val_loss: 2.5472 - val_accuracy: 0.7462 - 66s/epoch - 159ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0517 - accuracy: 0.9941 - val_loss: 2.7549 - val_accuracy: 0.7300 - 64s/epoch - 156ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0536 - accuracy: 0.9948 - val_loss: 1.3628 - val_accuracy: 0.6521 - 61s/epoch - 147ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0463 - accuracy: 0.9966 - val_loss: 2.0213 - val_accuracy: 0.7306 - 64s/epoch - 155ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0500 - accuracy: 0.9950 - val_loss: 3.2166 - val_accuracy: 0.7492 - 66s/epoch - 160ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0371 - accuracy: 0.9959 - val_loss: 2.7750 - val_accuracy: 0.6988 - 66s/epoch - 160ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0412 - accuracy: 0.9961 - val_loss: 2.0597 - val_accuracy: 0.7090 - 68s/epoch - 165ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0330 - accuracy: 0.9973 - val_loss: 3.1501 - val_accuracy: 0.7304 - 66s/epoch - 159ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0478 - accuracy: 0.9959 - val_loss: 2.0999 - val_accuracy: 0.7163 - 63s/epoch - 153ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0495 - accuracy: 0.9945 - val_loss: 2.4525 - val_accuracy: 0.7277 - 60s/epoch - 145ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0378 - accuracy: 0.9964 - val_loss: 2.4612 - val_accuracy: 0.7546 - 61s/epoch - 148ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0359 - accuracy: 0.9970 - val_loss: 2.5130 - val_accuracy: 0.6994 - 67s/epoch - 162ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0530 - accuracy: 0.9934 - val_loss: 1.3538 - val_accuracy: 0.6948 - 66s/epoch - 161ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0358 - accuracy: 0.9976 - val_loss: 1.8167 - val_accuracy: 0.7575 - 66s/epoch - 160ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0414 - accuracy: 0.9958 - val_loss: 3.6525 - val_accuracy: 0.7437 - 66s/epoch - 160ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0364 - accuracy: 0.9972 - val_loss: 2.9723 - val_accuracy: 0.6890 - 65s/epoch - 157ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0488 - accuracy: 0.9948 - val_loss: 1.6854 - val_accuracy: 0.6131 - 64s/epoch - 155ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0420 - accuracy: 0.9963 - val_loss: 2.4962 - val_accuracy: 0.7067 - 63s/epoch - 153ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0406 - accuracy: 0.9961 - val_loss: 2.0636 - val_accuracy: 0.6633 - 65s/epoch - 158ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0440 - accuracy: 0.9952 - val_loss: 1.7303 - val_accuracy: 0.6600 - 65s/epoch - 158ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0342 - accuracy: 0.9973 - val_loss: 2.9370 - val_accuracy: 0.6985 - 64s/epoch - 155ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0459 - accuracy: 0.9948 - val_loss: 2.3160 - val_accuracy: 0.7625 - 60s/epoch - 144ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0430 - accuracy: 0.9966 - val_loss: 1.8081 - val_accuracy: 0.7715 - 60s/epoch - 146ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0511 - accuracy: 0.9945 - val_loss: 2.0090 - val_accuracy: 0.7196 - 66s/epoch - 159ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0420 - accuracy: 0.9961 - val_loss: 2.2254 - val_accuracy: 0.7387 - 65s/epoch - 158ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0485 - accuracy: 0.9947 - val_loss: 3.2276 - val_accuracy: 0.7010 - 65s/epoch - 158ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0329 - accuracy: 0.9981 - val_loss: 3.5552 - val_accuracy: 0.7067 - 65s/epoch - 156ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0392 - accuracy: 0.9968 - val_loss: 2.5032 - val_accuracy: 0.7060 - 64s/epoch - 155ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0475 - accuracy: 0.9948 - val_loss: 2.4659 - val_accuracy: 0.7069 - 64s/epoch - 155ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0459 - accuracy: 0.9950 - val_loss: 1.6302 - val_accuracy: 0.6798 - 65s/epoch - 158ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0391 - accuracy: 0.9967 - val_loss: 2.7954 - val_accuracy: 0.6881 - 65s/epoch - 158ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0397 - accuracy: 0.9970 - val_loss: 2.7269 - val_accuracy: 0.6935 - 62s/epoch - 151ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0298 - accuracy: 0.9976 - val_loss: 2.9918 - val_accuracy: 0.7552 - 64s/epoch - 156ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0394 - accuracy: 0.9955 - val_loss: 3.1756 - val_accuracy: 0.7046 - 64s/epoch - 154ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0392 - accuracy: 0.9964 - val_loss: 2.3688 - val_accuracy: 0.6969 - 67s/epoch - 163ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0500 - accuracy: 0.9948 - val_loss: 1.8304 - val_accuracy: 0.7619 - 62s/epoch - 151ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0420 - accuracy: 0.9961 - val_loss: 1.8152 - val_accuracy: 0.7060 - 65s/epoch - 157ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0430 - accuracy: 0.9958 - val_loss: 2.3785 - val_accuracy: 0.6781 - 63s/epoch - 152ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0464 - accuracy: 0.9950 - val_loss: 2.7369 - val_accuracy: 0.6992 - 65s/epoch - 157ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0425 - accuracy: 0.9955 - val_loss: 2.5153 - val_accuracy: 0.6933 - 66s/epoch - 160ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0364 - accuracy: 0.9970 - val_loss: 2.5449 - val_accuracy: 0.6877 - 66s/epoch - 159ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0472 - accuracy: 0.9955 - val_loss: 2.1925 - val_accuracy: 0.7054 - 65s/epoch - 157ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0433 - accuracy: 0.9951 - val_loss: 1.5273 - val_accuracy: 0.6844 - 63s/epoch - 153ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0447 - accuracy: 0.9957 - val_loss: 2.5607 - val_accuracy: 0.6560 - 64s/epoch - 154ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0468 - accuracy: 0.9952 - val_loss: 1.2632 - val_accuracy: 0.6621 - 64s/epoch - 154ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0374 - accuracy: 0.9970 - val_loss: 2.4839 - val_accuracy: 0.6983 - 64s/epoch - 156ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0328 - accuracy: 0.9970 - val_loss: 2.7852 - val_accuracy: 0.6535 - 60s/epoch - 146ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0334 - accuracy: 0.9963 - val_loss: 2.9673 - val_accuracy: 0.6856 - 62s/epoch - 150ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0458 - accuracy: 0.9952 - val_loss: 2.0945 - val_accuracy: 0.6517 - 66s/epoch - 160ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0470 - accuracy: 0.9951 - val_loss: 2.2312 - val_accuracy: 0.6954 - 66s/epoch - 160ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0420 - accuracy: 0.9964 - val_loss: 2.7721 - val_accuracy: 0.6798 - 65s/epoch - 157ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0430 - accuracy: 0.9960 - val_loss: 1.6161 - val_accuracy: 0.6637 - 64s/epoch - 155ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0364 - accuracy: 0.9967 - val_loss: 2.2653 - val_accuracy: 0.6615 - 65s/epoch - 157ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0386 - accuracy: 0.9964 - val_loss: 2.4853 - val_accuracy: 0.6531 - 63s/epoch - 153ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0461 - accuracy: 0.9942 - val_loss: 2.4233 - val_accuracy: 0.6735 - 67s/epoch - 162ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0376 - accuracy: 0.9970 - val_loss: 2.8389 - val_accuracy: 0.7083 - 65s/epoch - 157ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0399 - accuracy: 0.9952 - val_loss: 3.3493 - val_accuracy: 0.6975 - 62s/epoch - 150ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0491 - accuracy: 0.9957 - val_loss: 2.1374 - val_accuracy: 0.6877 - 68s/epoch - 165ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0290 - accuracy: 0.9988 - val_loss: 2.8043 - val_accuracy: 0.6998 - 65s/epoch - 156ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0423 - accuracy: 0.9950 - val_loss: 2.7353 - val_accuracy: 0.6810 - 65s/epoch - 157ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0438 - accuracy: 0.9950 - val_loss: 1.9691 - val_accuracy: 0.6283 - 63s/epoch - 154ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0411 - accuracy: 0.9964 - val_loss: 2.9816 - val_accuracy: 0.6552 - 61s/epoch - 147ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0449 - accuracy: 0.9963 - val_loss: 2.0950 - val_accuracy: 0.6856 - 64s/epoch - 156ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0342 - accuracy: 0.9976 - val_loss: 2.2542 - val_accuracy: 0.6865 - 60s/epoch - 146ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0431 - accuracy: 0.9958 - val_loss: 1.9834 - val_accuracy: 0.6846 - 65s/epoch - 157ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0457 - accuracy: 0.9952 - val_loss: 2.1485 - val_accuracy: 0.6827 - 67s/epoch - 163ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0321 - accuracy: 0.9977 - val_loss: 3.4144 - val_accuracy: 0.6985 - 67s/epoch - 161ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0380 - accuracy: 0.9963 - val_loss: 2.7890 - val_accuracy: 0.7013 - 62s/epoch - 150ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.99567\n",
      "413/413 - 61s - loss: 0.0470 - accuracy: 0.9943 - val_loss: 1.9592 - val_accuracy: 0.7142 - 61s/epoch - 147ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0344 - accuracy: 0.9973 - val_loss: 2.8791 - val_accuracy: 0.6858 - 67s/epoch - 161ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0513 - accuracy: 0.9939 - val_loss: 1.5818 - val_accuracy: 0.6612 - 65s/epoch - 157ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0403 - accuracy: 0.9965 - val_loss: 2.8201 - val_accuracy: 0.6977 - 66s/epoch - 160ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0361 - accuracy: 0.9975 - val_loss: 3.2219 - val_accuracy: 0.6963 - 63s/epoch - 153ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.99567\n",
      "413/413 - 57s - loss: 0.0477 - accuracy: 0.9942 - val_loss: 3.3147 - val_accuracy: 0.6890 - 57s/epoch - 138ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.99567\n",
      "413/413 - 59s - loss: 0.0368 - accuracy: 0.9970 - val_loss: 2.2958 - val_accuracy: 0.6392 - 59s/epoch - 143ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0434 - accuracy: 0.9955 - val_loss: 2.1953 - val_accuracy: 0.6985 - 65s/epoch - 158ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0337 - accuracy: 0.9970 - val_loss: 2.4928 - val_accuracy: 0.7248 - 66s/epoch - 160ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.99567\n",
      "413/413 - 69s - loss: 0.0408 - accuracy: 0.9956 - val_loss: 3.0530 - val_accuracy: 0.6931 - 69s/epoch - 167ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.99567\n",
      "413/413 - 70s - loss: 0.0413 - accuracy: 0.9955 - val_loss: 2.7611 - val_accuracy: 0.6979 - 70s/epoch - 170ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0359 - accuracy: 0.9959 - val_loss: 2.5796 - val_accuracy: 0.6452 - 66s/epoch - 159ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0459 - accuracy: 0.9948 - val_loss: 2.1341 - val_accuracy: 0.6919 - 64s/epoch - 155ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0420 - accuracy: 0.9957 - val_loss: 3.2317 - val_accuracy: 0.6538 - 60s/epoch - 146ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.99567\n",
      "413/413 - 59s - loss: 0.0407 - accuracy: 0.9958 - val_loss: 2.0897 - val_accuracy: 0.6612 - 59s/epoch - 143ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0351 - accuracy: 0.9973 - val_loss: 2.5796 - val_accuracy: 0.6983 - 67s/epoch - 161ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.99567\n",
      "413/413 - 68s - loss: 0.0337 - accuracy: 0.9967 - val_loss: 3.5333 - val_accuracy: 0.6994 - 68s/epoch - 166ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0279 - accuracy: 0.9981 - val_loss: 2.7814 - val_accuracy: 0.6915 - 66s/epoch - 160ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0321 - accuracy: 0.9964 - val_loss: 3.3010 - val_accuracy: 0.6919 - 67s/epoch - 162ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0383 - accuracy: 0.9961 - val_loss: 3.2813 - val_accuracy: 0.6687 - 65s/epoch - 158ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0380 - accuracy: 0.9965 - val_loss: 2.3169 - val_accuracy: 0.6231 - 67s/epoch - 162ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0593 - accuracy: 0.9930 - val_loss: 3.9018 - val_accuracy: 0.6950 - 65s/epoch - 156ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0363 - accuracy: 0.9974 - val_loss: 2.7117 - val_accuracy: 0.6446 - 60s/epoch - 146ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0387 - accuracy: 0.9961 - val_loss: 3.2433 - val_accuracy: 0.6950 - 62s/epoch - 150ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0360 - accuracy: 0.9963 - val_loss: 2.2428 - val_accuracy: 0.6827 - 67s/epoch - 161ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0521 - accuracy: 0.9954 - val_loss: 2.1748 - val_accuracy: 0.7296 - 67s/epoch - 161ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0364 - accuracy: 0.9964 - val_loss: 2.9385 - val_accuracy: 0.7110 - 65s/epoch - 157ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0308 - accuracy: 0.9978 - val_loss: 3.0519 - val_accuracy: 0.6694 - 60s/epoch - 144ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.99567\n",
      "413/413 - 58s - loss: 0.0365 - accuracy: 0.9962 - val_loss: 2.2700 - val_accuracy: 0.6846 - 58s/epoch - 140ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0312 - accuracy: 0.9976 - val_loss: 1.4851 - val_accuracy: 0.6573 - 65s/epoch - 157ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0404 - accuracy: 0.9961 - val_loss: 2.6311 - val_accuracy: 0.7306 - 66s/epoch - 160ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.99567\n",
      "413/413 - 67s - loss: 0.0464 - accuracy: 0.9948 - val_loss: 2.2291 - val_accuracy: 0.6448 - 67s/epoch - 162ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0377 - accuracy: 0.9958 - val_loss: 3.5087 - val_accuracy: 0.7004 - 65s/epoch - 158ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.99567\n",
      "413/413 - 59s - loss: 0.0456 - accuracy: 0.9941 - val_loss: 2.2145 - val_accuracy: 0.6456 - 59s/epoch - 144ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.99567\n",
      "413/413 - 60s - loss: 0.0318 - accuracy: 0.9983 - val_loss: 3.5699 - val_accuracy: 0.6917 - 60s/epoch - 144ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0372 - accuracy: 0.9967 - val_loss: 2.7658 - val_accuracy: 0.6494 - 66s/epoch - 160ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.99567\n",
      "413/413 - 65s - loss: 0.0446 - accuracy: 0.9953 - val_loss: 2.1808 - val_accuracy: 0.6815 - 65s/epoch - 158ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.99567\n",
      "413/413 - 63s - loss: 0.0324 - accuracy: 0.9975 - val_loss: 3.9246 - val_accuracy: 0.6975 - 63s/epoch - 153ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.99567\n",
      "413/413 - 62s - loss: 0.0415 - accuracy: 0.9962 - val_loss: 2.3382 - val_accuracy: 0.6998 - 62s/epoch - 149ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.99567\n",
      "413/413 - 64s - loss: 0.0274 - accuracy: 0.9983 - val_loss: 3.4095 - val_accuracy: 0.7019 - 64s/epoch - 156ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0369 - accuracy: 0.9963 - val_loss: 2.1008 - val_accuracy: 0.6465 - 66s/epoch - 159ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0355 - accuracy: 0.9969 - val_loss: 3.0565 - val_accuracy: 0.6256 - 66s/epoch - 159ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.99567\n",
      "413/413 - 66s - loss: 0.0469 - accuracy: 0.9945 - val_loss: 4.1663 - val_accuracy: 0.6923 - 66s/epoch - 159ms/step\n",
      "179/179 [==============================] - 7s 39ms/step\n",
      "Classification accuracy: 0.528532 \n"
     ]
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43119222 0.5688078 ]\n",
      " [0.67419416 0.32580587]\n",
      " [0.39153677 0.60846317]\n",
      " ...\n",
      " [0.7552755  0.24472453]\n",
      " [0.7724776  0.22752233]\n",
      " [0.83143944 0.16856062]]\n",
      "[1 0 1 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[2272 1028]\n",
      " [1608  792]]\n",
      "[53.75 58.56 43.52]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for TSGL on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)\n",
    "\n",
    "plot_confusion_matrix(preds_TSGL, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for TSGL on ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.63831, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 21s - loss: 0.3810 - accuracy: 0.8192 - val_loss: 1.6383 - val_accuracy: 0.6617 - 21s/epoch - 50ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0697 - accuracy: 0.9789 - val_loss: 2.0355 - val_accuracy: 0.6875 - 18s/epoch - 44ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.63831\n",
      "413/413 - 19s - loss: 0.0434 - accuracy: 0.9873 - val_loss: 2.8920 - val_accuracy: 0.6904 - 19s/epoch - 45ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0396 - accuracy: 0.9871 - val_loss: 2.4820 - val_accuracy: 0.7204 - 18s/epoch - 43ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0329 - accuracy: 0.9883 - val_loss: 1.9143 - val_accuracy: 0.6835 - 17s/epoch - 41ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0238 - accuracy: 0.9927 - val_loss: 2.5088 - val_accuracy: 0.6450 - 17s/epoch - 42ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0248 - accuracy: 0.9927 - val_loss: 2.3175 - val_accuracy: 0.6975 - 17s/epoch - 41ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0304 - accuracy: 0.9894 - val_loss: 3.0988 - val_accuracy: 0.6810 - 17s/epoch - 42ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0239 - accuracy: 0.9923 - val_loss: 2.5828 - val_accuracy: 0.7212 - 17s/epoch - 41ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0211 - accuracy: 0.9947 - val_loss: 2.9606 - val_accuracy: 0.7256 - 17s/epoch - 40ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0173 - accuracy: 0.9949 - val_loss: 2.0321 - val_accuracy: 0.7302 - 16s/epoch - 39ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0185 - accuracy: 0.9942 - val_loss: 3.0831 - val_accuracy: 0.7048 - 16s/epoch - 39ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0157 - accuracy: 0.9958 - val_loss: 3.4268 - val_accuracy: 0.7258 - 17s/epoch - 41ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0141 - accuracy: 0.9961 - val_loss: 2.9288 - val_accuracy: 0.7029 - 18s/epoch - 44ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0242 - accuracy: 0.9934 - val_loss: 2.6596 - val_accuracy: 0.7240 - 18s/epoch - 44ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0161 - accuracy: 0.9961 - val_loss: 3.1502 - val_accuracy: 0.6687 - 18s/epoch - 45ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0183 - accuracy: 0.9958 - val_loss: 3.4038 - val_accuracy: 0.7267 - 18s/epoch - 44ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0085 - accuracy: 0.9975 - val_loss: 2.3609 - val_accuracy: 0.7167 - 18s/epoch - 44ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0124 - accuracy: 0.9962 - val_loss: 3.3200 - val_accuracy: 0.6687 - 18s/epoch - 45ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.63831\n",
      "413/413 - 19s - loss: 0.0206 - accuracy: 0.9939 - val_loss: 3.1064 - val_accuracy: 0.6733 - 19s/epoch - 46ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0084 - accuracy: 0.9976 - val_loss: 2.3155 - val_accuracy: 0.7029 - 18s/epoch - 44ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0091 - accuracy: 0.9971 - val_loss: 2.7474 - val_accuracy: 0.6925 - 18s/epoch - 44ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0114 - accuracy: 0.9961 - val_loss: 3.5013 - val_accuracy: 0.6913 - 18s/epoch - 44ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0123 - accuracy: 0.9961 - val_loss: 3.5156 - val_accuracy: 0.6871 - 18s/epoch - 44ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 4.1367 - val_accuracy: 0.6704 - 18s/epoch - 44ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0076 - accuracy: 0.9967 - val_loss: 3.4868 - val_accuracy: 0.6925 - 18s/epoch - 45ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0138 - accuracy: 0.9964 - val_loss: 2.3030 - val_accuracy: 0.7260 - 18s/epoch - 44ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0086 - accuracy: 0.9979 - val_loss: 1.6559 - val_accuracy: 0.7329 - 18s/epoch - 44ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0081 - accuracy: 0.9973 - val_loss: 3.0258 - val_accuracy: 0.6954 - 17s/epoch - 42ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0070 - accuracy: 0.9976 - val_loss: 2.8997 - val_accuracy: 0.7135 - 17s/epoch - 41ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0128 - accuracy: 0.9955 - val_loss: 2.5788 - val_accuracy: 0.6723 - 16s/epoch - 39ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0098 - accuracy: 0.9968 - val_loss: 3.0979 - val_accuracy: 0.6587 - 16s/epoch - 39ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0108 - accuracy: 0.9969 - val_loss: 3.7628 - val_accuracy: 0.6610 - 16s/epoch - 39ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0096 - accuracy: 0.9968 - val_loss: 2.6957 - val_accuracy: 0.6658 - 16s/epoch - 40ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0052 - accuracy: 0.9983 - val_loss: 4.1661 - val_accuracy: 0.6677 - 17s/epoch - 41ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0041 - accuracy: 0.9990 - val_loss: 2.7722 - val_accuracy: 0.6775 - 18s/epoch - 43ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.63831\n",
      "413/413 - 19s - loss: 0.0048 - accuracy: 0.9988 - val_loss: 2.9573 - val_accuracy: 0.6712 - 19s/epoch - 45ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.63831\n",
      "413/413 - 19s - loss: 0.0062 - accuracy: 0.9981 - val_loss: 2.9689 - val_accuracy: 0.6681 - 19s/epoch - 45ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0178 - accuracy: 0.9950 - val_loss: 2.4008 - val_accuracy: 0.7344 - 18s/epoch - 45ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.63831\n",
      "413/413 - 19s - loss: 0.0046 - accuracy: 0.9985 - val_loss: 3.2482 - val_accuracy: 0.6885 - 19s/epoch - 45ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0113 - accuracy: 0.9969 - val_loss: 2.8108 - val_accuracy: 0.7183 - 18s/epoch - 44ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0076 - accuracy: 0.9980 - val_loss: 2.8493 - val_accuracy: 0.7306 - 18s/epoch - 45ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0034 - accuracy: 0.9987 - val_loss: 3.5717 - val_accuracy: 0.6702 - 18s/epoch - 44ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 2.6793 - val_accuracy: 0.6915 - 18s/epoch - 44ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0037 - accuracy: 0.9988 - val_loss: 3.7018 - val_accuracy: 0.7346 - 18s/epoch - 44ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0057 - accuracy: 0.9981 - val_loss: 2.6349 - val_accuracy: 0.7412 - 18s/epoch - 44ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0085 - accuracy: 0.9976 - val_loss: 4.8453 - val_accuracy: 0.6710 - 18s/epoch - 44ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0061 - accuracy: 0.9984 - val_loss: 3.8185 - val_accuracy: 0.6650 - 18s/epoch - 43ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0116 - accuracy: 0.9967 - val_loss: 3.6028 - val_accuracy: 0.6767 - 17s/epoch - 42ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0074 - accuracy: 0.9980 - val_loss: 4.2729 - val_accuracy: 0.6706 - 17s/epoch - 41ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0054 - accuracy: 0.9977 - val_loss: 3.8839 - val_accuracy: 0.6712 - 16s/epoch - 39ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0041 - accuracy: 0.9987 - val_loss: 3.6766 - val_accuracy: 0.6800 - 17s/epoch - 40ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.63831\n",
      "413/413 - 16s - loss: 0.0122 - accuracy: 0.9965 - val_loss: 3.7362 - val_accuracy: 0.6742 - 16s/epoch - 40ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0104 - accuracy: 0.9977 - val_loss: 4.4786 - val_accuracy: 0.6696 - 17s/epoch - 40ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0047 - accuracy: 0.9987 - val_loss: 2.9051 - val_accuracy: 0.7117 - 17s/epoch - 42ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0040 - accuracy: 0.9989 - val_loss: 3.6652 - val_accuracy: 0.6748 - 18s/epoch - 45ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0043 - accuracy: 0.9983 - val_loss: 3.0246 - val_accuracy: 0.6802 - 18s/epoch - 44ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0063 - accuracy: 0.9978 - val_loss: 3.9373 - val_accuracy: 0.6808 - 18s/epoch - 44ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0072 - accuracy: 0.9977 - val_loss: 2.7234 - val_accuracy: 0.7277 - 18s/epoch - 44ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0064 - accuracy: 0.9980 - val_loss: 3.6848 - val_accuracy: 0.6783 - 18s/epoch - 44ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0072 - accuracy: 0.9978 - val_loss: 2.5679 - val_accuracy: 0.6790 - 18s/epoch - 44ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0077 - accuracy: 0.9972 - val_loss: 2.9046 - val_accuracy: 0.7133 - 18s/epoch - 43ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.63831\n",
      "413/413 - 17s - loss: 0.0044 - accuracy: 0.9983 - val_loss: 3.9305 - val_accuracy: 0.6685 - 17s/epoch - 42ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0087 - accuracy: 0.9980 - val_loss: 3.5825 - val_accuracy: 0.6967 - 18s/epoch - 43ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0024 - accuracy: 0.9991 - val_loss: 3.8916 - val_accuracy: 0.6896 - 18s/epoch - 44ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0055 - accuracy: 0.9981 - val_loss: 3.2310 - val_accuracy: 0.6733 - 18s/epoch - 43ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0031 - accuracy: 0.9991 - val_loss: 2.5382 - val_accuracy: 0.6854 - 18s/epoch - 43ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0075 - accuracy: 0.9975 - val_loss: 2.3340 - val_accuracy: 0.6831 - 18s/epoch - 43ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.63831\n",
      "413/413 - 18s - loss: 0.0068 - accuracy: 0.9979 - val_loss: 2.5531 - val_accuracy: 0.6715 - 18s/epoch - 44ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss improved from 1.63831 to 1.44149, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 18s - loss: 0.0075 - accuracy: 0.9983 - val_loss: 1.4415 - val_accuracy: 0.6819 - 18s/epoch - 44ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0100 - accuracy: 0.9970 - val_loss: 3.2281 - val_accuracy: 0.6919 - 17s/epoch - 42ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0025 - accuracy: 0.9992 - val_loss: 3.3005 - val_accuracy: 0.7248 - 16s/epoch - 40ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 3.7041 - val_accuracy: 0.7088 - 16s/epoch - 39ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0062 - accuracy: 0.9974 - val_loss: 2.3452 - val_accuracy: 0.7146 - 16s/epoch - 39ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0072 - accuracy: 0.9980 - val_loss: 4.2292 - val_accuracy: 0.7100 - 17s/epoch - 40ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 3.8455 - val_accuracy: 0.6792 - 17s/epoch - 40ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0049 - accuracy: 0.9989 - val_loss: 2.9517 - val_accuracy: 0.6846 - 18s/epoch - 43ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0051 - accuracy: 0.9981 - val_loss: 4.0235 - val_accuracy: 0.6775 - 18s/epoch - 44ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0029 - accuracy: 0.9989 - val_loss: 2.5096 - val_accuracy: 0.7246 - 18s/epoch - 44ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0051 - accuracy: 0.9980 - val_loss: 2.6027 - val_accuracy: 0.6875 - 18s/epoch - 42ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0054 - accuracy: 0.9987 - val_loss: 3.5430 - val_accuracy: 0.6810 - 17s/epoch - 41ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0069 - accuracy: 0.9977 - val_loss: 2.9305 - val_accuracy: 0.6888 - 17s/epoch - 42ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0018 - accuracy: 0.9992 - val_loss: 4.0588 - val_accuracy: 0.6727 - 18s/epoch - 44ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0026 - accuracy: 0.9993 - val_loss: 3.3755 - val_accuracy: 0.6610 - 18s/epoch - 44ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0048 - accuracy: 0.9983 - val_loss: 3.8109 - val_accuracy: 0.6796 - 18s/epoch - 44ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0075 - accuracy: 0.9980 - val_loss: 3.6694 - val_accuracy: 0.6769 - 18s/epoch - 44ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0034 - accuracy: 0.9987 - val_loss: 3.9456 - val_accuracy: 0.6719 - 18s/epoch - 44ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0085 - accuracy: 0.9970 - val_loss: 3.2404 - val_accuracy: 0.6846 - 19s/epoch - 45ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0030 - accuracy: 0.9989 - val_loss: 2.1813 - val_accuracy: 0.6910 - 18s/epoch - 44ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0121 - accuracy: 0.9968 - val_loss: 2.3956 - val_accuracy: 0.6783 - 18s/epoch - 43ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0026 - accuracy: 0.9990 - val_loss: 2.6547 - val_accuracy: 0.7138 - 18s/epoch - 44ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0036 - accuracy: 0.9990 - val_loss: 2.9637 - val_accuracy: 0.7019 - 18s/epoch - 43ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0069 - accuracy: 0.9982 - val_loss: 3.0163 - val_accuracy: 0.7054 - 17s/epoch - 42ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0045 - accuracy: 0.9980 - val_loss: 2.1600 - val_accuracy: 0.7135 - 18s/epoch - 43ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0018 - accuracy: 0.9993 - val_loss: 3.3194 - val_accuracy: 0.6921 - 18s/epoch - 43ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 3.3759 - val_accuracy: 0.7258 - 18s/epoch - 43ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0098 - accuracy: 0.9974 - val_loss: 3.2074 - val_accuracy: 0.7081 - 17s/epoch - 42ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.9241 - val_accuracy: 0.7038 - 17s/epoch - 42ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 9.7639e-04 - accuracy: 0.9997 - val_loss: 4.3643 - val_accuracy: 0.7200 - 17s/epoch - 40ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 3.6216 - val_accuracy: 0.6829 - 16s/epoch - 39ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0061 - accuracy: 0.9977 - val_loss: 2.4320 - val_accuracy: 0.6942 - 16s/epoch - 39ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0039 - accuracy: 0.9988 - val_loss: 2.9057 - val_accuracy: 0.7294 - 17s/epoch - 41ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0039 - accuracy: 0.9986 - val_loss: 2.8590 - val_accuracy: 0.6929 - 17s/epoch - 41ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0031 - accuracy: 0.9992 - val_loss: 3.5829 - val_accuracy: 0.6904 - 17s/epoch - 40ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 4.6147 - val_accuracy: 0.6677 - 17s/epoch - 41ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0125 - accuracy: 0.9968 - val_loss: 2.9277 - val_accuracy: 0.6979 - 18s/epoch - 44ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.9713 - val_accuracy: 0.7100 - 18s/epoch - 44ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 4.1040 - val_accuracy: 0.7369 - 18s/epoch - 44ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0033 - accuracy: 0.9989 - val_loss: 2.7646 - val_accuracy: 0.7271 - 18s/epoch - 44ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0049 - accuracy: 0.9986 - val_loss: 2.6060 - val_accuracy: 0.7367 - 18s/epoch - 44ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0029 - accuracy: 0.9988 - val_loss: 3.4901 - val_accuracy: 0.6885 - 18s/epoch - 44ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0075 - accuracy: 0.9976 - val_loss: 3.4220 - val_accuracy: 0.6710 - 18s/epoch - 44ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 4.3448 - val_accuracy: 0.6737 - 18s/epoch - 44ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0018 - accuracy: 0.9996 - val_loss: 2.6828 - val_accuracy: 0.6958 - 18s/epoch - 44ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0052 - accuracy: 0.9983 - val_loss: 3.6257 - val_accuracy: 0.7235 - 18s/epoch - 44ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0022 - accuracy: 0.9993 - val_loss: 2.6576 - val_accuracy: 0.7088 - 18s/epoch - 44ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0030 - accuracy: 0.9989 - val_loss: 3.1862 - val_accuracy: 0.6794 - 17s/epoch - 42ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0034 - accuracy: 0.9992 - val_loss: 2.8572 - val_accuracy: 0.7206 - 17s/epoch - 42ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 3.6503 - val_accuracy: 0.6910 - 16s/epoch - 40ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0019 - accuracy: 0.9992 - val_loss: 4.2911 - val_accuracy: 0.6685 - 16s/epoch - 39ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0067 - accuracy: 0.9978 - val_loss: 3.3543 - val_accuracy: 0.6885 - 16s/epoch - 39ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0023 - accuracy: 0.9991 - val_loss: 3.9355 - val_accuracy: 0.7008 - 17s/epoch - 41ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 4.0237e-04 - accuracy: 1.0000 - val_loss: 4.2461 - val_accuracy: 0.6923 - 17s/epoch - 41ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 9.6778e-04 - accuracy: 0.9995 - val_loss: 4.7041 - val_accuracy: 0.6969 - 17s/epoch - 41ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0063 - accuracy: 0.9978 - val_loss: 3.3906 - val_accuracy: 0.7125 - 17s/epoch - 40ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0047 - accuracy: 0.9983 - val_loss: 3.4784 - val_accuracy: 0.7244 - 16s/epoch - 40ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0037 - accuracy: 0.9990 - val_loss: 3.3457 - val_accuracy: 0.7508 - 17s/epoch - 42ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 4.3380 - val_accuracy: 0.6742 - 19s/epoch - 46ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0015 - accuracy: 0.9993 - val_loss: 3.2991 - val_accuracy: 0.7552 - 19s/epoch - 47ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0028 - accuracy: 0.9991 - val_loss: 3.6415 - val_accuracy: 0.7119 - 19s/epoch - 45ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0077 - accuracy: 0.9982 - val_loss: 2.0375 - val_accuracy: 0.7433 - 18s/epoch - 45ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0037 - accuracy: 0.9985 - val_loss: 2.9073 - val_accuracy: 0.7410 - 18s/epoch - 44ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0035 - accuracy: 0.9988 - val_loss: 3.2416 - val_accuracy: 0.7179 - 18s/epoch - 45ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 4.5014 - val_accuracy: 0.6840 - 18s/epoch - 44ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 4.1881 - val_accuracy: 0.6706 - 18s/epoch - 44ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0021 - accuracy: 0.9992 - val_loss: 3.6456 - val_accuracy: 0.7244 - 18s/epoch - 44ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0050 - accuracy: 0.9987 - val_loss: 2.2573 - val_accuracy: 0.7202 - 18s/epoch - 44ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 4.0805 - val_accuracy: 0.6737 - 18s/epoch - 44ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 4.9198e-04 - accuracy: 0.9998 - val_loss: 4.1214 - val_accuracy: 0.6865 - 18s/epoch - 43ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.0636 - val_accuracy: 0.7063 - 17s/epoch - 41ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0063 - accuracy: 0.9985 - val_loss: 2.6628 - val_accuracy: 0.7206 - 17s/epoch - 40ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0032 - accuracy: 0.9986 - val_loss: 3.5815 - val_accuracy: 0.6804 - 16s/epoch - 39ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 3.3146 - val_accuracy: 0.7235 - 16s/epoch - 38ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0028 - accuracy: 0.9990 - val_loss: 4.3617 - val_accuracy: 0.6821 - 16s/epoch - 39ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0017 - accuracy: 0.9995 - val_loss: 3.1586 - val_accuracy: 0.7467 - 17s/epoch - 40ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 3.7674 - val_accuracy: 0.6513 - 17s/epoch - 42ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0055 - accuracy: 0.9983 - val_loss: 3.2303 - val_accuracy: 0.6794 - 18s/epoch - 45ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 3.4286 - val_accuracy: 0.7471 - 19s/epoch - 45ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0030 - accuracy: 0.9990 - val_loss: 2.5758 - val_accuracy: 0.7550 - 19s/epoch - 45ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 3.4500 - val_accuracy: 0.7106 - 18s/epoch - 44ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0074 - accuracy: 0.9983 - val_loss: 3.1795 - val_accuracy: 0.7471 - 18s/epoch - 44ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 3.5616 - val_accuracy: 0.7287 - 18s/epoch - 45ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 2.8998 - val_accuracy: 0.6910 - 18s/epoch - 45ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0045 - accuracy: 0.9983 - val_loss: 3.8621 - val_accuracy: 0.7369 - 18s/epoch - 44ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0053 - accuracy: 0.9986 - val_loss: 2.9483 - val_accuracy: 0.7063 - 18s/epoch - 44ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0065 - accuracy: 0.9986 - val_loss: 2.9255 - val_accuracy: 0.7033 - 19s/epoch - 46ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 8.1465e-04 - accuracy: 0.9998 - val_loss: 4.0097 - val_accuracy: 0.6779 - 18s/epoch - 44ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0032 - accuracy: 0.9991 - val_loss: 3.1398 - val_accuracy: 0.7065 - 18s/epoch - 44ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0026 - accuracy: 0.9991 - val_loss: 4.1885 - val_accuracy: 0.7308 - 18s/epoch - 43ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0033 - accuracy: 0.9987 - val_loss: 2.9289 - val_accuracy: 0.6808 - 17s/epoch - 41ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 5.9435e-04 - accuracy: 0.9998 - val_loss: 4.8300 - val_accuracy: 0.6894 - 17s/epoch - 40ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0023 - accuracy: 0.9992 - val_loss: 3.1023 - val_accuracy: 0.7194 - 17s/epoch - 40ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 5.1203 - val_accuracy: 0.6779 - 17s/epoch - 41ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 7.9719e-05 - accuracy: 1.0000 - val_loss: 4.8044 - val_accuracy: 0.7035 - 18s/epoch - 44ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 4.3277 - val_accuracy: 0.7377 - 18s/epoch - 45ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0053 - accuracy: 0.9983 - val_loss: 3.0547 - val_accuracy: 0.6877 - 18s/epoch - 44ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0038 - accuracy: 0.9989 - val_loss: 3.9678 - val_accuracy: 0.7265 - 18s/epoch - 44ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0017 - accuracy: 0.9992 - val_loss: 4.6191 - val_accuracy: 0.7221 - 17s/epoch - 42ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0051 - accuracy: 0.9984 - val_loss: 4.4552 - val_accuracy: 0.7279 - 17s/epoch - 41ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0056 - accuracy: 0.9984 - val_loss: 4.4175 - val_accuracy: 0.6815 - 16s/epoch - 40ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0031 - accuracy: 0.9989 - val_loss: 4.4446 - val_accuracy: 0.7335 - 16s/epoch - 40ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 8.1076e-04 - accuracy: 0.9997 - val_loss: 4.0299 - val_accuracy: 0.7373 - 16s/epoch - 39ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0049 - accuracy: 0.9980 - val_loss: 4.0061 - val_accuracy: 0.7269 - 16s/epoch - 39ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 3.9809 - val_accuracy: 0.7077 - 16s/epoch - 39ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 8.1131e-04 - accuracy: 0.9998 - val_loss: 4.6393 - val_accuracy: 0.7569 - 16s/epoch - 40ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 4.2054 - val_accuracy: 0.7531 - 17s/epoch - 40ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 3.0169e-04 - accuracy: 0.9999 - val_loss: 4.9564 - val_accuracy: 0.7233 - 18s/epoch - 43ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0034 - accuracy: 0.9990 - val_loss: 2.7503 - val_accuracy: 0.7223 - 18s/epoch - 45ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0044 - accuracy: 0.9988 - val_loss: 4.8544 - val_accuracy: 0.7092 - 19s/epoch - 45ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 3.4621 - val_accuracy: 0.7281 - 18s/epoch - 44ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0040 - accuracy: 0.9989 - val_loss: 4.6066 - val_accuracy: 0.7115 - 18s/epoch - 45ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0044 - accuracy: 0.9985 - val_loss: 4.2171 - val_accuracy: 0.6938 - 19s/epoch - 45ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0015 - accuracy: 0.9993 - val_loss: 5.3891 - val_accuracy: 0.6719 - 18s/epoch - 44ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0020 - accuracy: 0.9992 - val_loss: 4.0120 - val_accuracy: 0.7079 - 18s/epoch - 45ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 4.1795 - val_accuracy: 0.6804 - 18s/epoch - 43ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0069 - accuracy: 0.9980 - val_loss: 2.5706 - val_accuracy: 0.7312 - 17s/epoch - 42ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0030 - accuracy: 0.9991 - val_loss: 3.9333 - val_accuracy: 0.6833 - 18s/epoch - 43ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0064 - accuracy: 0.9985 - val_loss: 3.3562 - val_accuracy: 0.7227 - 18s/epoch - 43ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 9.5042e-04 - accuracy: 0.9998 - val_loss: 3.9724 - val_accuracy: 0.7054 - 18s/epoch - 44ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0027 - accuracy: 0.9990 - val_loss: 2.9613 - val_accuracy: 0.7273 - 19s/epoch - 46ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 4.5452 - val_accuracy: 0.7269 - 17s/epoch - 41ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0018 - accuracy: 0.9992 - val_loss: 3.7043 - val_accuracy: 0.7273 - 18s/epoch - 44ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 3.2995 - val_accuracy: 0.7871 - 19s/epoch - 45ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0023 - accuracy: 0.9992 - val_loss: 4.0192 - val_accuracy: 0.7196 - 18s/epoch - 43ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0029 - accuracy: 0.9989 - val_loss: 3.4758 - val_accuracy: 0.7354 - 17s/epoch - 41ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0035 - accuracy: 0.9986 - val_loss: 4.3434 - val_accuracy: 0.6717 - 17s/epoch - 40ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0050 - accuracy: 0.9989 - val_loss: 3.6224 - val_accuracy: 0.7442 - 16s/epoch - 39ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 8.3536e-04 - accuracy: 0.9998 - val_loss: 4.1639 - val_accuracy: 0.7600 - 16s/epoch - 39ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 5.1207e-04 - accuracy: 0.9998 - val_loss: 4.3178 - val_accuracy: 0.7075 - 16s/epoch - 39ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0039 - accuracy: 0.9986 - val_loss: 2.6528 - val_accuracy: 0.7565 - 16s/epoch - 40ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0044 - accuracy: 0.9992 - val_loss: 3.7112 - val_accuracy: 0.7060 - 17s/epoch - 41ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 6.0695e-04 - accuracy: 0.9998 - val_loss: 5.5726 - val_accuracy: 0.6752 - 18s/epoch - 44ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0035 - accuracy: 0.9987 - val_loss: 3.7940 - val_accuracy: 0.6654 - 18s/epoch - 45ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0035 - accuracy: 0.9987 - val_loss: 6.0606 - val_accuracy: 0.7052 - 18s/epoch - 45ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0029 - accuracy: 0.9993 - val_loss: 4.0683 - val_accuracy: 0.6767 - 18s/epoch - 44ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0076 - accuracy: 0.9982 - val_loss: 3.7718 - val_accuracy: 0.6779 - 18s/epoch - 45ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0030 - accuracy: 0.9990 - val_loss: 3.7542 - val_accuracy: 0.7352 - 18s/epoch - 44ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0014 - accuracy: 0.9997 - val_loss: 5.1481 - val_accuracy: 0.6794 - 18s/epoch - 44ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 5.1440 - val_accuracy: 0.6773 - 18s/epoch - 45ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0035 - accuracy: 0.9990 - val_loss: 3.7383 - val_accuracy: 0.6829 - 18s/epoch - 45ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0059 - accuracy: 0.9991 - val_loss: 4.7766 - val_accuracy: 0.6704 - 18s/epoch - 44ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0012 - accuracy: 0.9995 - val_loss: 5.2401 - val_accuracy: 0.6656 - 18s/epoch - 45ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 7.0030e-04 - accuracy: 0.9997 - val_loss: 5.2900 - val_accuracy: 0.6729 - 18s/epoch - 44ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0036 - accuracy: 0.9988 - val_loss: 4.4243 - val_accuracy: 0.6727 - 18s/epoch - 43ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0019 - accuracy: 0.9995 - val_loss: 5.4618 - val_accuracy: 0.6773 - 18s/epoch - 44ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 4.6969 - val_accuracy: 0.6837 - 18s/epoch - 44ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 9.8199e-04 - accuracy: 0.9998 - val_loss: 5.3338 - val_accuracy: 0.6787 - 17s/epoch - 42ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0075 - accuracy: 0.9983 - val_loss: 3.8446 - val_accuracy: 0.7171 - 17s/epoch - 40ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 3.8760 - val_accuracy: 0.6923 - 16s/epoch - 39ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 7.6675e-04 - accuracy: 0.9998 - val_loss: 4.3696 - val_accuracy: 0.6827 - 16s/epoch - 39ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 1.2484e-04 - accuracy: 1.0000 - val_loss: 5.6855 - val_accuracy: 0.6792 - 16s/epoch - 39ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0030 - accuracy: 0.9990 - val_loss: 5.1643 - val_accuracy: 0.6750 - 17s/epoch - 40ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0063 - accuracy: 0.9980 - val_loss: 4.7648 - val_accuracy: 0.6792 - 17s/epoch - 41ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0017 - accuracy: 0.9992 - val_loss: 4.9949 - val_accuracy: 0.6804 - 18s/epoch - 45ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0012 - accuracy: 0.9995 - val_loss: 4.2937 - val_accuracy: 0.6906 - 19s/epoch - 46ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0012 - accuracy: 0.9995 - val_loss: 4.4057 - val_accuracy: 0.6479 - 19s/epoch - 45ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0047 - accuracy: 0.9989 - val_loss: 4.2572 - val_accuracy: 0.6869 - 18s/epoch - 45ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 6.0791e-04 - accuracy: 0.9998 - val_loss: 5.3752 - val_accuracy: 0.6804 - 18s/epoch - 45ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 5.9571 - val_accuracy: 0.6754 - 19s/epoch - 45ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 4.5928e-04 - accuracy: 0.9998 - val_loss: 6.0898 - val_accuracy: 0.6725 - 19s/epoch - 46ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0021 - accuracy: 0.9992 - val_loss: 4.4427 - val_accuracy: 0.7269 - 18s/epoch - 44ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0029 - accuracy: 0.9994 - val_loss: 5.2631 - val_accuracy: 0.6798 - 18s/epoch - 43ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9995 - val_loss: 5.1798 - val_accuracy: 0.7256 - 19s/epoch - 45ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0020 - accuracy: 0.9995 - val_loss: 3.9816 - val_accuracy: 0.6973 - 18s/epoch - 42ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 4.5946 - val_accuracy: 0.7023 - 18s/epoch - 44ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0035 - accuracy: 0.9990 - val_loss: 4.9749 - val_accuracy: 0.6742 - 19s/epoch - 47ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0019 - accuracy: 0.9992 - val_loss: 5.6032 - val_accuracy: 0.6767 - 19s/epoch - 45ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 4.2340 - val_accuracy: 0.6731 - 19s/epoch - 46ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0014 - accuracy: 0.9993 - val_loss: 4.2536 - val_accuracy: 0.7027 - 19s/epoch - 46ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0011 - accuracy: 0.9995 - val_loss: 5.4695 - val_accuracy: 0.6756 - 19s/epoch - 45ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0020 - accuracy: 0.9993 - val_loss: 4.4496 - val_accuracy: 0.6715 - 18s/epoch - 45ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 1.44149\n",
      "413/413 - 20s - loss: 0.0060 - accuracy: 0.9986 - val_loss: 3.9894 - val_accuracy: 0.6927 - 20s/epoch - 48ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 1.44149\n",
      "413/413 - 21s - loss: 0.0010 - accuracy: 0.9995 - val_loss: 5.2112 - val_accuracy: 0.6785 - 21s/epoch - 50ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 1.44149\n",
      "413/413 - 21s - loss: 0.0072 - accuracy: 0.9986 - val_loss: 3.6097 - val_accuracy: 0.6819 - 21s/epoch - 52ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 1.44149\n",
      "413/413 - 21s - loss: 0.0035 - accuracy: 0.9985 - val_loss: 4.3139 - val_accuracy: 0.7056 - 21s/epoch - 52ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 1.44149\n",
      "413/413 - 23s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 4.2167 - val_accuracy: 0.6925 - 23s/epoch - 57ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 1.44149\n",
      "413/413 - 20s - loss: 6.5262e-04 - accuracy: 0.9998 - val_loss: 5.2058 - val_accuracy: 0.7210 - 20s/epoch - 48ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 7.9089e-04 - accuracy: 0.9997 - val_loss: 6.0216 - val_accuracy: 0.6925 - 19s/epoch - 46ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 6.7658e-04 - accuracy: 0.9998 - val_loss: 5.5624 - val_accuracy: 0.6865 - 19s/epoch - 45ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 1.2202e-04 - accuracy: 1.0000 - val_loss: 6.0940 - val_accuracy: 0.6737 - 19s/epoch - 45ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0032 - accuracy: 0.9989 - val_loss: 4.9205 - val_accuracy: 0.6744 - 19s/epoch - 45ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0029 - accuracy: 0.9989 - val_loss: 4.4335 - val_accuracy: 0.6944 - 18s/epoch - 45ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 4.0956e-04 - accuracy: 0.9998 - val_loss: 6.0577 - val_accuracy: 0.6821 - 19s/epoch - 45ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0025 - accuracy: 0.9992 - val_loss: 4.9130 - val_accuracy: 0.6810 - 19s/epoch - 46ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0019 - accuracy: 0.9992 - val_loss: 5.5142 - val_accuracy: 0.6733 - 18s/epoch - 45ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0050 - accuracy: 0.9986 - val_loss: 4.1497 - val_accuracy: 0.6744 - 18s/epoch - 44ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0042 - accuracy: 0.9987 - val_loss: 4.5195 - val_accuracy: 0.6754 - 19s/epoch - 45ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0013 - accuracy: 0.9995 - val_loss: 3.8854 - val_accuracy: 0.7050 - 18s/epoch - 43ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 9.0138e-04 - accuracy: 0.9996 - val_loss: 5.6236 - val_accuracy: 0.6787 - 17s/epoch - 41ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0026 - accuracy: 0.9991 - val_loss: 4.6095 - val_accuracy: 0.6810 - 17s/epoch - 40ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0019 - accuracy: 0.9993 - val_loss: 4.3517 - val_accuracy: 0.6737 - 16s/epoch - 39ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0044 - accuracy: 0.9989 - val_loss: 3.8317 - val_accuracy: 0.6815 - 16s/epoch - 39ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 3.8236e-04 - accuracy: 0.9998 - val_loss: 4.9814 - val_accuracy: 0.6760 - 16s/epoch - 39ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 6.4399e-04 - accuracy: 0.9997 - val_loss: 4.7451 - val_accuracy: 0.6706 - 17s/epoch - 40ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 4.4422 - val_accuracy: 0.6754 - 17s/epoch - 42ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 1.1214e-04 - accuracy: 1.0000 - val_loss: 5.6194 - val_accuracy: 0.6717 - 18s/epoch - 44ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0086 - accuracy: 0.9983 - val_loss: 4.4726 - val_accuracy: 0.6671 - 19s/epoch - 45ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0069 - accuracy: 0.9978 - val_loss: 4.6514 - val_accuracy: 0.6771 - 19s/epoch - 45ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 8.1814e-04 - accuracy: 0.9998 - val_loss: 5.8964 - val_accuracy: 0.6717 - 19s/epoch - 46ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 9.7625e-04 - accuracy: 0.9997 - val_loss: 4.8905 - val_accuracy: 0.6823 - 19s/epoch - 45ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 5.4645e-04 - accuracy: 0.9997 - val_loss: 5.2040 - val_accuracy: 0.6842 - 18s/epoch - 45ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0052 - accuracy: 0.9986 - val_loss: 4.2032 - val_accuracy: 0.6844 - 18s/epoch - 44ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0017 - accuracy: 0.9993 - val_loss: 5.5730 - val_accuracy: 0.6681 - 18s/epoch - 44ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0034 - accuracy: 0.9987 - val_loss: 3.3114 - val_accuracy: 0.7000 - 18s/epoch - 43ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 6.9044e-04 - accuracy: 0.9997 - val_loss: 6.0656 - val_accuracy: 0.6685 - 17s/epoch - 42ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 3.1899e-04 - accuracy: 0.9999 - val_loss: 5.5339 - val_accuracy: 0.6742 - 17s/epoch - 40ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 4.1808 - val_accuracy: 0.6704 - 16s/epoch - 39ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 0.0032 - accuracy: 0.9991 - val_loss: 4.3104 - val_accuracy: 0.6723 - 16s/epoch - 39ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0027 - accuracy: 0.9991 - val_loss: 4.5653 - val_accuracy: 0.6740 - 17s/epoch - 41ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 4.4239e-04 - accuracy: 0.9998 - val_loss: 5.3593 - val_accuracy: 0.6706 - 16s/epoch - 40ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0024 - accuracy: 0.9994 - val_loss: 5.6754 - val_accuracy: 0.6698 - 17s/epoch - 42ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 7.5328e-04 - accuracy: 0.9996 - val_loss: 6.8957 - val_accuracy: 0.7071 - 19s/epoch - 45ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0018 - accuracy: 0.9994 - val_loss: 4.8577 - val_accuracy: 0.6988 - 18s/epoch - 45ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0016 - accuracy: 0.9993 - val_loss: 6.1993 - val_accuracy: 0.6706 - 18s/epoch - 45ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 4.2114 - val_accuracy: 0.6796 - 19s/epoch - 45ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 4.6236 - val_accuracy: 0.6808 - 19s/epoch - 45ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 5.9959 - val_accuracy: 0.6869 - 18s/epoch - 45ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 4.1021e-04 - accuracy: 0.9998 - val_loss: 6.8957 - val_accuracy: 0.6725 - 18s/epoch - 45ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 6.5823e-04 - accuracy: 0.9998 - val_loss: 7.1745 - val_accuracy: 0.6879 - 18s/epoch - 45ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 9.2422e-04 - accuracy: 0.9998 - val_loss: 6.9664 - val_accuracy: 0.6910 - 19s/epoch - 47ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0061 - accuracy: 0.9980 - val_loss: 3.6331 - val_accuracy: 0.6931 - 18s/epoch - 44ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 1.44149\n",
      "413/413 - 19s - loss: 9.4546e-04 - accuracy: 0.9995 - val_loss: 4.3206 - val_accuracy: 0.6758 - 19s/epoch - 45ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 3.2857e-04 - accuracy: 0.9998 - val_loss: 4.0678 - val_accuracy: 0.7002 - 18s/epoch - 44ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 4.9969e-04 - accuracy: 0.9998 - val_loss: 4.6545 - val_accuracy: 0.6342 - 18s/epoch - 43ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 1.44149\n",
      "413/413 - 18s - loss: 0.0054 - accuracy: 0.9980 - val_loss: 3.5571 - val_accuracy: 0.6750 - 18s/epoch - 44ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0023 - accuracy: 0.9995 - val_loss: 3.8767 - val_accuracy: 0.6888 - 17s/epoch - 42ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 6.7215e-04 - accuracy: 0.9998 - val_loss: 5.6112 - val_accuracy: 0.6744 - 17s/epoch - 42ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 5.6673e-04 - accuracy: 0.9998 - val_loss: 5.8028 - val_accuracy: 0.6675 - 17s/epoch - 42ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 1.44149\n",
      "413/413 - 17s - loss: 0.0034 - accuracy: 0.9989 - val_loss: 4.1682 - val_accuracy: 0.6756 - 17s/epoch - 42ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 1.44149\n",
      "413/413 - 16s - loss: 2.7198e-04 - accuracy: 0.9999 - val_loss: 5.9237 - val_accuracy: 0.6708 - 16s/epoch - 40ms/step\n",
      "179/179 [==============================] - 2s 11ms/step\n",
      "Classification accuracy: 0.500526 \n"
     ]
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9436807  0.05631756]\n",
      " [0.9659392  0.03405956]\n",
      " [0.9671538  0.03284499]\n",
      " ...\n",
      " [0.919286   0.08071029]\n",
      " [0.944395   0.05560271]\n",
      " [0.9511646  0.0488331 ]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[1234 2066]\n",
      " [1635  765]]\n",
      "[35.07 43.01 27.02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for DeepConvNet on New_ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)\n",
    "plot_confusion_matrix(preds_Deep, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for DeepConvNet on New_ICA data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.02840, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 11s - loss: 0.2252 - accuracy: 0.9053 - val_loss: 1.0284 - val_accuracy: 0.6737 - 11s/epoch - 28ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.02840\n",
      "413/413 - 10s - loss: 0.0484 - accuracy: 0.9839 - val_loss: 1.2406 - val_accuracy: 0.7281 - 10s/epoch - 25ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.02840\n",
      "413/413 - 10s - loss: 0.0095 - accuracy: 0.9977 - val_loss: 1.5280 - val_accuracy: 0.7190 - 10s/epoch - 25ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.02840\n",
      "413/413 - 10s - loss: 0.0084 - accuracy: 0.9980 - val_loss: 1.4683 - val_accuracy: 0.6829 - 10s/epoch - 25ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0076 - accuracy: 0.9976 - val_loss: 2.0300 - val_accuracy: 0.7033 - 11s/epoch - 26ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0040 - accuracy: 0.9992 - val_loss: 1.6976 - val_accuracy: 0.6879 - 11s/epoch - 26ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0140 - accuracy: 0.9961 - val_loss: 2.3529 - val_accuracy: 0.6558 - 11s/epoch - 26ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 2.2322 - val_accuracy: 0.6633 - 11s/epoch - 26ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0054 - accuracy: 0.9986 - val_loss: 2.5808 - val_accuracy: 0.6335 - 11s/epoch - 26ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0199 - accuracy: 0.9933 - val_loss: 1.7003 - val_accuracy: 0.7131 - 11s/epoch - 27ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 2.0337 - val_accuracy: 0.6750 - 12s/epoch - 28ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0059 - accuracy: 0.9981 - val_loss: 1.7752 - val_accuracy: 0.7019 - 12s/epoch - 28ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0014 - accuracy: 0.9998 - val_loss: 1.9849 - val_accuracy: 0.6844 - 12s/epoch - 29ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 2.1269 - val_accuracy: 0.6852 - 12s/epoch - 29ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.8842e-04 - accuracy: 0.9998 - val_loss: 2.7543 - val_accuracy: 0.6325 - 12s/epoch - 28ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 2.1849 - val_accuracy: 0.6656 - 11s/epoch - 28ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.1753e-04 - accuracy: 1.0000 - val_loss: 2.4537 - val_accuracy: 0.6606 - 12s/epoch - 28ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.7018e-04 - accuracy: 0.9999 - val_loss: 2.3464 - val_accuracy: 0.6633 - 12s/epoch - 29ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0122 - accuracy: 0.9955 - val_loss: 1.2350 - val_accuracy: 0.6785 - 12s/epoch - 28ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0130 - accuracy: 0.9955 - val_loss: 1.4785 - val_accuracy: 0.7212 - 12s/epoch - 28ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0054 - accuracy: 0.9981 - val_loss: 2.4836 - val_accuracy: 0.6669 - 12s/epoch - 28ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.6681e-04 - accuracy: 1.0000 - val_loss: 2.4545 - val_accuracy: 0.6756 - 12s/epoch - 29ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.7370e-04 - accuracy: 1.0000 - val_loss: 2.7074 - val_accuracy: 0.6558 - 12s/epoch - 29ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0128 - accuracy: 0.9965 - val_loss: 2.2831 - val_accuracy: 0.6827 - 12s/epoch - 29ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 6.2415e-04 - accuracy: 1.0000 - val_loss: 2.4242 - val_accuracy: 0.6802 - 12s/epoch - 28ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0207 - accuracy: 0.9938 - val_loss: 2.7968 - val_accuracy: 0.6377 - 12s/epoch - 28ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0023 - accuracy: 0.9994 - val_loss: 1.9591 - val_accuracy: 0.6806 - 11s/epoch - 28ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.8400e-04 - accuracy: 0.9999 - val_loss: 2.1715 - val_accuracy: 0.6850 - 11s/epoch - 27ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 2.0519 - val_accuracy: 0.6902 - 11s/epoch - 27ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.2436e-04 - accuracy: 0.9999 - val_loss: 2.4782 - val_accuracy: 0.6667 - 11s/epoch - 27ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.0659e-04 - accuracy: 1.0000 - val_loss: 2.2670 - val_accuracy: 0.6929 - 12s/epoch - 28ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.9785e-04 - accuracy: 1.0000 - val_loss: 2.1907 - val_accuracy: 0.6956 - 11s/epoch - 27ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 9.8382e-05 - accuracy: 1.0000 - val_loss: 2.5028 - val_accuracy: 0.6781 - 12s/epoch - 28ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.5249e-04 - accuracy: 1.0000 - val_loss: 3.0697 - val_accuracy: 0.6546 - 12s/epoch - 28ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.6605e-04 - accuracy: 1.0000 - val_loss: 2.4813 - val_accuracy: 0.6781 - 12s/epoch - 28ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 6.7912e-05 - accuracy: 1.0000 - val_loss: 2.9338 - val_accuracy: 0.6629 - 12s/epoch - 28ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.0625e-04 - accuracy: 1.0000 - val_loss: 2.1820 - val_accuracy: 0.7135 - 11s/epoch - 27ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0199 - accuracy: 0.9938 - val_loss: 2.5120 - val_accuracy: 0.6854 - 11s/epoch - 27ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0227 - accuracy: 0.9935 - val_loss: 2.1590 - val_accuracy: 0.6877 - 11s/epoch - 28ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0015 - accuracy: 0.9998 - val_loss: 2.6381 - val_accuracy: 0.6635 - 11s/epoch - 27ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.0439e-04 - accuracy: 1.0000 - val_loss: 2.3256 - val_accuracy: 0.6942 - 11s/epoch - 26ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.5003e-04 - accuracy: 0.9998 - val_loss: 2.9550 - val_accuracy: 0.6579 - 11s/epoch - 27ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.8529e-04 - accuracy: 1.0000 - val_loss: 2.7258 - val_accuracy: 0.6798 - 11s/epoch - 27ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.9796e-04 - accuracy: 0.9999 - val_loss: 2.6116 - val_accuracy: 0.6781 - 11s/epoch - 26ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.2255e-04 - accuracy: 1.0000 - val_loss: 2.6720 - val_accuracy: 0.6719 - 11s/epoch - 28ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.9112e-05 - accuracy: 1.0000 - val_loss: 2.8443 - val_accuracy: 0.6696 - 12s/epoch - 28ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.8792e-05 - accuracy: 1.0000 - val_loss: 2.7152 - val_accuracy: 0.6819 - 11s/epoch - 27ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0110 - accuracy: 0.9967 - val_loss: 1.9083 - val_accuracy: 0.6567 - 11s/epoch - 27ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 2.1509 - val_accuracy: 0.6790 - 12s/epoch - 29ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 5.2657e-04 - accuracy: 0.9999 - val_loss: 2.2130 - val_accuracy: 0.6940 - 12s/epoch - 29ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.9119e-04 - accuracy: 1.0000 - val_loss: 1.9963 - val_accuracy: 0.7052 - 12s/epoch - 29ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.6753e-05 - accuracy: 1.0000 - val_loss: 2.4995 - val_accuracy: 0.6812 - 12s/epoch - 28ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.4224e-05 - accuracy: 1.0000 - val_loss: 2.8109 - val_accuracy: 0.6658 - 12s/epoch - 28ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0014 - accuracy: 0.9998 - val_loss: 1.8514 - val_accuracy: 0.7029 - 11s/epoch - 28ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0106 - accuracy: 0.9967 - val_loss: 2.6609 - val_accuracy: 0.6598 - 11s/epoch - 28ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.7134e-04 - accuracy: 0.9999 - val_loss: 2.5004 - val_accuracy: 0.6792 - 11s/epoch - 27ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.6493e-04 - accuracy: 1.0000 - val_loss: 2.4266 - val_accuracy: 0.6860 - 11s/epoch - 26ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 8.2073e-05 - accuracy: 1.0000 - val_loss: 2.5091 - val_accuracy: 0.6846 - 11s/epoch - 26ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.2179e-05 - accuracy: 1.0000 - val_loss: 2.5631 - val_accuracy: 0.6885 - 11s/epoch - 27ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.8407e-04 - accuracy: 1.0000 - val_loss: 2.1649 - val_accuracy: 0.6950 - 11s/epoch - 26ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0091 - accuracy: 0.9967 - val_loss: 2.3501 - val_accuracy: 0.6496 - 11s/epoch - 27ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0047 - accuracy: 0.9987 - val_loss: 2.5167 - val_accuracy: 0.6758 - 11s/epoch - 27ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.1025e-04 - accuracy: 1.0000 - val_loss: 2.5333 - val_accuracy: 0.6817 - 11s/epoch - 28ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.8388e-04 - accuracy: 1.0000 - val_loss: 2.7957 - val_accuracy: 0.6777 - 12s/epoch - 28ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 6.8956e-05 - accuracy: 1.0000 - val_loss: 2.5210 - val_accuracy: 0.6917 - 12s/epoch - 28ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 5.1159e-05 - accuracy: 1.0000 - val_loss: 2.7232 - val_accuracy: 0.6806 - 12s/epoch - 29ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.9900e-04 - accuracy: 1.0000 - val_loss: 2.7824 - val_accuracy: 0.6729 - 12s/epoch - 29ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 5.9001e-05 - accuracy: 1.0000 - val_loss: 2.6365 - val_accuracy: 0.6833 - 12s/epoch - 29ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.3235e-05 - accuracy: 1.0000 - val_loss: 2.7922 - val_accuracy: 0.6785 - 12s/epoch - 30ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.1320e-05 - accuracy: 1.0000 - val_loss: 2.7001 - val_accuracy: 0.6877 - 12s/epoch - 29ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.1604e-05 - accuracy: 1.0000 - val_loss: 2.7438 - val_accuracy: 0.6933 - 12s/epoch - 29ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.5048e-05 - accuracy: 1.0000 - val_loss: 2.9719 - val_accuracy: 0.6792 - 12s/epoch - 28ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.3822e-05 - accuracy: 1.0000 - val_loss: 3.0112 - val_accuracy: 0.6748 - 12s/epoch - 28ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.1372e-05 - accuracy: 1.0000 - val_loss: 2.9370 - val_accuracy: 0.6829 - 11s/epoch - 27ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.5198e-05 - accuracy: 1.0000 - val_loss: 2.9868 - val_accuracy: 0.6775 - 11s/epoch - 27ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0092 - accuracy: 0.9964 - val_loss: 1.1821 - val_accuracy: 0.7508 - 11s/epoch - 27ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0026 - accuracy: 0.9991 - val_loss: 2.3739 - val_accuracy: 0.6658 - 11s/epoch - 27ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.9002e-04 - accuracy: 0.9999 - val_loss: 2.5869 - val_accuracy: 0.6708 - 11s/epoch - 28ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.0386e-04 - accuracy: 1.0000 - val_loss: 2.4526 - val_accuracy: 0.6762 - 11s/epoch - 26ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.3710e-04 - accuracy: 0.9999 - val_loss: 2.1698 - val_accuracy: 0.6898 - 11s/epoch - 26ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0034 - accuracy: 0.9991 - val_loss: 2.2280 - val_accuracy: 0.6796 - 11s/epoch - 26ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.0833e-04 - accuracy: 1.0000 - val_loss: 2.5983 - val_accuracy: 0.6706 - 11s/epoch - 26ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0053 - accuracy: 0.9982 - val_loss: 1.2282 - val_accuracy: 0.7281 - 11s/epoch - 26ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0023 - accuracy: 0.9994 - val_loss: 2.2756 - val_accuracy: 0.7244 - 11s/epoch - 26ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.2536e-04 - accuracy: 1.0000 - val_loss: 2.3291 - val_accuracy: 0.7196 - 11s/epoch - 26ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.02840\n",
      "413/413 - 10s - loss: 5.6817e-05 - accuracy: 1.0000 - val_loss: 2.5370 - val_accuracy: 0.6900 - 10s/epoch - 25ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.7957e-05 - accuracy: 1.0000 - val_loss: 2.6551 - val_accuracy: 0.6854 - 11s/epoch - 26ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.7127e-04 - accuracy: 0.9999 - val_loss: 1.1657 - val_accuracy: 0.6994 - 11s/epoch - 27ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0074 - accuracy: 0.9973 - val_loss: 2.5059 - val_accuracy: 0.6669 - 12s/epoch - 28ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.02840\n",
      "413/413 - 13s - loss: 3.0314e-04 - accuracy: 0.9999 - val_loss: 2.3950 - val_accuracy: 0.6725 - 13s/epoch - 31ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1438e-04 - accuracy: 1.0000 - val_loss: 2.7166 - val_accuracy: 0.6723 - 12s/epoch - 29ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 5.5476e-05 - accuracy: 1.0000 - val_loss: 2.5526 - val_accuracy: 0.6769 - 12s/epoch - 29ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 6.1996e-05 - accuracy: 1.0000 - val_loss: 2.5440 - val_accuracy: 0.6923 - 12s/epoch - 28ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.7955e-05 - accuracy: 1.0000 - val_loss: 2.4759 - val_accuracy: 0.6875 - 12s/epoch - 29ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.0950e-05 - accuracy: 1.0000 - val_loss: 2.6638 - val_accuracy: 0.6796 - 12s/epoch - 29ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.1134e-05 - accuracy: 1.0000 - val_loss: 2.7693 - val_accuracy: 0.6829 - 12s/epoch - 28ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0108 - accuracy: 0.9965 - val_loss: 1.4963 - val_accuracy: 0.7119 - 12s/epoch - 29ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0026 - accuracy: 0.9994 - val_loss: 2.3193 - val_accuracy: 0.6896 - 12s/epoch - 29ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.0178e-04 - accuracy: 0.9998 - val_loss: 2.4116 - val_accuracy: 0.6906 - 12s/epoch - 29ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.4369e-04 - accuracy: 1.0000 - val_loss: 2.5627 - val_accuracy: 0.6773 - 12s/epoch - 29ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.7746e-05 - accuracy: 1.0000 - val_loss: 2.8735 - val_accuracy: 0.6758 - 12s/epoch - 28ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.9582e-05 - accuracy: 1.0000 - val_loss: 2.5903 - val_accuracy: 0.6812 - 12s/epoch - 29ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.2705e-05 - accuracy: 1.0000 - val_loss: 2.6037 - val_accuracy: 0.6835 - 12s/epoch - 29ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.7079e-05 - accuracy: 1.0000 - val_loss: 2.6476 - val_accuracy: 0.6802 - 12s/epoch - 28ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.1408e-05 - accuracy: 1.0000 - val_loss: 2.6252 - val_accuracy: 0.6785 - 12s/epoch - 29ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.7640e-05 - accuracy: 1.0000 - val_loss: 2.6321 - val_accuracy: 0.6802 - 12s/epoch - 28ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.6245e-05 - accuracy: 1.0000 - val_loss: 2.8205 - val_accuracy: 0.6754 - 11s/epoch - 28ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.7842e-05 - accuracy: 1.0000 - val_loss: 2.8338 - val_accuracy: 0.6783 - 12s/epoch - 28ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.9301e-06 - accuracy: 1.0000 - val_loss: 2.8731 - val_accuracy: 0.6773 - 11s/epoch - 27ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.2487e-05 - accuracy: 1.0000 - val_loss: 2.7089 - val_accuracy: 0.6827 - 11s/epoch - 27ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.0191e-05 - accuracy: 1.0000 - val_loss: 1.6694 - val_accuracy: 0.7271 - 11s/epoch - 27ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 2.0170 - val_accuracy: 0.7235 - 11s/epoch - 27ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.6435e-04 - accuracy: 1.0000 - val_loss: 2.4583 - val_accuracy: 0.7148 - 11s/epoch - 27ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 0.0015 - accuracy: 0.9995 - val_loss: 2.3750 - val_accuracy: 0.7240 - 14s/epoch - 33ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 1.02840\n",
      "413/413 - 13s - loss: 2.6955e-04 - accuracy: 0.9999 - val_loss: 2.1011 - val_accuracy: 0.6940 - 13s/epoch - 31ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 5.6851e-05 - accuracy: 1.0000 - val_loss: 2.2006 - val_accuracy: 0.6935 - 12s/epoch - 29ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.1088e-05 - accuracy: 1.0000 - val_loss: 2.6427 - val_accuracy: 0.6767 - 12s/epoch - 28ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.2833e-05 - accuracy: 1.0000 - val_loss: 2.2454 - val_accuracy: 0.6979 - 12s/epoch - 29ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.5285e-05 - accuracy: 1.0000 - val_loss: 1.9951 - val_accuracy: 0.7219 - 12s/epoch - 29ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0065 - accuracy: 0.9983 - val_loss: 2.1618 - val_accuracy: 0.6971 - 12s/epoch - 29ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.6328e-04 - accuracy: 1.0000 - val_loss: 2.2916 - val_accuracy: 0.6921 - 11s/epoch - 28ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.6524e-04 - accuracy: 1.0000 - val_loss: 2.8539 - val_accuracy: 0.6790 - 11s/epoch - 27ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.1993e-05 - accuracy: 1.0000 - val_loss: 2.3797 - val_accuracy: 0.6948 - 11s/epoch - 27ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0037 - accuracy: 0.9986 - val_loss: 1.7581 - val_accuracy: 0.6954 - 12s/epoch - 28ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0026 - accuracy: 0.9991 - val_loss: 1.5727 - val_accuracy: 0.7231 - 12s/epoch - 29ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 2.5089 - val_accuracy: 0.6758 - 12s/epoch - 29ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 2.1651 - val_accuracy: 0.6842 - 12s/epoch - 28ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.6516e-04 - accuracy: 1.0000 - val_loss: 2.3535 - val_accuracy: 0.6890 - 12s/epoch - 29ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.4111e-05 - accuracy: 1.0000 - val_loss: 2.4340 - val_accuracy: 0.6888 - 12s/epoch - 28ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.9975e-04 - accuracy: 0.9999 - val_loss: 2.6886 - val_accuracy: 0.6902 - 12s/epoch - 29ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.5395e-05 - accuracy: 1.0000 - val_loss: 2.0974 - val_accuracy: 0.7248 - 11s/epoch - 28ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 3.0044e-05 - accuracy: 1.0000 - val_loss: 2.4459 - val_accuracy: 0.7027 - 11s/epoch - 27ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0032 - accuracy: 0.9988 - val_loss: 2.1480 - val_accuracy: 0.7000 - 11s/epoch - 26ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.0354e-04 - accuracy: 1.0000 - val_loss: 2.2491 - val_accuracy: 0.7031 - 11s/epoch - 27ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.5314e-05 - accuracy: 1.0000 - val_loss: 2.3302 - val_accuracy: 0.6975 - 11s/epoch - 27ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.9939e-05 - accuracy: 1.0000 - val_loss: 2.4493 - val_accuracy: 0.6935 - 11s/epoch - 27ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0048 - accuracy: 0.9988 - val_loss: 2.0877 - val_accuracy: 0.6815 - 11s/epoch - 26ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 2.3854 - val_accuracy: 0.7192 - 11s/epoch - 26ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.0777e-04 - accuracy: 1.0000 - val_loss: 2.5686 - val_accuracy: 0.6904 - 11s/epoch - 26ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.6686e-05 - accuracy: 1.0000 - val_loss: 2.5995 - val_accuracy: 0.6890 - 12s/epoch - 28ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.4637e-05 - accuracy: 1.0000 - val_loss: 2.6786 - val_accuracy: 0.6906 - 11s/epoch - 27ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.0693e-05 - accuracy: 1.0000 - val_loss: 2.6565 - val_accuracy: 0.6906 - 11s/epoch - 27ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.0055e-05 - accuracy: 1.0000 - val_loss: 2.5934 - val_accuracy: 0.6910 - 12s/epoch - 29ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.9960e-05 - accuracy: 1.0000 - val_loss: 2.8119 - val_accuracy: 0.6902 - 12s/epoch - 29ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0035 - accuracy: 0.9988 - val_loss: 2.4886 - val_accuracy: 0.6873 - 11s/epoch - 27ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0034 - accuracy: 0.9988 - val_loss: 2.4725 - val_accuracy: 0.6821 - 11s/epoch - 27ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.2441e-04 - accuracy: 1.0000 - val_loss: 2.9402 - val_accuracy: 0.6825 - 11s/epoch - 26ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 8.2338e-04 - accuracy: 0.9998 - val_loss: 3.1967 - val_accuracy: 0.6392 - 11s/epoch - 27ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.2376e-04 - accuracy: 1.0000 - val_loss: 2.8396 - val_accuracy: 0.6683 - 11s/epoch - 28ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.4097e-05 - accuracy: 1.0000 - val_loss: 2.7683 - val_accuracy: 0.6748 - 11s/epoch - 28ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.0081e-05 - accuracy: 1.0000 - val_loss: 3.1026 - val_accuracy: 0.6706 - 12s/epoch - 29ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.4500e-05 - accuracy: 1.0000 - val_loss: 2.9682 - val_accuracy: 0.6735 - 12s/epoch - 29ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.4304e-05 - accuracy: 1.0000 - val_loss: 3.3242 - val_accuracy: 0.6679 - 12s/epoch - 29ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.7965e-05 - accuracy: 1.0000 - val_loss: 2.7730 - val_accuracy: 0.6871 - 12s/epoch - 29ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.7192e-05 - accuracy: 1.0000 - val_loss: 3.1333 - val_accuracy: 0.6773 - 12s/epoch - 29ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1695e-05 - accuracy: 1.0000 - val_loss: 2.9522 - val_accuracy: 0.6812 - 12s/epoch - 29ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 9.6026e-06 - accuracy: 1.0000 - val_loss: 3.0008 - val_accuracy: 0.6806 - 12s/epoch - 29ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.0722e-06 - accuracy: 1.0000 - val_loss: 2.9880 - val_accuracy: 0.6812 - 11s/epoch - 28ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.0210e-05 - accuracy: 1.0000 - val_loss: 3.1099 - val_accuracy: 0.6783 - 12s/epoch - 28ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.1697e-06 - accuracy: 1.0000 - val_loss: 3.3209 - val_accuracy: 0.6706 - 11s/epoch - 28ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.5127e-06 - accuracy: 1.0000 - val_loss: 2.7452 - val_accuracy: 0.6900 - 12s/epoch - 29ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0076 - accuracy: 0.9970 - val_loss: 2.4006 - val_accuracy: 0.6873 - 12s/epoch - 29ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.9678e-04 - accuracy: 0.9999 - val_loss: 2.1758 - val_accuracy: 0.6854 - 12s/epoch - 28ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.9716e-04 - accuracy: 0.9999 - val_loss: 2.2914 - val_accuracy: 0.6831 - 11s/epoch - 28ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.3412e-05 - accuracy: 1.0000 - val_loss: 2.5420 - val_accuracy: 0.6790 - 11s/epoch - 28ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.1653e-05 - accuracy: 1.0000 - val_loss: 2.5337 - val_accuracy: 0.6808 - 12s/epoch - 28ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.0917e-05 - accuracy: 1.0000 - val_loss: 2.4899 - val_accuracy: 0.6856 - 12s/epoch - 29ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.0488e-05 - accuracy: 1.0000 - val_loss: 2.4751 - val_accuracy: 0.6867 - 11s/epoch - 27ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.5246e-05 - accuracy: 1.0000 - val_loss: 2.6037 - val_accuracy: 0.6842 - 11s/epoch - 27ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.1359e-05 - accuracy: 1.0000 - val_loss: 2.5120 - val_accuracy: 0.6883 - 11s/epoch - 27ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.9173e-06 - accuracy: 1.0000 - val_loss: 2.6884 - val_accuracy: 0.6829 - 11s/epoch - 28ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.0088e-05 - accuracy: 1.0000 - val_loss: 2.8599 - val_accuracy: 0.6794 - 12s/epoch - 28ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.3188e-05 - accuracy: 1.0000 - val_loss: 3.1571 - val_accuracy: 0.6729 - 11s/epoch - 27ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.8393e-06 - accuracy: 1.0000 - val_loss: 3.0276 - val_accuracy: 0.6775 - 12s/epoch - 28ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.5111e-05 - accuracy: 1.0000 - val_loss: 2.7856 - val_accuracy: 0.6860 - 11s/epoch - 28ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 6.8800e-06 - accuracy: 1.0000 - val_loss: 2.9722 - val_accuracy: 0.6802 - 11s/epoch - 28ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.4338e-06 - accuracy: 1.0000 - val_loss: 3.1314 - val_accuracy: 0.6762 - 11s/epoch - 27ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0040 - accuracy: 0.9983 - val_loss: 2.6695 - val_accuracy: 0.6898 - 11s/epoch - 27ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 2.1334 - val_accuracy: 0.6906 - 11s/epoch - 28ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.4318e-04 - accuracy: 1.0000 - val_loss: 1.8496 - val_accuracy: 0.7335 - 11s/epoch - 27ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.3452e-04 - accuracy: 0.9999 - val_loss: 2.1467 - val_accuracy: 0.6844 - 11s/epoch - 26ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 3.2819e-05 - accuracy: 1.0000 - val_loss: 2.3267 - val_accuracy: 0.6796 - 11s/epoch - 25ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.4431e-05 - accuracy: 1.0000 - val_loss: 2.2487 - val_accuracy: 0.6883 - 11s/epoch - 26ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.5898e-05 - accuracy: 1.0000 - val_loss: 2.4263 - val_accuracy: 0.6779 - 11s/epoch - 27ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.9030e-05 - accuracy: 1.0000 - val_loss: 2.3156 - val_accuracy: 0.6806 - 11s/epoch - 27ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.0510e-05 - accuracy: 1.0000 - val_loss: 2.5398 - val_accuracy: 0.6746 - 12s/epoch - 29ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.2149e-05 - accuracy: 1.0000 - val_loss: 2.5072 - val_accuracy: 0.6754 - 12s/epoch - 29ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1392e-05 - accuracy: 1.0000 - val_loss: 2.4660 - val_accuracy: 0.6775 - 12s/epoch - 29ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1278e-05 - accuracy: 1.0000 - val_loss: 2.9851 - val_accuracy: 0.6690 - 12s/epoch - 29ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 9.2887e-06 - accuracy: 1.0000 - val_loss: 2.7838 - val_accuracy: 0.6710 - 12s/epoch - 29ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0079 - accuracy: 0.9977 - val_loss: 1.8719 - val_accuracy: 0.6821 - 12s/epoch - 29ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0055 - accuracy: 0.9986 - val_loss: 2.1688 - val_accuracy: 0.7046 - 12s/epoch - 29ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 1.02840\n",
      "413/413 - 13s - loss: 1.7484e-04 - accuracy: 1.0000 - val_loss: 2.4669 - val_accuracy: 0.6877 - 13s/epoch - 30ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.1568e-05 - accuracy: 1.0000 - val_loss: 2.4420 - val_accuracy: 0.6775 - 12s/epoch - 29ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.3724e-05 - accuracy: 1.0000 - val_loss: 2.5493 - val_accuracy: 0.6860 - 12s/epoch - 29ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 1.02840\n",
      "413/413 - 13s - loss: 3.4962e-05 - accuracy: 1.0000 - val_loss: 2.5800 - val_accuracy: 0.6904 - 13s/epoch - 31ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 6.8335e-05 - accuracy: 1.0000 - val_loss: 2.6423 - val_accuracy: 0.6965 - 14s/epoch - 33ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 6.5468e-05 - accuracy: 1.0000 - val_loss: 2.1548 - val_accuracy: 0.7144 - 14s/epoch - 33ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 4.0901e-05 - accuracy: 1.0000 - val_loss: 2.5235 - val_accuracy: 0.6983 - 14s/epoch - 34ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 2.8496e-05 - accuracy: 1.0000 - val_loss: 2.7308 - val_accuracy: 0.6888 - 14s/epoch - 34ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 0.0074 - accuracy: 0.9975 - val_loss: 1.5081 - val_accuracy: 0.7479 - 14s/epoch - 33ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 1.02840\n",
      "413/413 - 16s - loss: 0.0021 - accuracy: 0.9997 - val_loss: 2.5437 - val_accuracy: 0.6675 - 16s/epoch - 38ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 1.0290e-04 - accuracy: 1.0000 - val_loss: 2.5281 - val_accuracy: 0.6681 - 14s/epoch - 34ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 1.02840\n",
      "413/413 - 13s - loss: 7.2112e-05 - accuracy: 1.0000 - val_loss: 2.3550 - val_accuracy: 0.6731 - 13s/epoch - 33ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 1.02840\n",
      "413/413 - 14s - loss: 0.0035 - accuracy: 0.9989 - val_loss: 2.5299 - val_accuracy: 0.6740 - 14s/epoch - 33ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 1.02840\n",
      "413/413 - 13s - loss: 2.3624e-04 - accuracy: 1.0000 - val_loss: 2.7301 - val_accuracy: 0.6679 - 13s/epoch - 30ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.8068e-05 - accuracy: 1.0000 - val_loss: 2.7555 - val_accuracy: 0.6673 - 12s/epoch - 30ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.5292e-04 - accuracy: 1.0000 - val_loss: 2.4277 - val_accuracy: 0.6837 - 12s/epoch - 29ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.2801e-05 - accuracy: 1.0000 - val_loss: 2.5867 - val_accuracy: 0.6773 - 12s/epoch - 29ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.0879e-05 - accuracy: 1.0000 - val_loss: 3.0186 - val_accuracy: 0.6677 - 12s/epoch - 29ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.5802e-05 - accuracy: 1.0000 - val_loss: 2.8298 - val_accuracy: 0.6687 - 12s/epoch - 28ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.3927e-05 - accuracy: 1.0000 - val_loss: 2.7509 - val_accuracy: 0.6700 - 11s/epoch - 27ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.5893e-06 - accuracy: 1.0000 - val_loss: 3.0563 - val_accuracy: 0.6646 - 11s/epoch - 27ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.5439e-06 - accuracy: 1.0000 - val_loss: 2.9027 - val_accuracy: 0.6675 - 11s/epoch - 26ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.9370e-06 - accuracy: 1.0000 - val_loss: 2.9457 - val_accuracy: 0.6706 - 11s/epoch - 26ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.2260e-05 - accuracy: 1.0000 - val_loss: 3.1860 - val_accuracy: 0.6746 - 11s/epoch - 26ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0071 - accuracy: 0.9977 - val_loss: 2.9021 - val_accuracy: 0.6635 - 11s/epoch - 26ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 6.5844e-04 - accuracy: 0.9999 - val_loss: 2.1368 - val_accuracy: 0.6873 - 11s/epoch - 26ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.3620e-05 - accuracy: 1.0000 - val_loss: 2.4858 - val_accuracy: 0.6904 - 11s/epoch - 25ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.4447e-04 - accuracy: 1.0000 - val_loss: 2.4761 - val_accuracy: 0.7073 - 11s/epoch - 26ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 9.6322e-05 - accuracy: 1.0000 - val_loss: 2.8630 - val_accuracy: 0.6781 - 11s/epoch - 26ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.0582e-05 - accuracy: 1.0000 - val_loss: 3.1140 - val_accuracy: 0.6750 - 11s/epoch - 26ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.9203e-05 - accuracy: 1.0000 - val_loss: 2.9980 - val_accuracy: 0.6785 - 11s/epoch - 26ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.1513e-05 - accuracy: 1.0000 - val_loss: 3.1286 - val_accuracy: 0.6750 - 11s/epoch - 26ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 1.3205 - val_accuracy: 0.6521 - 12s/epoch - 28ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0065 - accuracy: 0.9980 - val_loss: 2.8368 - val_accuracy: 0.6694 - 12s/epoch - 28ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1715e-04 - accuracy: 1.0000 - val_loss: 2.8169 - val_accuracy: 0.6679 - 12s/epoch - 29ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.0265e-05 - accuracy: 1.0000 - val_loss: 2.5569 - val_accuracy: 0.6798 - 12s/epoch - 29ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1202e-04 - accuracy: 1.0000 - val_loss: 3.0818 - val_accuracy: 0.6675 - 12s/epoch - 29ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.7138e-05 - accuracy: 1.0000 - val_loss: 2.6598 - val_accuracy: 0.6775 - 12s/epoch - 29ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.0914e-05 - accuracy: 1.0000 - val_loss: 2.2867 - val_accuracy: 0.6890 - 12s/epoch - 29ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.2658e-05 - accuracy: 1.0000 - val_loss: 2.6589 - val_accuracy: 0.6790 - 12s/epoch - 29ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.7465e-05 - accuracy: 1.0000 - val_loss: 2.8336 - val_accuracy: 0.6737 - 12s/epoch - 29ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.6153e-05 - accuracy: 1.0000 - val_loss: 2.3105 - val_accuracy: 0.6888 - 12s/epoch - 29ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.5024e-05 - accuracy: 1.0000 - val_loss: 2.7420 - val_accuracy: 0.6817 - 12s/epoch - 29ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.2273e-05 - accuracy: 1.0000 - val_loss: 2.6998 - val_accuracy: 0.6848 - 12s/epoch - 29ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.0450e-05 - accuracy: 1.0000 - val_loss: 2.6994 - val_accuracy: 0.6871 - 12s/epoch - 29ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 6.1750e-06 - accuracy: 1.0000 - val_loss: 2.7803 - val_accuracy: 0.6840 - 12s/epoch - 29ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 6.5660e-06 - accuracy: 1.0000 - val_loss: 2.8544 - val_accuracy: 0.6806 - 12s/epoch - 29ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.8641e-06 - accuracy: 1.0000 - val_loss: 3.0684 - val_accuracy: 0.6752 - 12s/epoch - 30ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.5217e-05 - accuracy: 1.0000 - val_loss: 2.9074 - val_accuracy: 0.6823 - 12s/epoch - 29ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.7077e-06 - accuracy: 1.0000 - val_loss: 2.9408 - val_accuracy: 0.6800 - 12s/epoch - 29ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1555e-05 - accuracy: 1.0000 - val_loss: 2.8822 - val_accuracy: 0.6831 - 12s/epoch - 30ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.5077e-06 - accuracy: 1.0000 - val_loss: 2.7850 - val_accuracy: 0.6844 - 12s/epoch - 30ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0070 - accuracy: 0.9977 - val_loss: 1.8467 - val_accuracy: 0.6875 - 12s/epoch - 30ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0026 - accuracy: 0.9991 - val_loss: 1.6124 - val_accuracy: 0.6833 - 12s/epoch - 28ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 5.7953e-04 - accuracy: 0.9997 - val_loss: 2.4498 - val_accuracy: 0.6685 - 12s/epoch - 29ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.3252e-04 - accuracy: 1.0000 - val_loss: 2.5866 - val_accuracy: 0.6737 - 12s/epoch - 29ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.1837e-05 - accuracy: 1.0000 - val_loss: 2.5461 - val_accuracy: 0.6858 - 12s/epoch - 29ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.2354e-05 - accuracy: 1.0000 - val_loss: 2.6329 - val_accuracy: 0.6767 - 11s/epoch - 27ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.0290e-05 - accuracy: 1.0000 - val_loss: 2.2818 - val_accuracy: 0.6792 - 12s/epoch - 29ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.9338e-05 - accuracy: 1.0000 - val_loss: 2.6063 - val_accuracy: 0.6754 - 12s/epoch - 29ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.4516e-05 - accuracy: 1.0000 - val_loss: 2.6965 - val_accuracy: 0.6744 - 12s/epoch - 29ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0045 - accuracy: 0.9983 - val_loss: 2.1147 - val_accuracy: 0.6931 - 12s/epoch - 29ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.3663e-04 - accuracy: 0.9999 - val_loss: 2.8176 - val_accuracy: 0.6769 - 12s/epoch - 28ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.5681e-04 - accuracy: 1.0000 - val_loss: 2.6978 - val_accuracy: 0.6800 - 12s/epoch - 29ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.2210e-04 - accuracy: 0.9998 - val_loss: 2.1318 - val_accuracy: 0.6833 - 11s/epoch - 28ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 6.9580e-05 - accuracy: 1.0000 - val_loss: 2.4722 - val_accuracy: 0.6804 - 11s/epoch - 28ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 8.2698e-05 - accuracy: 1.0000 - val_loss: 2.9756 - val_accuracy: 0.6810 - 11s/epoch - 27ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 2.4894e-05 - accuracy: 1.0000 - val_loss: 2.4130 - val_accuracy: 0.6842 - 11s/epoch - 28ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.3040e-05 - accuracy: 1.0000 - val_loss: 2.3942 - val_accuracy: 0.6846 - 11s/epoch - 26ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.3100e-05 - accuracy: 1.0000 - val_loss: 2.4987 - val_accuracy: 0.6831 - 11s/epoch - 26ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 1.02840\n",
      "413/413 - 10s - loss: 1.4317e-05 - accuracy: 1.0000 - val_loss: 2.8007 - val_accuracy: 0.6800 - 10s/epoch - 25ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.3062e-05 - accuracy: 1.0000 - val_loss: 2.3168 - val_accuracy: 0.6869 - 11s/epoch - 25ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 1.02840\n",
      "413/413 - 10s - loss: 0.0063 - accuracy: 0.9980 - val_loss: 2.3864 - val_accuracy: 0.7058 - 10s/epoch - 25ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 1.02840\n",
      "413/413 - 10s - loss: 2.5400e-04 - accuracy: 1.0000 - val_loss: 2.6421 - val_accuracy: 0.6687 - 10s/epoch - 25ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 5.0280e-05 - accuracy: 1.0000 - val_loss: 2.5216 - val_accuracy: 0.6715 - 11s/epoch - 25ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.3403e-05 - accuracy: 1.0000 - val_loss: 2.5658 - val_accuracy: 0.6725 - 11s/epoch - 26ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 8.6743e-04 - accuracy: 0.9997 - val_loss: 2.2351 - val_accuracy: 0.6837 - 11s/epoch - 26ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 8.3678e-05 - accuracy: 1.0000 - val_loss: 2.3534 - val_accuracy: 0.6837 - 11s/epoch - 27ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 3.7527e-05 - accuracy: 1.0000 - val_loss: 2.3058 - val_accuracy: 0.6817 - 11s/epoch - 27ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0037 - accuracy: 0.9992 - val_loss: 2.4773 - val_accuracy: 0.6800 - 12s/epoch - 29ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0039 - accuracy: 0.9986 - val_loss: 2.5117 - val_accuracy: 0.6698 - 12s/epoch - 29ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.2877e-04 - accuracy: 1.0000 - val_loss: 2.0516 - val_accuracy: 0.6821 - 12s/epoch - 29ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.7647e-04 - accuracy: 0.9997 - val_loss: 2.6736 - val_accuracy: 0.6762 - 12s/epoch - 29ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.5779e-04 - accuracy: 1.0000 - val_loss: 2.3514 - val_accuracy: 0.6804 - 12s/epoch - 29ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 4.1184e-05 - accuracy: 1.0000 - val_loss: 2.1925 - val_accuracy: 0.6837 - 12s/epoch - 29ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 5.7790e-05 - accuracy: 1.0000 - val_loss: 2.2043 - val_accuracy: 0.6862 - 12s/epoch - 29ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 3.8638e-05 - accuracy: 1.0000 - val_loss: 2.6261 - val_accuracy: 0.6810 - 12s/epoch - 28ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.6691e-05 - accuracy: 1.0000 - val_loss: 2.5621 - val_accuracy: 0.6815 - 12s/epoch - 29ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.3724e-05 - accuracy: 1.0000 - val_loss: 2.4885 - val_accuracy: 0.6837 - 12s/epoch - 29ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.0521e-05 - accuracy: 1.0000 - val_loss: 2.6711 - val_accuracy: 0.6800 - 12s/epoch - 29ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 9.0747e-06 - accuracy: 1.0000 - val_loss: 2.8108 - val_accuracy: 0.6806 - 12s/epoch - 29ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.4828e-06 - accuracy: 1.0000 - val_loss: 3.0413 - val_accuracy: 0.6779 - 12s/epoch - 29ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 0.0098 - accuracy: 0.9965 - val_loss: 1.8277 - val_accuracy: 0.6975 - 12s/epoch - 29ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 8.1600e-04 - accuracy: 0.9998 - val_loss: 2.2232 - val_accuracy: 0.7229 - 12s/epoch - 29ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 9.2377e-05 - accuracy: 1.0000 - val_loss: 2.4799 - val_accuracy: 0.6973 - 12s/epoch - 29ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.1390e-05 - accuracy: 1.0000 - val_loss: 2.4618 - val_accuracy: 0.7127 - 12s/epoch - 28ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 3.2871e-05 - accuracy: 1.0000 - val_loss: 2.4733 - val_accuracy: 0.7190 - 11s/epoch - 27ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.1686e-04 - accuracy: 1.0000 - val_loss: 2.2423 - val_accuracy: 0.7010 - 12s/epoch - 28ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 7.4729e-05 - accuracy: 1.0000 - val_loss: 2.5629 - val_accuracy: 0.7152 - 12s/epoch - 29ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 2.8844e-05 - accuracy: 1.0000 - val_loss: 2.8257 - val_accuracy: 0.6938 - 12s/epoch - 28ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 1.8169e-05 - accuracy: 1.0000 - val_loss: 2.6629 - val_accuracy: 0.7088 - 12s/epoch - 28ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 1.1860e-05 - accuracy: 1.0000 - val_loss: 2.6046 - val_accuracy: 0.7002 - 11s/epoch - 27ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 1.02840\n",
      "413/413 - 13s - loss: 1.5122e-05 - accuracy: 1.0000 - val_loss: 2.7390 - val_accuracy: 0.6990 - 13s/epoch - 31ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 1.02840\n",
      "413/413 - 12s - loss: 9.0130e-06 - accuracy: 1.0000 - val_loss: 2.7677 - val_accuracy: 0.6994 - 12s/epoch - 30ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0055 - accuracy: 0.9989 - val_loss: 2.0405 - val_accuracy: 0.7092 - 11s/epoch - 28ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 0.0032 - accuracy: 0.9993 - val_loss: 1.8658 - val_accuracy: 0.6710 - 11s/epoch - 27ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 4.4897e-04 - accuracy: 1.0000 - val_loss: 2.5114 - val_accuracy: 0.6658 - 11s/epoch - 27ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 1.02840\n",
      "413/413 - 11s - loss: 7.5098e-05 - accuracy: 1.0000 - val_loss: 2.2593 - val_accuracy: 0.6706 - 11s/epoch - 27ms/step\n",
      "179/179 [==============================] - 2s 8ms/step\n",
      "Classification accuracy: 0.529114 \n"
     ]
    }
   ],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7731253  0.13813083]\n",
      " [0.7301787  0.11612521]\n",
      " [0.7701354  0.0596882 ]\n",
      " ...\n",
      " [0.9902281  0.01274396]\n",
      " [0.9435766  0.08014703]\n",
      " [0.94646096 0.04191666]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 1. 1. ... 0. 0. 0.]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[2199 1101]\n",
      " [1702  698]]\n",
      "[50.82 56.37 38.8 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Confusion matrix for ShallowConvNet on ICA data'}, xlabel='Predicted label', ylabel='True label'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)\n",
    "\n",
    "plot_confusion_matrix(preds_Shallow, test_labels, ['Non-Stressed', 'Stressed'], title = 'Confusion matrix for ShallowConvNet on ICA data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6df28382c6e70c1c1d3c02fa7b17b6f3b6fcf9f5d22d2410beebd122bfaf45e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
