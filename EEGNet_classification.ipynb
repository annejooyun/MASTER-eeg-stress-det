{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from load_data import load_data\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification\n",
    "import utils.variables as v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P010_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_002\n",
      "ERROR:root:1) Failed to read data for recording P020_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P023_S002_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs: \n",
      " ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n",
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n",
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n",
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n",
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n",
      "(44, 8, 38400)\n",
      "(13200, 8, 128)\n",
      "(13200, 1)\n",
      "(5700, 8, 128)\n",
      "(5700, 1)\n",
      "(4800, 8, 128)\n",
      "(4800, 1)\n",
      "Shape of train data set: (13200, 8, 128)\n",
      "Shape of train labels set: (13200, 1)\n",
      "Shape of validation data set: (4800, 8, 128)\n",
      "Shape of validation labels set: (4800, 1)\n",
      "Shape of test data set: (5700, 8, 128)\n",
      "Shape of test labels set: (5700, 1)\n"
     ]
    }
   ],
   "source": [
    "data_type = 'new_ica'\n",
    "label_type = 'stai'\n",
    "\n",
    "train_data, test_data, val_data, train_labels, test_labels, val_labels = load_data(data_type, label_type, epoched = True, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.67345, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 7s - loss: 0.6851 - accuracy: 0.5674 - val_loss: 0.6734 - val_accuracy: 0.6875 - 7s/epoch - 33ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 0.67345 to 0.66480, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 5s - loss: 0.6849 - accuracy: 0.5681 - val_loss: 0.6648 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.66480\n",
      "207/207 - 5s - loss: 0.6845 - accuracy: 0.5682 - val_loss: 0.6894 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 0.66480 to 0.64947, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 5s - loss: 0.6844 - accuracy: 0.5682 - val_loss: 0.6495 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 0.64947 to 0.63064, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 5s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6306 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6440 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6634 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6800 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6491 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6760 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6607 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6642 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.63064\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6562 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.63064 to 0.62400, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6240 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.62400\n",
      "207/207 - 7s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6523 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6495 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6485 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6679 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6416 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6861 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.62400\n",
      "207/207 - 7s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6640 - val_accuracy: 0.6875 - 7s/epoch - 31ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6843 - accuracy: 0.5680 - val_loss: 0.6393 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.9264 - val_accuracy: 0.3125 - 6s/epoch - 31ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6317 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6473 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5680 - val_loss: 0.6415 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6434 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6734 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6469 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6501 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6442 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6519 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6307 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6388 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5681 - val_loss: 0.6657 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6357 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6415 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.62400\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6468 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.62400\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6441 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6262 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6405 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6679 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6515 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6402 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6386 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6367 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6304 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6526 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6639 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6403 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.62400\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6524 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.62400\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6434 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.62400\n",
      "207/207 - 12s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6553 - val_accuracy: 0.6875 - 12s/epoch - 57ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6842 - accuracy: 0.5682 - val_loss: 0.6568 - val_accuracy: 0.6875 - 9s/epoch - 44ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6555 - val_accuracy: 0.6875 - 9s/epoch - 45ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6521 - val_accuracy: 0.6875 - 9s/epoch - 43ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 9s/epoch - 44ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6496 - val_accuracy: 0.6875 - 9s/epoch - 44ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.62400\n",
      "207/207 - 10s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6574 - val_accuracy: 0.6875 - 10s/epoch - 47ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6814 - val_accuracy: 0.6875 - 9s/epoch - 43ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6673 - val_accuracy: 0.6875 - 9s/epoch - 45ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6529 - val_accuracy: 0.6875 - 9s/epoch - 43ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6930 - val_accuracy: 0.6700 - 9s/epoch - 44ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6449 - val_accuracy: 0.6875 - 9s/epoch - 44ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6663 - val_accuracy: 0.6875 - 9s/epoch - 45ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6564 - val_accuracy: 0.6875 - 9s/epoch - 45ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6556 - val_accuracy: 0.6875 - 9s/epoch - 45ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.62400\n",
      "207/207 - 9s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6435 - val_accuracy: 0.6875 - 9s/epoch - 45ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss improved from 0.62400 to 0.62136, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 9s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6214 - val_accuracy: 0.6875 - 9s/epoch - 44ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.62136\n",
      "207/207 - 10s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6263 - val_accuracy: 0.6875 - 10s/epoch - 51ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.62136\n",
      "207/207 - 11s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6396 - val_accuracy: 0.6875 - 11s/epoch - 52ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.62136\n",
      "207/207 - 10s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6380 - val_accuracy: 0.6875 - 10s/epoch - 49ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.62136\n",
      "207/207 - 8s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6577 - val_accuracy: 0.6875 - 8s/epoch - 41ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6838 - accuracy: 0.5683 - val_loss: 0.6426 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6838 - accuracy: 0.5683 - val_loss: 0.6754 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6621 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6484 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6556 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6552 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6521 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6277 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6837 - accuracy: 0.5682 - val_loss: 0.6614 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.62136\n",
      "207/207 - 8s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 8s/epoch - 37ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6895 - val_accuracy: 0.6875 - 7s/epoch - 34ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.62136\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6526 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss improved from 0.62136 to 0.62112, saving model to /tmp\\checkpoint.h5\n",
      "207/207 - 7s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6211 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6490 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.62112\n",
      "207/207 - 8s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6512 - val_accuracy: 0.6875 - 8s/epoch - 38ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6487 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6473 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6543 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6538 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6488 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.62112\n",
      "207/207 - 8s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6433 - val_accuracy: 0.6875 - 8s/epoch - 37ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.62112\n",
      "207/207 - 8s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6460 - val_accuracy: 0.6875 - 8s/epoch - 37ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6487 - val_accuracy: 0.6875 - 7s/epoch - 35ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6533 - val_accuracy: 0.6875 - 7s/epoch - 33ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6566 - val_accuracy: 0.6875 - 7s/epoch - 33ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.7027 - val_accuracy: 0.3125 - 7s/epoch - 33ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6818 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6527 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6497 - val_accuracy: 0.6875 - 7s/epoch - 31ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6566 - val_accuracy: 0.6875 - 7s/epoch - 33ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6559 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6469 - val_accuracy: 0.6875 - 7s/epoch - 34ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6649 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6477 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6657 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6448 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6307 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6467 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6351 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6589 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6534 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6469 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6588 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6508 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6435 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6404 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6547 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6498 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6549 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6519 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6474 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6669 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6666 - val_accuracy: 0.6875 - 7s/epoch - 31ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6668 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6356 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6364 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6602 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6605 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6391 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6442 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6553 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6474 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6512 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6486 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6569 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6516 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6562 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6523 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6539 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6644 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6562 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6535 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6523 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6464 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6598 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6490 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6614 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6478 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6474 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6536 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6531 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6427 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6491 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6568 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6531 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6597 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6501 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6575 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6521 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6488 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6467 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6502 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6512 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6524 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6515 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6488 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6453 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6518 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6489 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6478 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6494 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6526 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6549 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6527 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6507 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6521 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6509 - val_accuracy: 0.6875 - 7s/epoch - 34ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.62112\n",
      "207/207 - 8s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6489 - val_accuracy: 0.6875 - 8s/epoch - 41ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.62112\n",
      "207/207 - 8s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6480 - val_accuracy: 0.6875 - 8s/epoch - 38ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6505 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6528 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6529 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6501 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6578 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6522 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6487 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6391 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6404 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6459 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.62112\n",
      "207/207 - 5s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6565 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.62112\n",
      "207/207 - 5s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6620 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.62112\n",
      "207/207 - 5s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6554 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6532 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6527 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6531 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.62112\n",
      "207/207 - 5s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6495 - val_accuracy: 0.6875 - 5s/epoch - 27ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6512 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6482 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6504 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6478 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6543 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6525 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6484 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6487 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6493 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6490 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6513 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6505 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6508 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6478 - val_accuracy: 0.6875 - 7s/epoch - 31ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6514 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6523 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6522 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6502 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6527 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6519 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6492 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6445 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6536 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6498 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6498 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6534 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6467 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6514 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6528 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6483 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6522 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6505 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6503 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6503 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6500 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6562 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6469 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6476 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6535 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6510 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6476 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6522 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6552 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6482 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.62112\n",
      "207/207 - 5s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6529 - val_accuracy: 0.6875 - 5s/epoch - 27ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6507 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6465 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6537 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6515 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.62112\n",
      "207/207 - 5s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6496 - val_accuracy: 0.6875 - 5s/epoch - 27ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6534 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6497 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.62112\n",
      "207/207 - 5s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6488 - val_accuracy: 0.6875 - 5s/epoch - 26ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6514 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6495 - val_accuracy: 0.6875 - 6s/epoch - 27ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6526 - val_accuracy: 0.6875 - 6s/epoch - 28ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6543 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6499 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6508 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6505 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6499 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6841 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 7s/epoch - 34ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6494 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6505 - val_accuracy: 0.6875 - 7s/epoch - 31ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6495 - val_accuracy: 0.6875 - 7s/epoch - 36ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6490 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6515 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6529 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6523 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6528 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6563 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6512 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6492 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.62112\n",
      "207/207 - 7s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6479 - val_accuracy: 0.6875 - 7s/epoch - 32ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6527 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6528 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6501 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6504 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6503 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6509 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6494 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6507 - val_accuracy: 0.6875 - 6s/epoch - 31ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6839 - accuracy: 0.5682 - val_loss: 0.6528 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6838 - accuracy: 0.5682 - val_loss: 0.6535 - val_accuracy: 0.6875 - 6s/epoch - 30ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.62112\n",
      "207/207 - 6s - loss: 0.6840 - accuracy: 0.5682 - val_loss: 0.6550 - val_accuracy: 0.6875 - 6s/epoch - 29ms/step\n",
      "179/179 [==============================] - 1s 6ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    }
   ],
   "source": [
    "probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type = 'new_ica', epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[3300    0]\n",
      " [2400    0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5867822  0.3884373 ]\n",
      " [0.58693403 0.38833806]\n",
      " [0.5868745  0.38833988]\n",
      " ...\n",
      " [0.5868858  0.38832617]\n",
      " [0.58690196 0.38826445]\n",
      " [0.58695865 0.3882019 ]]\n"
     ]
    }
   ],
   "source": [
    "print(probs_EEGNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.70372, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 43s - loss: 0.7966 - accuracy: 0.5592 - val_loss: 0.7037 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.70372\n",
      "413/413 - 38s - loss: 0.7220 - accuracy: 0.5624 - val_loss: 0.7449 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 0.70372 to 0.65350, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 36s - loss: 0.7195 - accuracy: 0.5611 - val_loss: 0.6535 - val_accuracy: 0.6875 - 36s/epoch - 88ms/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.65350\n",
      "413/413 - 35s - loss: 0.7200 - accuracy: 0.5627 - val_loss: 0.6917 - val_accuracy: 0.6862 - 35s/epoch - 84ms/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.65350\n",
      "413/413 - 38s - loss: 0.7125 - accuracy: 0.5659 - val_loss: 0.6569 - val_accuracy: 0.6875 - 38s/epoch - 92ms/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.65350\n",
      "413/413 - 40s - loss: 0.7187 - accuracy: 0.5643 - val_loss: 0.6786 - val_accuracy: 0.6875 - 40s/epoch - 96ms/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.65350\n",
      "413/413 - 41s - loss: 0.7197 - accuracy: 0.5658 - val_loss: 0.6609 - val_accuracy: 0.6875 - 41s/epoch - 98ms/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.65350\n",
      "413/413 - 44s - loss: 0.7170 - accuracy: 0.5677 - val_loss: 0.6795 - val_accuracy: 0.6875 - 44s/epoch - 108ms/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.65350\n",
      "413/413 - 41s - loss: 0.7256 - accuracy: 0.5626 - val_loss: 0.6649 - val_accuracy: 0.6862 - 41s/epoch - 100ms/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.65350\n",
      "413/413 - 46s - loss: 0.7247 - accuracy: 0.5622 - val_loss: 0.6751 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.65350\n",
      "413/413 - 44s - loss: 0.7134 - accuracy: 0.5667 - val_loss: 0.7608 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.65350\n",
      "413/413 - 40s - loss: 0.7076 - accuracy: 0.5673 - val_loss: 0.8395 - val_accuracy: 0.3125 - 40s/epoch - 97ms/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.65350\n",
      "413/413 - 40s - loss: 0.7085 - accuracy: 0.5654 - val_loss: 0.6624 - val_accuracy: 0.6875 - 40s/epoch - 98ms/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.65350\n",
      "413/413 - 42s - loss: 0.7105 - accuracy: 0.5631 - val_loss: 0.6732 - val_accuracy: 0.6875 - 42s/epoch - 102ms/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.65350\n",
      "413/413 - 38s - loss: 0.7152 - accuracy: 0.5660 - val_loss: 0.7852 - val_accuracy: 0.3125 - 38s/epoch - 93ms/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.65350 to 0.64239, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 37s - loss: 0.7077 - accuracy: 0.5671 - val_loss: 0.6424 - val_accuracy: 0.6875 - 37s/epoch - 90ms/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.64239\n",
      "413/413 - 38s - loss: 0.7030 - accuracy: 0.5673 - val_loss: 0.6792 - val_accuracy: 0.6875 - 38s/epoch - 91ms/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.64239\n",
      "413/413 - 41s - loss: 0.7027 - accuracy: 0.5679 - val_loss: 0.6802 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.64239\n",
      "413/413 - 41s - loss: 0.7027 - accuracy: 0.5668 - val_loss: 0.7085 - val_accuracy: 0.6875 - 41s/epoch - 98ms/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.64239\n",
      "413/413 - 41s - loss: 0.7098 - accuracy: 0.5672 - val_loss: 0.6974 - val_accuracy: 0.6862 - 41s/epoch - 98ms/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.64239\n",
      "413/413 - 39s - loss: 0.7065 - accuracy: 0.5670 - val_loss: 0.6935 - val_accuracy: 0.6875 - 39s/epoch - 95ms/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.64239\n",
      "413/413 - 38s - loss: 0.6918 - accuracy: 0.5680 - val_loss: 0.6509 - val_accuracy: 0.6875 - 38s/epoch - 91ms/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.64239\n",
      "413/413 - 38s - loss: 0.6915 - accuracy: 0.5680 - val_loss: 0.7683 - val_accuracy: 0.3125 - 38s/epoch - 92ms/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.64239\n",
      "413/413 - 36s - loss: 0.7103 - accuracy: 0.5661 - val_loss: 0.6550 - val_accuracy: 0.6875 - 36s/epoch - 87ms/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.64239\n",
      "413/413 - 39s - loss: 0.7021 - accuracy: 0.5679 - val_loss: 0.6639 - val_accuracy: 0.6875 - 39s/epoch - 95ms/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss improved from 0.64239 to 0.63020, saving model to /tmp\\checkpoint.h5\n",
      "413/413 - 42s - loss: 0.6909 - accuracy: 0.5681 - val_loss: 0.6302 - val_accuracy: 0.6875 - 42s/epoch - 103ms/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.7031 - accuracy: 0.5683 - val_loss: 0.6507 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6994 - accuracy: 0.5678 - val_loss: 0.6609 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6993 - accuracy: 0.5679 - val_loss: 0.6589 - val_accuracy: 0.6875 - 40s/epoch - 96ms/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.63020\n",
      "413/413 - 39s - loss: 0.6930 - accuracy: 0.5682 - val_loss: 0.6547 - val_accuracy: 0.6875 - 39s/epoch - 94ms/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6910 - accuracy: 0.5682 - val_loss: 0.6332 - val_accuracy: 0.6875 - 40s/epoch - 97ms/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.63020\n",
      "413/413 - 37s - loss: 0.6987 - accuracy: 0.5677 - val_loss: 0.7280 - val_accuracy: 0.3137 - 37s/epoch - 90ms/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.63020\n",
      "413/413 - 36s - loss: 0.7030 - accuracy: 0.5680 - val_loss: 0.6617 - val_accuracy: 0.6875 - 36s/epoch - 87ms/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.63020\n",
      "413/413 - 37s - loss: 0.6944 - accuracy: 0.5681 - val_loss: 0.6903 - val_accuracy: 0.6850 - 37s/epoch - 89ms/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.63020\n",
      "413/413 - 36s - loss: 0.6963 - accuracy: 0.5683 - val_loss: 0.6545 - val_accuracy: 0.6875 - 36s/epoch - 86ms/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.63020\n",
      "413/413 - 37s - loss: 0.6923 - accuracy: 0.5682 - val_loss: 0.6769 - val_accuracy: 0.6875 - 37s/epoch - 90ms/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6935 - accuracy: 0.5680 - val_loss: 0.7189 - val_accuracy: 0.3125 - 41s/epoch - 98ms/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6899 - accuracy: 0.5682 - val_loss: 0.6592 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6963 - accuracy: 0.5680 - val_loss: 0.6554 - val_accuracy: 0.6875 - 41s/epoch - 98ms/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6939 - accuracy: 0.5681 - val_loss: 0.6587 - val_accuracy: 0.6875 - 40s/epoch - 98ms/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6952 - accuracy: 0.5682 - val_loss: 0.6608 - val_accuracy: 0.6875 - 42s/epoch - 102ms/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6906 - accuracy: 0.5683 - val_loss: 0.6546 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.63020\n",
      "413/413 - 37s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6567 - val_accuracy: 0.6875 - 37s/epoch - 91ms/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6686 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6875 - accuracy: 0.5682 - val_loss: 0.6524 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.63020\n",
      "413/413 - 36s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6534 - val_accuracy: 0.6875 - 36s/epoch - 88ms/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6866 - accuracy: 0.5682 - val_loss: 0.6538 - val_accuracy: 0.6875 - 40s/epoch - 97ms/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6868 - accuracy: 0.5682 - val_loss: 0.6496 - val_accuracy: 0.6875 - 40s/epoch - 98ms/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6569 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6899 - accuracy: 0.5682 - val_loss: 0.6493 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6952 - accuracy: 0.5683 - val_loss: 0.6830 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6891 - accuracy: 0.5681 - val_loss: 0.6571 - val_accuracy: 0.6875 - 40s/epoch - 97ms/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6871 - accuracy: 0.5681 - val_loss: 0.6506 - val_accuracy: 0.6875 - 40s/epoch - 98ms/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6869 - accuracy: 0.5682 - val_loss: 0.6486 - val_accuracy: 0.6875 - 40s/epoch - 97ms/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6888 - accuracy: 0.5681 - val_loss: 0.6531 - val_accuracy: 0.6875 - 41s/epoch - 98ms/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6573 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6889 - accuracy: 0.5682 - val_loss: 0.6611 - val_accuracy: 0.6875 - 40s/epoch - 96ms/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.63020\n",
      "413/413 - 39s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6525 - val_accuracy: 0.6875 - 39s/epoch - 93ms/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.63020\n",
      "413/413 - 38s - loss: 0.6881 - accuracy: 0.5682 - val_loss: 0.6678 - val_accuracy: 0.6875 - 38s/epoch - 93ms/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6896 - accuracy: 0.5682 - val_loss: 0.6537 - val_accuracy: 0.6875 - 40s/epoch - 97ms/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6889 - accuracy: 0.5682 - val_loss: 0.6551 - val_accuracy: 0.6875 - 40s/epoch - 96ms/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6881 - accuracy: 0.5682 - val_loss: 0.6570 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6583 - val_accuracy: 0.6875 - 40s/epoch - 96ms/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.63020\n",
      "413/413 - 38s - loss: 0.6882 - accuracy: 0.5682 - val_loss: 0.6606 - val_accuracy: 0.6875 - 38s/epoch - 91ms/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6871 - accuracy: 0.5682 - val_loss: 0.6620 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6866 - accuracy: 0.5682 - val_loss: 0.6529 - val_accuracy: 0.6875 - 40s/epoch - 98ms/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6869 - accuracy: 0.5682 - val_loss: 0.6498 - val_accuracy: 0.6875 - 40s/epoch - 98ms/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6907 - accuracy: 0.5681 - val_loss: 0.6625 - val_accuracy: 0.6875 - 40s/epoch - 96ms/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.63020\n",
      "413/413 - 39s - loss: 0.6921 - accuracy: 0.5678 - val_loss: 0.6616 - val_accuracy: 0.6875 - 39s/epoch - 95ms/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.63020\n",
      "413/413 - 37s - loss: 0.6904 - accuracy: 0.5680 - val_loss: 0.6431 - val_accuracy: 0.6875 - 37s/epoch - 90ms/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6924 - accuracy: 0.5682 - val_loss: 0.6553 - val_accuracy: 0.6875 - 35s/epoch - 86ms/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.63020\n",
      "413/413 - 38s - loss: 0.6932 - accuracy: 0.5681 - val_loss: 0.6604 - val_accuracy: 0.6875 - 38s/epoch - 93ms/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6908 - accuracy: 0.5680 - val_loss: 0.6539 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6550 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6896 - accuracy: 0.5682 - val_loss: 0.6565 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6613 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6628 - val_accuracy: 0.6875 - 40s/epoch - 98ms/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.63020\n",
      "413/413 - 39s - loss: 0.6866 - accuracy: 0.5682 - val_loss: 0.6574 - val_accuracy: 0.6875 - 39s/epoch - 95ms/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6882 - accuracy: 0.5682 - val_loss: 0.6583 - val_accuracy: 0.6875 - 40s/epoch - 96ms/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6570 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6900 - accuracy: 0.5682 - val_loss: 0.6693 - val_accuracy: 0.6875 - 40s/epoch - 97ms/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6882 - accuracy: 0.5682 - val_loss: 0.6528 - val_accuracy: 0.6875 - 40s/epoch - 97ms/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6875 - accuracy: 0.5682 - val_loss: 0.6547 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6883 - accuracy: 0.5682 - val_loss: 0.6525 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6889 - accuracy: 0.5682 - val_loss: 0.6458 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.63020\n",
      "413/413 - 39s - loss: 0.6898 - accuracy: 0.5681 - val_loss: 0.6591 - val_accuracy: 0.6875 - 39s/epoch - 94ms/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6876 - accuracy: 0.5682 - val_loss: 0.6529 - val_accuracy: 0.6875 - 42s/epoch - 103ms/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6882 - accuracy: 0.5682 - val_loss: 0.6545 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6558 - val_accuracy: 0.6875 - 41s/epoch - 99ms/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6885 - accuracy: 0.5681 - val_loss: 0.6316 - val_accuracy: 0.6862 - 41s/epoch - 100ms/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.63020\n",
      "413/413 - 39s - loss: 0.6919 - accuracy: 0.5682 - val_loss: 0.6576 - val_accuracy: 0.6875 - 39s/epoch - 93ms/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.63020\n",
      "413/413 - 38s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6639 - val_accuracy: 0.6875 - 38s/epoch - 93ms/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6560 - val_accuracy: 0.6862 - 40s/epoch - 97ms/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6923 - accuracy: 0.5682 - val_loss: 0.6554 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6605 - val_accuracy: 0.6875 - 46s/epoch - 112ms/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6901 - accuracy: 0.5682 - val_loss: 0.6566 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6609 - val_accuracy: 0.6875 - 45s/epoch - 110ms/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6540 - val_accuracy: 0.6875 - 45s/epoch - 110ms/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6899 - accuracy: 0.5682 - val_loss: 0.6566 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6544 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6887 - accuracy: 0.5682 - val_loss: 0.6414 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6898 - accuracy: 0.5681 - val_loss: 0.6593 - val_accuracy: 0.6875 - 47s/epoch - 113ms/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.63020\n",
      "413/413 - 48s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6591 - val_accuracy: 0.6875 - 48s/epoch - 115ms/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6914 - accuracy: 0.5681 - val_loss: 0.6359 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6910 - accuracy: 0.5682 - val_loss: 0.6574 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6575 - val_accuracy: 0.6875 - 46s/epoch - 112ms/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6882 - accuracy: 0.5682 - val_loss: 0.6536 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6890 - accuracy: 0.5682 - val_loss: 0.6624 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6587 - val_accuracy: 0.6875 - 47s/epoch - 113ms/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6892 - accuracy: 0.5683 - val_loss: 0.6614 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6886 - accuracy: 0.5681 - val_loss: 0.6531 - val_accuracy: 0.6875 - 43s/epoch - 105ms/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6889 - accuracy: 0.5681 - val_loss: 0.6549 - val_accuracy: 0.6875 - 43s/epoch - 105ms/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6888 - accuracy: 0.5680 - val_loss: 0.6514 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.63020\n",
      "413/413 - 49s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6548 - val_accuracy: 0.6875 - 49s/epoch - 120ms/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6890 - accuracy: 0.5682 - val_loss: 0.6584 - val_accuracy: 0.6875 - 46s/epoch - 112ms/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6514 - val_accuracy: 0.6875 - 47s/epoch - 113ms/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6488 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6869 - accuracy: 0.5682 - val_loss: 0.6480 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6543 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6877 - accuracy: 0.5680 - val_loss: 0.6551 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6893 - accuracy: 0.5683 - val_loss: 0.6485 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6877 - accuracy: 0.5683 - val_loss: 0.6548 - val_accuracy: 0.6875 - 45s/epoch - 110ms/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6594 - val_accuracy: 0.6875 - 43s/epoch - 105ms/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6901 - accuracy: 0.5682 - val_loss: 0.6547 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6892 - accuracy: 0.5682 - val_loss: 0.6471 - val_accuracy: 0.6875 - 46s/epoch - 110ms/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6525 - val_accuracy: 0.6875 - 47s/epoch - 113ms/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6460 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6898 - accuracy: 0.5682 - val_loss: 0.6513 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6890 - accuracy: 0.5682 - val_loss: 0.6489 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6889 - accuracy: 0.5682 - val_loss: 0.6449 - val_accuracy: 0.6875 - 44s/epoch - 105ms/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6889 - accuracy: 0.5681 - val_loss: 0.6579 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6615 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6883 - accuracy: 0.5682 - val_loss: 0.6569 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6537 - val_accuracy: 0.6875 - 47s/epoch - 114ms/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6900 - accuracy: 0.5684 - val_loss: 0.7010 - val_accuracy: 0.4475 - 44s/epoch - 106ms/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6907 - accuracy: 0.5683 - val_loss: 0.6566 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6889 - accuracy: 0.5685 - val_loss: 0.6480 - val_accuracy: 0.6875 - 42s/epoch - 102ms/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6890 - accuracy: 0.5682 - val_loss: 0.6616 - val_accuracy: 0.6875 - 42s/epoch - 102ms/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6886 - accuracy: 0.5679 - val_loss: 0.6527 - val_accuracy: 0.6875 - 45s/epoch - 110ms/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6881 - accuracy: 0.5671 - val_loss: 0.6386 - val_accuracy: 0.6875 - 47s/epoch - 113ms/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6901 - accuracy: 0.5680 - val_loss: 0.6424 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6893 - accuracy: 0.5682 - val_loss: 0.6698 - val_accuracy: 0.6862 - 43s/epoch - 105ms/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6900 - accuracy: 0.5683 - val_loss: 0.6562 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6598 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6537 - val_accuracy: 0.6875 - 45s/epoch - 110ms/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6607 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6896 - accuracy: 0.5682 - val_loss: 0.6521 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6892 - accuracy: 0.5680 - val_loss: 0.6558 - val_accuracy: 0.6875 - 45s/epoch - 110ms/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6532 - val_accuracy: 0.6875 - 43s/epoch - 105ms/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6867 - accuracy: 0.5682 - val_loss: 0.6568 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6872 - accuracy: 0.5682 - val_loss: 0.6538 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6595 - val_accuracy: 0.6875 - 42s/epoch - 103ms/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6864 - accuracy: 0.5682 - val_loss: 0.6548 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6867 - accuracy: 0.5682 - val_loss: 0.6588 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6880 - accuracy: 0.5683 - val_loss: 0.6508 - val_accuracy: 0.6875 - 47s/epoch - 115ms/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6876 - accuracy: 0.5682 - val_loss: 0.6540 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6869 - accuracy: 0.5682 - val_loss: 0.6474 - val_accuracy: 0.6875 - 44s/epoch - 108ms/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6887 - accuracy: 0.5681 - val_loss: 0.6890 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6520 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6914 - accuracy: 0.5680 - val_loss: 0.6674 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6913 - accuracy: 0.5682 - val_loss: 0.6543 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6918 - accuracy: 0.5682 - val_loss: 0.6620 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6914 - accuracy: 0.5682 - val_loss: 0.6513 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6896 - accuracy: 0.5682 - val_loss: 0.6568 - val_accuracy: 0.6875 - 43s/epoch - 105ms/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6709 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6890 - accuracy: 0.5674 - val_loss: 0.6484 - val_accuracy: 0.6875 - 42s/epoch - 101ms/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6497 - val_accuracy: 0.6875 - 44s/epoch - 105ms/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6592 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6899 - accuracy: 0.5681 - val_loss: 0.6507 - val_accuracy: 0.6875 - 42s/epoch - 101ms/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6913 - accuracy: 0.5682 - val_loss: 0.6652 - val_accuracy: 0.6875 - 46s/epoch - 110ms/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6553 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6881 - accuracy: 0.5679 - val_loss: 0.6474 - val_accuracy: 0.6875 - 47s/epoch - 113ms/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6903 - accuracy: 0.5682 - val_loss: 0.6485 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6407 - val_accuracy: 0.6862 - 45s/epoch - 110ms/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.63020\n",
      "413/413 - 50s - loss: 0.6883 - accuracy: 0.5681 - val_loss: 0.6579 - val_accuracy: 0.6875 - 50s/epoch - 121ms/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6883 - accuracy: 0.5682 - val_loss: 0.6610 - val_accuracy: 0.6875 - 46s/epoch - 112ms/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6872 - accuracy: 0.5682 - val_loss: 0.6740 - val_accuracy: 0.6875 - 46s/epoch - 112ms/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6383 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6902 - accuracy: 0.5682 - val_loss: 0.6576 - val_accuracy: 0.6875 - 46s/epoch - 112ms/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.63020\n",
      "413/413 - 47s - loss: 0.6912 - accuracy: 0.5682 - val_loss: 0.6705 - val_accuracy: 0.6875 - 47s/epoch - 113ms/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6889 - accuracy: 0.5682 - val_loss: 0.6589 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6921 - accuracy: 0.5680 - val_loss: 0.6618 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6922 - accuracy: 0.5681 - val_loss: 0.6389 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6930 - accuracy: 0.5681 - val_loss: 0.6550 - val_accuracy: 0.6875 - 46s/epoch - 111ms/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6896 - accuracy: 0.5682 - val_loss: 0.6560 - val_accuracy: 0.6875 - 46s/epoch - 110ms/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.63020\n",
      "413/413 - 46s - loss: 0.6898 - accuracy: 0.5681 - val_loss: 0.6588 - val_accuracy: 0.6875 - 46s/epoch - 112ms/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6889 - accuracy: 0.5682 - val_loss: 0.6661 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6889 - accuracy: 0.5682 - val_loss: 0.6541 - val_accuracy: 0.6875 - 41s/epoch - 98ms/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6702 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6900 - accuracy: 0.5683 - val_loss: 0.6563 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6901 - accuracy: 0.5683 - val_loss: 0.6577 - val_accuracy: 0.6875 - 44s/epoch - 106ms/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6900 - accuracy: 0.5681 - val_loss: 0.6686 - val_accuracy: 0.6875 - 43s/epoch - 104ms/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.63020\n",
      "413/413 - 40s - loss: 0.6883 - accuracy: 0.5682 - val_loss: 0.6700 - val_accuracy: 0.6862 - 40s/epoch - 97ms/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6536 - val_accuracy: 0.6875 - 45s/epoch - 108ms/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.63020\n",
      "413/413 - 48s - loss: 0.6887 - accuracy: 0.5682 - val_loss: 0.6657 - val_accuracy: 0.6875 - 48s/epoch - 115ms/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6889 - accuracy: 0.5679 - val_loss: 0.6504 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6876 - accuracy: 0.5682 - val_loss: 0.6602 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.63020\n",
      "413/413 - 44s - loss: 0.6890 - accuracy: 0.5680 - val_loss: 0.6531 - val_accuracy: 0.6875 - 44s/epoch - 107ms/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.63020\n",
      "413/413 - 42s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6521 - val_accuracy: 0.6875 - 42s/epoch - 102ms/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6874 - accuracy: 0.5683 - val_loss: 0.6526 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6875 - accuracy: 0.5681 - val_loss: 0.6466 - val_accuracy: 0.6875 - 43s/epoch - 105ms/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6407 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.63020\n",
      "413/413 - 45s - loss: 0.6887 - accuracy: 0.5681 - val_loss: 0.6602 - val_accuracy: 0.6875 - 45s/epoch - 109ms/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.63020\n",
      "413/413 - 39s - loss: 0.6901 - accuracy: 0.5680 - val_loss: 0.6562 - val_accuracy: 0.6875 - 39s/epoch - 94ms/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.63020\n",
      "413/413 - 36s - loss: 0.6878 - accuracy: 0.5681 - val_loss: 0.6612 - val_accuracy: 0.6875 - 36s/epoch - 87ms/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6875 - accuracy: 0.5682 - val_loss: 0.6506 - val_accuracy: 0.6875 - 34s/epoch - 82ms/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6875 - accuracy: 0.5681 - val_loss: 0.6746 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6479 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6892 - accuracy: 0.5682 - val_loss: 0.6436 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6883 - accuracy: 0.5682 - val_loss: 0.6536 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6875 - accuracy: 0.5682 - val_loss: 0.6621 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6587 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6871 - accuracy: 0.5682 - val_loss: 0.6580 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6886 - accuracy: 0.5680 - val_loss: 0.6520 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6904 - accuracy: 0.5682 - val_loss: 0.6809 - val_accuracy: 0.6812 - 34s/epoch - 83ms/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6914 - accuracy: 0.5682 - val_loss: 0.6634 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6889 - accuracy: 0.5683 - val_loss: 0.6627 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6901 - accuracy: 0.5682 - val_loss: 0.6709 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.63020\n",
      "413/413 - 38s - loss: 0.6899 - accuracy: 0.5682 - val_loss: 0.6566 - val_accuracy: 0.6875 - 38s/epoch - 93ms/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6902 - accuracy: 0.5681 - val_loss: 0.6700 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6372 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6460 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6905 - accuracy: 0.5682 - val_loss: 0.6548 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6912 - accuracy: 0.5682 - val_loss: 0.6619 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6881 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6566 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6882 - accuracy: 0.5681 - val_loss: 0.6525 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6877 - accuracy: 0.5679 - val_loss: 0.6448 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6876 - accuracy: 0.5682 - val_loss: 0.6630 - val_accuracy: 0.6862 - 34s/epoch - 83ms/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6881 - accuracy: 0.5681 - val_loss: 0.6465 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6509 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6910 - accuracy: 0.5680 - val_loss: 0.6559 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6883 - accuracy: 0.5682 - val_loss: 0.6569 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6869 - accuracy: 0.5682 - val_loss: 0.6524 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6876 - accuracy: 0.5682 - val_loss: 0.6569 - val_accuracy: 0.6875 - 34s/epoch - 84ms/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6458 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6898 - accuracy: 0.5682 - val_loss: 0.6486 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6872 - accuracy: 0.5682 - val_loss: 0.6536 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6881 - accuracy: 0.5681 - val_loss: 0.6357 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6902 - accuracy: 0.5682 - val_loss: 0.6623 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6899 - accuracy: 0.5681 - val_loss: 0.6473 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6888 - accuracy: 0.5682 - val_loss: 0.6573 - val_accuracy: 0.6875 - 34s/epoch - 82ms/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6876 - accuracy: 0.5682 - val_loss: 0.6546 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6675 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.63020\n",
      "413/413 - 37s - loss: 0.6897 - accuracy: 0.5682 - val_loss: 0.6580 - val_accuracy: 0.6875 - 37s/epoch - 89ms/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6892 - accuracy: 0.5682 - val_loss: 0.6540 - val_accuracy: 0.6875 - 34s/epoch - 82ms/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6329 - val_accuracy: 0.6862 - 34s/epoch - 83ms/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6547 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6487 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6546 - val_accuracy: 0.6862 - 34s/epoch - 83ms/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6549 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6483 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6885 - accuracy: 0.5683 - val_loss: 0.6561 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6903 - accuracy: 0.5682 - val_loss: 0.6579 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6877 - accuracy: 0.5682 - val_loss: 0.6574 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6877 - accuracy: 0.5682 - val_loss: 0.6526 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6891 - accuracy: 0.5682 - val_loss: 0.6493 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.63020\n",
      "413/413 - 36s - loss: 0.6871 - accuracy: 0.5682 - val_loss: 0.6580 - val_accuracy: 0.6875 - 36s/epoch - 87ms/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6515 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6547 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6907 - accuracy: 0.5683 - val_loss: 0.6571 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6895 - accuracy: 0.5682 - val_loss: 0.6570 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6898 - accuracy: 0.5682 - val_loss: 0.6802 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6898 - accuracy: 0.5682 - val_loss: 0.6653 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6892 - accuracy: 0.5682 - val_loss: 0.6597 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6888 - accuracy: 0.5681 - val_loss: 0.6537 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6874 - accuracy: 0.5682 - val_loss: 0.6397 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.63020\n",
      "413/413 - 41s - loss: 0.6889 - accuracy: 0.5683 - val_loss: 0.6578 - val_accuracy: 0.6875 - 41s/epoch - 100ms/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.63020\n",
      "413/413 - 43s - loss: 0.6888 - accuracy: 0.5683 - val_loss: 0.6939 - val_accuracy: 0.6862 - 43s/epoch - 104ms/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.63020\n",
      "413/413 - 38s - loss: 0.6885 - accuracy: 0.5683 - val_loss: 0.6657 - val_accuracy: 0.6875 - 38s/epoch - 93ms/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6901 - accuracy: 0.5682 - val_loss: 0.6467 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6896 - accuracy: 0.5682 - val_loss: 0.6537 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6892 - accuracy: 0.5682 - val_loss: 0.6521 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6900 - accuracy: 0.5682 - val_loss: 0.6517 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6898 - accuracy: 0.5682 - val_loss: 0.6562 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6877 - accuracy: 0.5682 - val_loss: 0.6510 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6880 - accuracy: 0.5682 - val_loss: 0.6552 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6872 - accuracy: 0.5682 - val_loss: 0.6503 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6881 - accuracy: 0.5680 - val_loss: 0.6557 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6886 - accuracy: 0.5682 - val_loss: 0.6581 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6531 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6884 - accuracy: 0.5682 - val_loss: 0.6530 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6870 - accuracy: 0.5682 - val_loss: 0.6539 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6882 - accuracy: 0.5682 - val_loss: 0.6730 - val_accuracy: 0.6850 - 35s/epoch - 84ms/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6892 - accuracy: 0.5678 - val_loss: 0.6626 - val_accuracy: 0.6862 - 35s/epoch - 84ms/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6896 - accuracy: 0.5682 - val_loss: 0.6591 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6559 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6894 - accuracy: 0.5680 - val_loss: 0.6554 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6878 - accuracy: 0.5682 - val_loss: 0.6559 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6639 - val_accuracy: 0.6875 - 34s/epoch - 84ms/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6554 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6885 - accuracy: 0.5682 - val_loss: 0.6571 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6879 - accuracy: 0.5682 - val_loss: 0.6536 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6868 - accuracy: 0.5682 - val_loss: 0.6527 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6871 - accuracy: 0.5682 - val_loss: 0.6579 - val_accuracy: 0.6875 - 35s/epoch - 85ms/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.63020\n",
      "413/413 - 38s - loss: 0.6873 - accuracy: 0.5682 - val_loss: 0.6515 - val_accuracy: 0.6875 - 38s/epoch - 92ms/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.63020\n",
      "413/413 - 35s - loss: 0.6911 - accuracy: 0.5682 - val_loss: 0.6545 - val_accuracy: 0.6875 - 35s/epoch - 84ms/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6900 - accuracy: 0.5682 - val_loss: 0.6674 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.63020\n",
      "413/413 - 34s - loss: 0.6894 - accuracy: 0.5682 - val_loss: 0.6560 - val_accuracy: 0.6875 - 34s/epoch - 83ms/step\n",
      "179/179 [==============================] - 5s 27ms/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type = 'new_ica', epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59316754 0.40683243]\n",
      " [0.5925944  0.4074056 ]\n",
      " [0.59207034 0.4079297 ]\n",
      " ...\n",
      " [0.59237415 0.40762585]\n",
      " [0.59371305 0.40628695]\n",
      " [0.5928277  0.40717235]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      " Confusion matrix:\n",
      "[[3300    0]\n",
      " [2400    0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DeepConvNet() got an unexpected keyword argument 'kernLength'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21612\\2130345034.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprobs_Deep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEEGNet_DeepConvNet_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'new_ica'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\annej\\OneDrive\\Documents\\GitHub\\MASTER-eeg-stress-det\\classifiers.py\u001b[0m in \u001b[0;36mEEGNet_DeepConvNet_classification\u001b[1;34m(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type, epoched)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoched\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'new_ica'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             model = DeepConvNet(nb_classes = 2, Chans = v.NUM_CHANNELS, Samples = v.EPOCH_LENGTH*v.NEW_SFREQ, \n\u001b[0m\u001b[0;32m    386\u001b[0m                                 dropoutRate = 0.5, kernLength = 128, F1 = 96, D = 1, F2 = 96, dropoutType = 'Dropout')\n\u001b[0;32m    387\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: DeepConvNet() got an unexpected keyword argument 'kernLength'"
     ]
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type = 'new_ica', epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type = 'new_ica', epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_TSGLEEGNet = EEGNet_TSGLEEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels, data_type = 'new_ica', epoched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs_TSGLEEGNet)\n",
    "preds_TSGLEEGNet = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_TSGLEEGNet)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_TSGLEEGNet = compute_metrics(test_labels, preds_TSGLEEGNet)\n",
    "print(performance_TSGLEEGNet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
