{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.data import extract_eeg_data, multi_to_binary_classification, split_dataset, dict_to_arr\n",
    "from utils.labels import get_stai_labels\n",
    "from utils.valid_recs import get_valid_recs\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification\n",
    "import utils.variables as v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n",
      "Data/ICA_data/sub-P010_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data/sub-P013_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data/sub-P013_ses-S001_run-002.mat not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/ICA_data/sub-P020_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data/sub-P023_ses-S002_run-002.mat not valid\n",
      "Returning valid recordings\n",
      "\n",
      "Valid recs ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n"
     ]
    }
   ],
   "source": [
    "valid_recs = get_valid_recs(data_type='ica', output_type = 'np')\n",
    "print(f'Valid recs {valid_recs}')\n",
    "\n",
    "x_dict_ = extract_eeg_data(valid_recs, data_type='ica', output_type='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n"
     ]
    }
   ],
   "source": [
    "y_dict_ = get_stai_labels(valid_recs) \n",
    "#y_dict = get_pss_labels(valid_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing invalid labels: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing invalid labels: {len(y_dict_)}\")\n",
    "\n",
    "print(y_dict_['P007_S001_002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n"
     ]
    }
   ],
   "source": [
    "x_dict, y_dict = multi_to_binary_classification(x_dict_, y_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing mildly stressed subjects: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing  mildly stressed subjects: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict, test_data_dict, val_data_dict, train_labels_dict, test_labels_dict, val_labels_dict = split_dataset(x_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train data set: {len(train_data_dict)}\")\n",
    "print(f\"Length of validation data set: {len(val_data_dict)}\")\n",
    "print(f\"Length of test data set: {len(test_data_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (44, 8, 75000)\n",
      "Shape of validation data set: (16, 8, 75000)\n",
      "Shape of test data set: (19, 8, 75000)\n",
      "Shape of train labels set: (44, 1)\n",
      "Shape of validation labels set: (16, 1)\n",
      "Shape of test labels set: (19, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = dict_to_arr(train_data_dict)\n",
    "test_data = dict_to_arr(test_data_dict)\n",
    "val_data = dict_to_arr(val_data_dict)\n",
    "\n",
    "train_labels = np.reshape(np.array(list(train_labels_dict.values())), (len(train_data),1))\n",
    "test_labels = np.reshape(np.array(list(test_labels_dict.values())), (len(test_data),1))\n",
    "val_labels = np.reshape(np.array(list(val_labels_dict.values())), (len(val_data),1))\n",
    "\n",
    "print(f\"Shape of train data set: {train_data.shape}\")\n",
    "print(f\"Shape of validation data set: {val_data.shape}\")\n",
    "print(f\"Shape of test data set: {test_data.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Shape of train labels set: {train_labels.shape}\")\n",
    "print(f\"Shape of validation labels set: {val_labels.shape}\")\n",
    "print(f\"Shape of test labels set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.71064, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 11s - loss: 0.9489 - accuracy: 0.4545 - val_loss: 0.7106 - val_accuracy: 0.3125 - 11s/epoch - 11s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.5629 - accuracy: 0.6818 - val_loss: 0.7657 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.3071 - accuracy: 0.8409 - val_loss: 0.7491 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1756 - accuracy: 0.9773 - val_loss: 0.7269 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1320 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.3750 - 9s/epoch - 9s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.7117 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.0876 - accuracy: 1.0000 - val_loss: 0.7109 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 0.71064 to 0.70986, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.7099 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 0.70986 to 0.70700, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.7070 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 0.70700 to 0.70200, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.7020 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 0.70200 to 0.69611, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 0.69611 to 0.68972, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 0.68972 to 0.68410, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.68410 to 0.67905, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.67905 to 0.67509, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.67509 to 0.67224, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.67224 to 0.67001, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.6700 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.67001 to 0.66768, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.6677 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 0.66768 to 0.66546, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.6655 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.66546 to 0.66335, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.66335 to 0.66125, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.66125 to 0.65953, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.6595 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.65953 to 0.65809, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss improved from 0.65809 to 0.65643, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss improved from 0.65643 to 0.65470, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss improved from 0.65470 to 0.65297, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.6530 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss improved from 0.65297 to 0.65119, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.6512 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 0.65119 to 0.64929, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.64929 to 0.64748, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 10s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.6250 - 10s/epoch - 10s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss improved from 0.64748 to 0.64566, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 11s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.6457 - val_accuracy: 0.6250 - 11s/epoch - 11s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss improved from 0.64566 to 0.64393, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 12s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.6439 - val_accuracy: 0.6875 - 12s/epoch - 12s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss improved from 0.64393 to 0.64230, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.6423 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss improved from 0.64230 to 0.64093, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss improved from 0.64093 to 0.63976, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss improved from 0.63976 to 0.63852, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.6385 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss improved from 0.63852 to 0.63742, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss improved from 0.63742 to 0.63661, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss improved from 0.63661 to 0.63567, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss improved from 0.63567 to 0.63472, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss improved from 0.63472 to 0.63351, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.6335 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss improved from 0.63351 to 0.63230, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss improved from 0.63230 to 0.63113, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.6311 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss improved from 0.63113 to 0.63003, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6300 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss improved from 0.63003 to 0.62889, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss improved from 0.62889 to 0.62770, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.6277 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss improved from 0.62770 to 0.62656, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss improved from 0.62656 to 0.62544, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss improved from 0.62544 to 0.62431, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.62431 to 0.62322, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss improved from 0.62322 to 0.62220, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.62220 to 0.62119, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss improved from 0.62119 to 0.62010, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss improved from 0.62010 to 0.61895, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss improved from 0.61895 to 0.61802, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss improved from 0.61802 to 0.61704, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss improved from 0.61704 to 0.61608, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss improved from 0.61608 to 0.61516, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss improved from 0.61516 to 0.61417, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss improved from 0.61417 to 0.61316, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss improved from 0.61316 to 0.61204, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss improved from 0.61204 to 0.61090, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss improved from 0.61090 to 0.60971, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss improved from 0.60971 to 0.60867, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss improved from 0.60867 to 0.60787, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 0.60787 to 0.60701, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss improved from 0.60701 to 0.60634, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss improved from 0.60634 to 0.60557, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss improved from 0.60557 to 0.60476, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss improved from 0.60476 to 0.60401, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss improved from 0.60401 to 0.60309, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss improved from 0.60309 to 0.60220, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss improved from 0.60220 to 0.60128, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss improved from 0.60128 to 0.60033, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss improved from 0.60033 to 0.59933, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss improved from 0.59933 to 0.59831, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss improved from 0.59831 to 0.59742, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5974 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss improved from 0.59742 to 0.59663, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss improved from 0.59663 to 0.59588, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.5959 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss improved from 0.59588 to 0.59515, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss improved from 0.59515 to 0.59454, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5945 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss improved from 0.59454 to 0.59406, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss improved from 0.59406 to 0.59354, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5935 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss improved from 0.59354 to 0.59277, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5928 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss improved from 0.59277 to 0.59194, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss improved from 0.59194 to 0.59118, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5912 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss improved from 0.59118 to 0.59035, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5903 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss improved from 0.59035 to 0.58935, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5893 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss improved from 0.58935 to 0.58838, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5884 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss improved from 0.58838 to 0.58752, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss improved from 0.58752 to 0.58672, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5867 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss improved from 0.58672 to 0.58592, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss improved from 0.58592 to 0.58500, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss improved from 0.58500 to 0.58396, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.5840 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss improved from 0.58396 to 0.58298, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5830 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss improved from 0.58298 to 0.58201, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.5820 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss improved from 0.58201 to 0.58121, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5812 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss improved from 0.58121 to 0.58055, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5805 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss improved from 0.58055 to 0.57995, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5799 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss improved from 0.57995 to 0.57959, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5796 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss improved from 0.57959 to 0.57911, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5791 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss improved from 0.57911 to 0.57875, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.5787 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss improved from 0.57875 to 0.57842, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5784 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 0.57842 to 0.57811, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5781 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 0.57811 to 0.57753, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss improved from 0.57753 to 0.57697, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5770 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss improved from 0.57697 to 0.57632, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5763 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss improved from 0.57632 to 0.57562, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5756 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss improved from 0.57562 to 0.57481, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5748 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss improved from 0.57481 to 0.57412, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss improved from 0.57412 to 0.57358, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss improved from 0.57358 to 0.57302, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss improved from 0.57302 to 0.57257, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss improved from 0.57257 to 0.57224, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss improved from 0.57224 to 0.57193, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5719 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss improved from 0.57193 to 0.57155, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5715 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss improved from 0.57155 to 0.57117, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss improved from 0.57117 to 0.57093, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 545s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5709 - val_accuracy: 0.7500 - 545s/epoch - 545s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss improved from 0.57093 to 0.57081, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss improved from 0.57081 to 0.57056, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 10s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5706 - val_accuracy: 0.7500 - 10s/epoch - 10s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss improved from 0.57056 to 0.57030, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5703 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss improved from 0.57030 to 0.57006, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5701 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss improved from 0.57006 to 0.56977, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5698 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss improved from 0.56977 to 0.56953, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.56953\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss improved from 0.56953 to 0.56948, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.5697 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5699 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5698 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss improved from 0.56948 to 0.56925, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.5693 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss improved from 0.56925 to 0.56909, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss improved from 0.56909 to 0.56870, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss improved from 0.56870 to 0.56844, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss improved from 0.56844 to 0.56825, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5683 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss improved from 0.56825 to 0.56816, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5682 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss improved from 0.56816 to 0.56805, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss improved from 0.56805 to 0.56801, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5680 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5702 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5709 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5727 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5735 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5749 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.56801\n",
      "1/1 - 10s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5756 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.56801\n",
      "1/1 - 10s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5761 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5769 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5781 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5788 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5792 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5797 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5804 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5813 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5825 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5834 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5842 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5846 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5851 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5863 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5891 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5902 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5911 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5921 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5934 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5957 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6000 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6208 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6244 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6259 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6256 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6246 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6279 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6307 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6332 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6405 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6417 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6448 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6505 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6670 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6693 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6795 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6738 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6756 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6903 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6960 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6997 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7001 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6994 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6970 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6962 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6965 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6983 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7019 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7041 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7071 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7096 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7130 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7161 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7184 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7212 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7259 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7260 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7267 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7272 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7291 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7308 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7326 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7345 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x137038a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x137038a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 414ms/step\n",
      "Classification accuracy: 0.562327 \n"
     ]
    }
   ],
   "source": [
    "probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[9 2]\n",
      " [8 0]]\n",
      "[47.37 52.94  0.  ]\n"
     ]
    }
   ],
   "source": [
    "# with init_filter data, 300 epochs, sigmoid\n",
    "probs_EEGNet_init_sigmoid = np.array([\n",
    "                  [0.7138277, 0.28617287],\n",
    "                  [0.5637057, 0.43629473],\n",
    "                  [0.6218500, 0.37815124],\n",
    "                  [0.7166857, 0.28331438],\n",
    "                  [0.6754987, 0.32450426],\n",
    "                  [0.8090515, 0.19095036],\n",
    "                  [0.5339635, 0.46603800],\n",
    "                  [0.2460488, 0.75395140],\n",
    "                  [0.4467932, 0.55320940],\n",
    "                  [0.6290466, 0.37095330],\n",
    "                  [0.5762418, 0.42375806],\n",
    "                  [0.1861840, 0.81381420],\n",
    "                  [0.6687245, 0.33127743],\n",
    "                  [0.7082854, 0.29171503],\n",
    "                  [0.5183024, 0.48169836],\n",
    "                  [0.7217397, 0.27826140],\n",
    "                  [0.6201925, 0.37980822],\n",
    "                  [0.6769989, 0.32300153],\n",
    "                  [0.4880893, 0.51191190]])\n",
    "\n",
    "\n",
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6622658  0.33773378]\n",
      " [0.6305761  0.3694238 ]\n",
      " [0.59458554 0.40541294]\n",
      " [0.58825123 0.41174808]\n",
      " [0.61485636 0.38514262]\n",
      " [0.6281727  0.37182656]\n",
      " [0.6707296  0.32926738]\n",
      " [0.7152163  0.28478536]\n",
      " [0.3621873  0.63781214]\n",
      " [0.56972075 0.43027884]\n",
      " [0.629088   0.37091017]\n",
      " [0.4353447  0.5646552 ]\n",
      " [0.56697416 0.43302345]\n",
      " [0.6457108  0.35428876]\n",
      " [0.6471834  0.35281634]\n",
      " [0.5027475  0.497253  ]\n",
      " [0.62844676 0.37155262]\n",
      " [0.57276446 0.42723358]\n",
      " [0.5813717  0.41862816]]\n"
     ]
    }
   ],
   "source": [
    "print(probs_EEGNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_SSVEP = EEGNet_SSVEP_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(probs_SSVEP)\\npreds_SSVEP = probs_SSVEP.argmax(axis = -1)  \\nprint(preds_SSVEP)\\nprint(test_labels.T)\\n\\nperformance_SSVEP = compute_metrics(test_labels, preds_SSVEP)\\nprint(performance_SSVEP)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(probs_SSVEP)\n",
    "preds_SSVEP = probs_SSVEP.argmax(axis = -1)  \n",
    "print(preds_SSVEP)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_SSVEP = compute_metrics(test_labels, preds_SSVEP)\n",
    "print(performance_SSVEP)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 6.27307, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 4.0190 - accuracy: 0.4773 - val_loss: 6.2731 - val_accuracy: 0.6875 - 23s/epoch - 12s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 6.27307 to 2.36650, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 3.7352 - accuracy: 0.7955 - val_loss: 2.3665 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 2.36650 to 1.98935, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.9717 - accuracy: 1.0000 - val_loss: 1.9894 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 1.98935 to 1.86849, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.8628 - accuracy: 1.0000 - val_loss: 1.8685 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 1.86849 to 1.71422, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.6239 - accuracy: 1.0000 - val_loss: 1.7142 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 1.71422 to 1.58675, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.5761 - accuracy: 1.0000 - val_loss: 1.5867 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 1.58675 to 1.42664, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 2.4057 - accuracy: 1.0000 - val_loss: 1.4266 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 1.42664 to 1.30816, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 2.3062 - accuracy: 1.0000 - val_loss: 1.3082 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 1.30816 to 1.21788, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.2305 - accuracy: 1.0000 - val_loss: 1.2179 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 1.21788 to 1.14220, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.0308 - accuracy: 1.0000 - val_loss: 1.1422 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 1.14220 to 1.08146, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.9791 - accuracy: 1.0000 - val_loss: 1.0815 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08146 to 1.05300, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.9983 - accuracy: 1.0000 - val_loss: 1.0530 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05300 to 1.02180, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.8117 - accuracy: 1.0000 - val_loss: 1.0218 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 1.02180 to 0.98353, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.7074 - accuracy: 1.0000 - val_loss: 0.9835 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.98353 to 0.95233, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.5503 - accuracy: 1.0000 - val_loss: 0.9523 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.95233 to 0.92529, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.7369 - accuracy: 1.0000 - val_loss: 0.9253 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.92529 to 0.90449, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 1.3877 - accuracy: 1.0000 - val_loss: 0.9045 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.90449 to 0.88787, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.3181 - accuracy: 1.0000 - val_loss: 0.8879 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.2535 - accuracy: 1.0000 - val_loss: 1.0483 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.9340 - accuracy: 0.7955 - val_loss: 1.0947 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 2.2689 - accuracy: 0.7273 - val_loss: 1.3368 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.8528 - accuracy: 0.8636 - val_loss: 0.8907 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 2.4674 - accuracy: 0.7955 - val_loss: 2.4543 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 3.8694 - accuracy: 0.7273 - val_loss: 3.1393 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 5.3905 - accuracy: 0.5455 - val_loss: 1.6290 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4653 - accuracy: 0.9318 - val_loss: 1.2138 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.8584 - accuracy: 0.8182 - val_loss: 1.3382 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4604 - accuracy: 1.0000 - val_loss: 1.5068 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.5651 - accuracy: 0.9545 - val_loss: 1.2809 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4737 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.88787\n",
      "2/2 - 21s - loss: 1.4356 - accuracy: 1.0000 - val_loss: 1.4939 - val_accuracy: 0.4375 - 21s/epoch - 10s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4822 - accuracy: 1.0000 - val_loss: 1.3995 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.5076 - accuracy: 1.0000 - val_loss: 1.3228 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.5063 - accuracy: 1.0000 - val_loss: 1.2688 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3816 - accuracy: 1.0000 - val_loss: 1.2280 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.4742 - accuracy: 1.0000 - val_loss: 1.1939 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3909 - accuracy: 1.0000 - val_loss: 1.1628 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3037 - accuracy: 1.0000 - val_loss: 1.1349 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2763 - accuracy: 1.0000 - val_loss: 1.1073 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3576 - accuracy: 1.0000 - val_loss: 1.0829 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2239 - accuracy: 1.0000 - val_loss: 1.0578 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.1845 - accuracy: 1.0000 - val_loss: 1.0315 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2561 - accuracy: 1.0000 - val_loss: 1.0041 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2256 - accuracy: 1.0000 - val_loss: 0.9873 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.1168 - accuracy: 1.0000 - val_loss: 0.9688 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2218 - accuracy: 1.0000 - val_loss: 0.9454 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.1900 - accuracy: 1.0000 - val_loss: 0.9130 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.0534 - accuracy: 1.0000 - val_loss: 0.8922 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.88787 to 0.88268, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.1142 - accuracy: 1.0000 - val_loss: 0.8827 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss improved from 0.88268 to 0.86842, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.0866 - accuracy: 1.0000 - val_loss: 0.8684 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.86842 to 0.84958, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.0811 - accuracy: 1.0000 - val_loss: 0.8496 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss improved from 0.84958 to 0.83640, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.9852 - accuracy: 1.0000 - val_loss: 0.8364 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss improved from 0.83640 to 0.82864, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.9599 - accuracy: 1.0000 - val_loss: 0.8286 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0331 - accuracy: 1.0000 - val_loss: 1.0365 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0016 - accuracy: 1.0000 - val_loss: 1.3143 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 3.1522 - accuracy: 0.8409 - val_loss: 1.0322 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 2.1564 - accuracy: 0.7727 - val_loss: 2.0707 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 2.5873 - accuracy: 0.6818 - val_loss: 0.9352 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1899 - accuracy: 0.9545 - val_loss: 1.0048 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1092 - accuracy: 0.9773 - val_loss: 0.9831 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0651 - accuracy: 1.0000 - val_loss: 1.1015 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 2.2930 - accuracy: 0.8182 - val_loss: 2.4593 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 3.6294 - accuracy: 0.7500 - val_loss: 1.3839 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.2361 - accuracy: 0.9545 - val_loss: 1.6808 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.8863 - accuracy: 0.8636 - val_loss: 1.7575 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1535 - accuracy: 0.9545 - val_loss: 1.9198 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0773 - accuracy: 1.0000 - val_loss: 1.9910 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1413 - accuracy: 0.9773 - val_loss: 1.9298 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0724 - accuracy: 1.0000 - val_loss: 1.8270 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1709 - accuracy: 1.0000 - val_loss: 1.7699 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1427 - accuracy: 1.0000 - val_loss: 1.6574 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0471 - accuracy: 1.0000 - val_loss: 1.5369 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1450 - accuracy: 1.0000 - val_loss: 1.4823 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0628 - accuracy: 1.0000 - val_loss: 1.4428 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0106 - accuracy: 1.0000 - val_loss: 1.4095 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0384 - accuracy: 1.0000 - val_loss: 1.3762 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9741 - accuracy: 1.0000 - val_loss: 1.3473 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9697 - accuracy: 1.0000 - val_loss: 1.3192 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0131 - accuracy: 1.0000 - val_loss: 1.2920 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 1.0093 - accuracy: 1.0000 - val_loss: 1.2716 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9805 - accuracy: 1.0000 - val_loss: 1.2463 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9536 - accuracy: 1.0000 - val_loss: 1.2223 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9038 - accuracy: 1.0000 - val_loss: 1.2020 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8922 - accuracy: 1.0000 - val_loss: 1.1807 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8723 - accuracy: 1.0000 - val_loss: 1.1667 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9936 - accuracy: 1.0000 - val_loss: 1.1575 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9077 - accuracy: 1.0000 - val_loss: 1.1391 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8402 - accuracy: 1.0000 - val_loss: 1.1148 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8226 - accuracy: 1.0000 - val_loss: 1.0878 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8075 - accuracy: 1.0000 - val_loss: 1.0632 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8791 - accuracy: 1.0000 - val_loss: 1.0434 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8540 - accuracy: 1.0000 - val_loss: 1.0278 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7821 - accuracy: 1.0000 - val_loss: 1.0174 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.9284 - accuracy: 1.0000 - val_loss: 1.0116 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7574 - accuracy: 1.0000 - val_loss: 1.0479 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8464 - accuracy: 1.0000 - val_loss: 1.0869 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8876 - accuracy: 1.0000 - val_loss: 1.0938 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7313 - accuracy: 1.0000 - val_loss: 1.0387 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7259 - accuracy: 1.0000 - val_loss: 0.9936 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7193 - accuracy: 1.0000 - val_loss: 0.9701 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7079 - accuracy: 1.0000 - val_loss: 0.9650 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7749 - accuracy: 1.0000 - val_loss: 0.9666 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7417 - accuracy: 1.0000 - val_loss: 0.9674 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6793 - accuracy: 1.0000 - val_loss: 0.9622 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7452 - accuracy: 1.0000 - val_loss: 0.9526 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7803 - accuracy: 1.0000 - val_loss: 0.9420 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7051 - accuracy: 1.0000 - val_loss: 0.9269 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7380 - accuracy: 1.0000 - val_loss: 0.9149 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7375 - accuracy: 1.0000 - val_loss: 0.9078 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.6659 - accuracy: 1.0000 - val_loss: 0.9037 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6917 - accuracy: 1.0000 - val_loss: 0.8993 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6782 - accuracy: 1.0000 - val_loss: 0.8919 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7630 - accuracy: 1.0000 - val_loss: 0.8900 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6120 - accuracy: 1.0000 - val_loss: 0.8745 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6875 - accuracy: 1.0000 - val_loss: 0.8740 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6350 - accuracy: 1.0000 - val_loss: 0.9020 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.8853 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.6264 - accuracy: 1.0000 - val_loss: 0.8511 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss improved from 0.82864 to 0.82616, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.6650 - accuracy: 1.0000 - val_loss: 0.8262 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.82616\n",
      "2/2 - 20s - loss: 0.5740 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss improved from 0.82616 to 0.81672, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.8167 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5720 - accuracy: 1.0000 - val_loss: 1.2585 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6166 - accuracy: 0.9773 - val_loss: 1.2214 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6112 - accuracy: 0.9773 - val_loss: 1.7733 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.3359 - accuracy: 0.8636 - val_loss: 2.2089 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.2774 - accuracy: 0.7727 - val_loss: 1.3418 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.9889 - accuracy: 0.7955 - val_loss: 1.3810 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.1751 - accuracy: 0.8409 - val_loss: 1.7032 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 1.5100 - accuracy: 0.8182 - val_loss: 1.5636 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7769 - accuracy: 0.9773 - val_loss: 1.3042 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7883 - accuracy: 1.0000 - val_loss: 1.2864 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7236 - accuracy: 1.0000 - val_loss: 1.3165 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7570 - accuracy: 1.0000 - val_loss: 1.3463 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7279 - accuracy: 1.0000 - val_loss: 1.3728 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7318 - accuracy: 1.0000 - val_loss: 1.3846 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7657 - accuracy: 1.0000 - val_loss: 1.3922 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.7992 - accuracy: 1.0000 - val_loss: 1.3968 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7519 - accuracy: 1.0000 - val_loss: 1.3938 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6983 - accuracy: 1.0000 - val_loss: 1.3849 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7421 - accuracy: 1.0000 - val_loss: 1.3691 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7473 - accuracy: 1.0000 - val_loss: 1.3577 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.8124 - accuracy: 1.0000 - val_loss: 1.3419 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6910 - accuracy: 1.0000 - val_loss: 1.3360 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6996 - accuracy: 1.0000 - val_loss: 1.3263 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6897 - accuracy: 1.0000 - val_loss: 1.3098 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7044 - accuracy: 1.0000 - val_loss: 1.2972 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6286 - accuracy: 1.0000 - val_loss: 1.2813 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6211 - accuracy: 1.0000 - val_loss: 1.2622 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6120 - accuracy: 1.0000 - val_loss: 1.2443 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6666 - accuracy: 1.0000 - val_loss: 1.2257 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5957 - accuracy: 1.0000 - val_loss: 1.2112 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6249 - accuracy: 1.0000 - val_loss: 1.1962 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6909 - accuracy: 1.0000 - val_loss: 1.1839 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7134 - accuracy: 1.0000 - val_loss: 1.1725 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5820 - accuracy: 1.0000 - val_loss: 1.1622 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6803 - accuracy: 1.0000 - val_loss: 1.1629 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6031 - accuracy: 1.0000 - val_loss: 1.1700 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6803 - accuracy: 1.0000 - val_loss: 1.1600 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.5840 - accuracy: 1.0000 - val_loss: 1.1387 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5437 - accuracy: 1.0000 - val_loss: 1.1252 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5717 - accuracy: 1.0000 - val_loss: 1.1111 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5364 - accuracy: 1.0000 - val_loss: 1.1035 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.5258 - accuracy: 1.0000 - val_loss: 1.0966 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5608 - accuracy: 1.0000 - val_loss: 1.0885 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5676 - accuracy: 1.0000 - val_loss: 1.0817 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5233 - accuracy: 1.0000 - val_loss: 1.0762 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5121 - accuracy: 1.0000 - val_loss: 1.0806 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5040 - accuracy: 1.0000 - val_loss: 1.0849 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5289 - accuracy: 1.0000 - val_loss: 1.0829 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.5306 - accuracy: 1.0000 - val_loss: 1.0787 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5467 - accuracy: 1.0000 - val_loss: 1.0709 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5448 - accuracy: 1.0000 - val_loss: 1.0631 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4916 - accuracy: 1.0000 - val_loss: 1.0525 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5509 - accuracy: 1.0000 - val_loss: 1.0414 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4842 - accuracy: 1.0000 - val_loss: 1.0310 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6044 - accuracy: 1.0000 - val_loss: 1.0225 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5225 - accuracy: 1.0000 - val_loss: 1.0146 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4739 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5132 - accuracy: 1.0000 - val_loss: 1.0035 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5114 - accuracy: 1.0000 - val_loss: 0.9993 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4610 - accuracy: 1.0000 - val_loss: 0.9958 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4979 - accuracy: 1.0000 - val_loss: 0.9937 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4943 - accuracy: 1.0000 - val_loss: 0.9918 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4510 - accuracy: 1.0000 - val_loss: 0.9906 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4914 - accuracy: 1.0000 - val_loss: 0.9892 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4892 - accuracy: 1.0000 - val_loss: 0.9855 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4749 - accuracy: 1.0000 - val_loss: 0.9794 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4385 - accuracy: 1.0000 - val_loss: 0.9736 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4364 - accuracy: 1.0000 - val_loss: 0.9622 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4292 - accuracy: 1.0000 - val_loss: 0.9497 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4698 - accuracy: 1.0000 - val_loss: 0.9396 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4288 - accuracy: 1.0000 - val_loss: 0.9350 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5168 - accuracy: 1.0000 - val_loss: 0.9343 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4195 - accuracy: 1.0000 - val_loss: 0.9319 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4166 - accuracy: 1.0000 - val_loss: 0.9280 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4456 - accuracy: 1.0000 - val_loss: 0.9244 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4554 - accuracy: 1.0000 - val_loss: 0.9219 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4044 - accuracy: 1.0000 - val_loss: 0.9189 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4060 - accuracy: 1.0000 - val_loss: 0.9118 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4017 - accuracy: 1.0000 - val_loss: 0.9043 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4382 - accuracy: 1.0000 - val_loss: 0.8973 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4358 - accuracy: 1.0000 - val_loss: 0.8955 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.3948 - accuracy: 1.0000 - val_loss: 0.9016 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.81672\n",
      "2/2 - 22s - loss: 0.4316 - accuracy: 1.0000 - val_loss: 0.9126 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4404 - accuracy: 1.0000 - val_loss: 0.9250 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4217 - accuracy: 1.0000 - val_loss: 0.9313 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4286 - accuracy: 1.0000 - val_loss: 0.9301 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3781 - accuracy: 1.0000 - val_loss: 0.9191 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4036 - accuracy: 1.0000 - val_loss: 0.8982 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3966 - accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3721 - accuracy: 1.0000 - val_loss: 0.8676 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4573 - accuracy: 1.0000 - val_loss: 0.8598 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3656 - accuracy: 1.0000 - val_loss: 0.8558 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3654 - accuracy: 1.0000 - val_loss: 0.8540 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3605 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3580 - accuracy: 1.0000 - val_loss: 0.8847 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3849 - accuracy: 1.0000 - val_loss: 0.8989 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3565 - accuracy: 1.0000 - val_loss: 0.9004 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3863 - accuracy: 1.0000 - val_loss: 0.8834 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4434 - accuracy: 1.0000 - val_loss: 5.4565 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 2.4289 - accuracy: 0.7500 - val_loss: 2.3055 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 1.5117 - accuracy: 0.7500 - val_loss: 1.0511 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4389 - accuracy: 0.9318 - val_loss: 1.5448 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6890 - accuracy: 0.9091 - val_loss: 2.6497 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 2.2237 - accuracy: 0.7727 - val_loss: 2.0479 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4655 - accuracy: 1.0000 - val_loss: 1.9882 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4712 - accuracy: 0.9773 - val_loss: 1.5476 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4398 - accuracy: 1.0000 - val_loss: 1.1877 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4305 - accuracy: 1.0000 - val_loss: 1.1122 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4149 - accuracy: 1.0000 - val_loss: 1.1378 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4063 - accuracy: 1.0000 - val_loss: 1.1841 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4395 - accuracy: 1.0000 - val_loss: 1.4746 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4805 - accuracy: 1.0000 - val_loss: 1.3403 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4093 - accuracy: 1.0000 - val_loss: 1.3806 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4042 - accuracy: 1.0000 - val_loss: 1.4454 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4806 - accuracy: 1.0000 - val_loss: 1.4847 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4361 - accuracy: 1.0000 - val_loss: 1.5081 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3850 - accuracy: 1.0000 - val_loss: 1.5062 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4204 - accuracy: 1.0000 - val_loss: 1.4936 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4135 - accuracy: 1.0000 - val_loss: 1.4758 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4045 - accuracy: 1.0000 - val_loss: 1.4577 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4008 - accuracy: 1.0000 - val_loss: 1.4308 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3854 - accuracy: 1.0000 - val_loss: 1.4042 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3622 - accuracy: 1.0000 - val_loss: 1.3753 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4079 - accuracy: 1.0000 - val_loss: 1.3525 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4273 - accuracy: 1.0000 - val_loss: 1.3265 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3491 - accuracy: 1.0000 - val_loss: 1.3020 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3795 - accuracy: 1.0000 - val_loss: 1.2804 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3770 - accuracy: 1.0000 - val_loss: 1.2505 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3695 - accuracy: 0.9773 - val_loss: 1.6178 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 1.9319 - accuracy: 0.8636 - val_loss: 2.6787 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 2.7819 - accuracy: 0.7045 - val_loss: 1.3174 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.1772 - accuracy: 0.7273 - val_loss: 1.5650 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.0724 - accuracy: 0.8636 - val_loss: 1.1737 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4415 - accuracy: 0.9545 - val_loss: 1.5253 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4122 - accuracy: 1.0000 - val_loss: 1.7718 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3823 - accuracy: 1.0000 - val_loss: 1.8601 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3999 - accuracy: 1.0000 - val_loss: 1.8770 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4154 - accuracy: 1.0000 - val_loss: 1.8678 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4462 - accuracy: 1.0000 - val_loss: 1.8583 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4148 - accuracy: 1.0000 - val_loss: 1.8322 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4454 - accuracy: 1.0000 - val_loss: 1.8115 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3863 - accuracy: 1.0000 - val_loss: 1.7807 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4161 - accuracy: 1.0000 - val_loss: 1.7550 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4047 - accuracy: 1.0000 - val_loss: 1.7438 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4044 - accuracy: 1.0000 - val_loss: 1.7490 - val_accuracy: 0.7500 - 21s/epoch - 10s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4033 - accuracy: 1.0000 - val_loss: 1.7299 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3924 - accuracy: 1.0000 - val_loss: 1.7112 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3680 - accuracy: 1.0000 - val_loss: 1.6761 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3863 - accuracy: 1.0000 - val_loss: 1.6428 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3840 - accuracy: 1.0000 - val_loss: 1.6157 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3546 - accuracy: 1.0000 - val_loss: 1.5906 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3527 - accuracy: 1.0000 - val_loss: 1.5687 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3730 - accuracy: 1.0000 - val_loss: 1.5423 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3720 - accuracy: 1.0000 - val_loss: 1.5121 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3424 - accuracy: 1.0000 - val_loss: 1.4766 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3672 - accuracy: 1.0000 - val_loss: 1.4390 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3340 - accuracy: 1.0000 - val_loss: 1.4057 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3600 - accuracy: 1.0000 - val_loss: 1.3778 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3545 - accuracy: 1.0000 - val_loss: 1.3527 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3471 - accuracy: 1.0000 - val_loss: 1.3316 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3942 - accuracy: 1.0000 - val_loss: 1.3089 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3207 - accuracy: 1.0000 - val_loss: 1.2901 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3177 - accuracy: 1.0000 - val_loss: 1.2717 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3141 - accuracy: 1.0000 - val_loss: 1.2560 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3204 - accuracy: 1.0000 - val_loss: 1.2417 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3144 - accuracy: 1.0000 - val_loss: 1.2373 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3271 - accuracy: 1.0000 - val_loss: 1.2335 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3076 - accuracy: 1.0000 - val_loss: 1.2269 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3075 - accuracy: 1.0000 - val_loss: 1.2168 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3045 - accuracy: 1.0000 - val_loss: 1.2029 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3001 - accuracy: 1.0000 - val_loss: 1.1845 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3048 - accuracy: 1.0000 - val_loss: 1.1650 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3584 - accuracy: 1.0000 - val_loss: 1.1488 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.2979 - accuracy: 1.0000 - val_loss: 1.1346 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3129 - accuracy: 1.0000 - val_loss: 1.1125 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3224 - accuracy: 1.0000 - val_loss: 1.0881 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.2921 - accuracy: 1.0000 - val_loss: 1.0705 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3023 - accuracy: 1.0000 - val_loss: 1.0557 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3084 - accuracy: 1.0000 - val_loss: 1.0433 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "Classification accuracy: 0.495845 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34468523 0.65531474]\n",
      " [0.4223048  0.5776952 ]\n",
      " [0.66058546 0.33941454]\n",
      " [0.82226074 0.1777392 ]\n",
      " [0.8576841  0.142316  ]\n",
      " [0.79356056 0.2064394 ]\n",
      " [0.43697017 0.5630298 ]\n",
      " [0.23311326 0.7668868 ]\n",
      " [0.06507578 0.93492424]\n",
      " [0.48482734 0.5151727 ]\n",
      " [0.5046285  0.49537155]\n",
      " [0.68665516 0.3133448 ]\n",
      " [0.46980935 0.5301907 ]\n",
      " [0.55152243 0.4484776 ]\n",
      " [0.11421819 0.8857818 ]\n",
      " [0.25776443 0.74223554]\n",
      " [0.45055872 0.5494413 ]\n",
      " [0.7503406  0.24965943]\n",
      " [0.5944622  0.40553778]]\n",
      "[1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[7 4]\n",
      " [2 6]]\n",
      "[68.42 77.78 60.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with init data, 300 epochs, softmax\n",
    "probs_TSGL_init = np.array([[0.34468523, 0.65531474],\n",
    "                            [0.4223048,  0.5776952 ],\n",
    "                            [0.66058546, 0.33941454],\n",
    "                            [0.82226074, 0.1777392 ],\n",
    "                            [0.85768410, 0.142316  ],\n",
    "                            [0.79356056, 0.2064394 ],\n",
    "                            [0.43697017, 0.5630298 ],\n",
    "                            [0.23311326, 0.7668868 ],\n",
    "                            [0.06507578, 0.93492424],\n",
    "                            [0.48482734, 0.5151727 ],\n",
    "                            [0.50462850, 0.49537155],\n",
    "                            [0.68665516, 0.3133448 ],\n",
    "                            [0.46980935, 0.5301907 ],\n",
    "                            [0.55152243, 0.4484776 ],\n",
    "                            [0.11421819, 0.8857818 ],\n",
    "                            [0.25776443, 0.74223554],\n",
    "                            [0.45055872, 0.5494413 ],\n",
    "                            [0.75034060, 0.24965943],\n",
    "                            [0.59446220, 0.40553778]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 8.09153, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 14s - loss: 7.8203 - accuracy: 0.4773 - val_loss: 8.0915 - val_accuracy: 0.6875 - 14s/epoch - 7s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 8.09153 to 1.37230, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 11s - loss: 12.2146 - accuracy: 0.5000 - val_loss: 1.3723 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 0.3350 - accuracy: 0.8636 - val_loss: 2.6437 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 11.8041 - accuracy: 0.6591 - val_loss: 10.2536 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 3.7748 - accuracy: 0.7727 - val_loss: 9.0993 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 1.9966 - accuracy: 0.7500 - val_loss: 8.0614 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 1.37230 to 1.26682, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6301 - accuracy: 0.8636 - val_loss: 1.2668 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5868 - accuracy: 0.9318 - val_loss: 11.3714 - val_accuracy: 0.6250 - 11s/epoch - 5s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.1444 - accuracy: 0.7045 - val_loss: 20.2057 - val_accuracy: 0.6250 - 11s/epoch - 5s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4914 - accuracy: 0.7955 - val_loss: 13.1583 - val_accuracy: 0.6250 - 11s/epoch - 5s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4072 - accuracy: 0.8409 - val_loss: 3.6578 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.8117 - accuracy: 0.8636 - val_loss: 5.8793 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.1059e-04 - accuracy: 1.0000 - val_loss: 7.0600 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.2872e-05 - accuracy: 1.0000 - val_loss: 7.7414 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0161 - accuracy: 1.0000 - val_loss: 8.5172 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 8.8442 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 8.6818 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 9.0005 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8203e-05 - accuracy: 1.0000 - val_loss: 8.9204 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3175e-04 - accuracy: 1.0000 - val_loss: 8.7869 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0755 - accuracy: 0.9545 - val_loss: 6.8538 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0280e-04 - accuracy: 1.0000 - val_loss: 4.1668 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2317e-05 - accuracy: 1.0000 - val_loss: 2.3026 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.0997 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7641 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.3052 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7746e-05 - accuracy: 1.0000 - val_loss: 3.6790 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.8392e-05 - accuracy: 1.0000 - val_loss: 3.9137 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.7343e-06 - accuracy: 1.0000 - val_loss: 4.0930 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7437e-04 - accuracy: 1.0000 - val_loss: 4.1246 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.4624e-05 - accuracy: 1.0000 - val_loss: 4.2337 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.8992e-05 - accuracy: 1.0000 - val_loss: 4.3333 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.9389e-05 - accuracy: 1.0000 - val_loss: 4.4658 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.6551e-05 - accuracy: 1.0000 - val_loss: 4.4993 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.2754e-05 - accuracy: 1.0000 - val_loss: 4.5091 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.6058e-04 - accuracy: 1.0000 - val_loss: 4.5108 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4780e-04 - accuracy: 1.0000 - val_loss: 4.4206 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.0822e-04 - accuracy: 1.0000 - val_loss: 4.4161 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6139e-05 - accuracy: 1.0000 - val_loss: 4.2555 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.5661e-06 - accuracy: 1.0000 - val_loss: 4.1531 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.5603e-04 - accuracy: 1.0000 - val_loss: 4.0003 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.4001e-06 - accuracy: 1.0000 - val_loss: 3.8715 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.6380e-05 - accuracy: 1.0000 - val_loss: 3.7605 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0580e-05 - accuracy: 1.0000 - val_loss: 3.7036 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.1831e-06 - accuracy: 1.0000 - val_loss: 3.5900 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0991e-05 - accuracy: 1.0000 - val_loss: 3.5034 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7391e-06 - accuracy: 1.0000 - val_loss: 3.3993 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4021e-05 - accuracy: 1.0000 - val_loss: 3.3614 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.2700e-06 - accuracy: 1.0000 - val_loss: 3.2842 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.0929e-05 - accuracy: 1.0000 - val_loss: 3.3054 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.5620e-06 - accuracy: 1.0000 - val_loss: 3.2620 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.1162e-06 - accuracy: 1.0000 - val_loss: 3.2385 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3844e-06 - accuracy: 1.0000 - val_loss: 3.1537 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1142e-05 - accuracy: 1.0000 - val_loss: 3.1087 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.2121e-06 - accuracy: 1.0000 - val_loss: 3.0678 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8342e-06 - accuracy: 1.0000 - val_loss: 3.0777 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0019e-06 - accuracy: 1.0000 - val_loss: 3.0956 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.5517e-06 - accuracy: 1.0000 - val_loss: 3.0525 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4384e-06 - accuracy: 1.0000 - val_loss: 3.0695 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.1972e-06 - accuracy: 1.0000 - val_loss: 3.0689 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5633e-06 - accuracy: 1.0000 - val_loss: 3.0752 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4695e-05 - accuracy: 1.0000 - val_loss: 3.1076 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7583e-06 - accuracy: 1.0000 - val_loss: 3.1152 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.2832e-06 - accuracy: 1.0000 - val_loss: 3.0205 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9642e-06 - accuracy: 1.0000 - val_loss: 3.0173 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0072e-06 - accuracy: 1.0000 - val_loss: 3.0043 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.1631e-06 - accuracy: 1.0000 - val_loss: 3.0301 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.7702e-06 - accuracy: 1.0000 - val_loss: 2.9666 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8948e-05 - accuracy: 1.0000 - val_loss: 2.9496 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9615e-06 - accuracy: 1.0000 - val_loss: 2.9547 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0301e-05 - accuracy: 1.0000 - val_loss: 2.9727 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.0636e-06 - accuracy: 1.0000 - val_loss: 2.9959 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.5018e-05 - accuracy: 1.0000 - val_loss: 2.9783 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.9010e-06 - accuracy: 1.0000 - val_loss: 2.9858 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.5788e-06 - accuracy: 1.0000 - val_loss: 3.0194 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2896e-06 - accuracy: 1.0000 - val_loss: 3.0285 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2869e-06 - accuracy: 1.0000 - val_loss: 3.0017 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.9595e-06 - accuracy: 1.0000 - val_loss: 2.9777 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.0393e-07 - accuracy: 1.0000 - val_loss: 3.0007 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4413e-06 - accuracy: 1.0000 - val_loss: 2.9549 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8694e-06 - accuracy: 1.0000 - val_loss: 2.9336 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.9764e-06 - accuracy: 1.0000 - val_loss: 2.9357 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.6135e-06 - accuracy: 1.0000 - val_loss: 2.9319 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.9423e-06 - accuracy: 1.0000 - val_loss: 2.9340 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4681e-06 - accuracy: 1.0000 - val_loss: 2.9443 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4735e-06 - accuracy: 1.0000 - val_loss: 2.9449 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.8054e-06 - accuracy: 1.0000 - val_loss: 2.9065 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4847e-06 - accuracy: 1.0000 - val_loss: 2.8372 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.3441e-06 - accuracy: 1.0000 - val_loss: 2.8268 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7851e-06 - accuracy: 1.0000 - val_loss: 2.8344 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0295e-06 - accuracy: 1.0000 - val_loss: 2.8560 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7583e-06 - accuracy: 1.0000 - val_loss: 2.8612 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.3973e-06 - accuracy: 1.0000 - val_loss: 2.7927 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.9653e-07 - accuracy: 1.0000 - val_loss: 2.8077 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3574e-06 - accuracy: 1.0000 - val_loss: 2.8145 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.1779e-06 - accuracy: 1.0000 - val_loss: 2.8140 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.5199e-06 - accuracy: 1.0000 - val_loss: 2.8430 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6987e-06 - accuracy: 1.0000 - val_loss: 2.8388 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3462e-06 - accuracy: 1.0000 - val_loss: 2.9189 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4221e-06 - accuracy: 1.0000 - val_loss: 2.9250 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.5343e-07 - accuracy: 1.0000 - val_loss: 2.8637 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9019e-06 - accuracy: 1.0000 - val_loss: 2.8715 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.6131e-07 - accuracy: 1.0000 - val_loss: 2.8610 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7502e-06 - accuracy: 1.0000 - val_loss: 2.8744 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.8878e-06 - accuracy: 1.0000 - val_loss: 2.8805 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5928e-06 - accuracy: 1.0000 - val_loss: 2.9053 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.7979e-07 - accuracy: 1.0000 - val_loss: 2.9275 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1268e-06 - accuracy: 1.0000 - val_loss: 2.9529 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0268e-06 - accuracy: 1.0000 - val_loss: 2.9282 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0130e-06 - accuracy: 1.0000 - val_loss: 2.8526 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.8938e-06 - accuracy: 1.0000 - val_loss: 2.8321 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7986e-06 - accuracy: 1.0000 - val_loss: 2.9094 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5115e-06 - accuracy: 1.0000 - val_loss: 2.8095 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.4976e-06 - accuracy: 1.0000 - val_loss: 2.7421 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.8288e-06 - accuracy: 1.0000 - val_loss: 2.7854 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3492e-06 - accuracy: 1.0000 - val_loss: 2.8464 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0405e-05 - accuracy: 1.0000 - val_loss: 2.8325 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7990e-06 - accuracy: 1.0000 - val_loss: 2.8115 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.4825e-07 - accuracy: 1.0000 - val_loss: 2.7810 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.2460e-06 - accuracy: 1.0000 - val_loss: 2.7953 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.6637e-05 - accuracy: 1.0000 - val_loss: 2.8530 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.1845e-07 - accuracy: 1.0000 - val_loss: 2.8149 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8369e-06 - accuracy: 1.0000 - val_loss: 2.8291 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3983e-05 - accuracy: 1.0000 - val_loss: 2.7948 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.7805e-07 - accuracy: 1.0000 - val_loss: 2.7932 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.9124e-06 - accuracy: 1.0000 - val_loss: 2.8082 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7445e-06 - accuracy: 1.0000 - val_loss: 2.8040 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.1279e-07 - accuracy: 1.0000 - val_loss: 2.8065 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5034e-06 - accuracy: 1.0000 - val_loss: 2.7610 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3438e-06 - accuracy: 1.0000 - val_loss: 2.8000 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.1723e-07 - accuracy: 1.0000 - val_loss: 2.7975 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.7853e-05 - accuracy: 1.0000 - val_loss: 2.7866 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.7604e-06 - accuracy: 1.0000 - val_loss: 2.7340 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.8552e-06 - accuracy: 1.0000 - val_loss: 2.7551 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2761e-06 - accuracy: 1.0000 - val_loss: 2.7162 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0157e-06 - accuracy: 1.0000 - val_loss: 2.7487 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6391e-06 - accuracy: 1.0000 - val_loss: 2.6918 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.1586e-06 - accuracy: 1.0000 - val_loss: 2.6823 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7366e-06 - accuracy: 1.0000 - val_loss: 2.6690 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.4028e-06 - accuracy: 1.0000 - val_loss: 2.6410 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.5294e-06 - accuracy: 1.0000 - val_loss: 2.6049 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.8743e-07 - accuracy: 1.0000 - val_loss: 2.5412 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8315e-06 - accuracy: 1.0000 - val_loss: 2.5509 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.5072e-07 - accuracy: 1.0000 - val_loss: 2.5805 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.4489e-06 - accuracy: 1.0000 - val_loss: 2.6074 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3438e-06 - accuracy: 1.0000 - val_loss: 2.5511 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.7054e-06 - accuracy: 1.0000 - val_loss: 2.5274 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.9669e-06 - accuracy: 1.0000 - val_loss: 2.5448 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7420e-05 - accuracy: 1.0000 - val_loss: 2.5524 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.3964e-07 - accuracy: 1.0000 - val_loss: 2.5333 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1142e-05 - accuracy: 1.0000 - val_loss: 2.5364 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7475e-06 - accuracy: 1.0000 - val_loss: 2.5498 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.0164e-06 - accuracy: 1.0000 - val_loss: 2.5437 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7281e-06 - accuracy: 1.0000 - val_loss: 2.5065 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4302e-06 - accuracy: 1.0000 - val_loss: 2.4829 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.3939e-07 - accuracy: 1.0000 - val_loss: 2.5209 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.4582e-06 - accuracy: 1.0000 - val_loss: 2.5431 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7556e-06 - accuracy: 1.0000 - val_loss: 2.5459 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.4711e-06 - accuracy: 1.0000 - val_loss: 2.5503 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.4801e-05 - accuracy: 1.0000 - val_loss: 2.4662 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.1725e-06 - accuracy: 1.0000 - val_loss: 2.4383 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.7166e-07 - accuracy: 1.0000 - val_loss: 2.4525 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3543e-06 - accuracy: 1.0000 - val_loss: 2.4080 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.5982e-06 - accuracy: 1.0000 - val_loss: 2.4045 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.5362e-06 - accuracy: 1.0000 - val_loss: 2.3716 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2977e-06 - accuracy: 1.0000 - val_loss: 2.4183 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.6697e-07 - accuracy: 1.0000 - val_loss: 2.4424 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.9313e-06 - accuracy: 1.0000 - val_loss: 2.4172 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.7143e-06 - accuracy: 1.0000 - val_loss: 2.4609 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.8086e-06 - accuracy: 1.0000 - val_loss: 2.4693 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.4728e-07 - accuracy: 1.0000 - val_loss: 2.4778 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.1108e-06 - accuracy: 1.0000 - val_loss: 2.5137 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0079e-06 - accuracy: 1.0000 - val_loss: 2.5433 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0595e-06 - accuracy: 1.0000 - val_loss: 2.4751 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.4505e-07 - accuracy: 1.0000 - val_loss: 2.4982 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.8889e-07 - accuracy: 1.0000 - val_loss: 2.4958 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4251e-06 - accuracy: 1.0000 - val_loss: 2.4690 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.4210e-07 - accuracy: 1.0000 - val_loss: 2.4695 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.6378e-07 - accuracy: 1.0000 - val_loss: 2.4889 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0810e-06 - accuracy: 1.0000 - val_loss: 2.5148 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.4299e-06 - accuracy: 1.0000 - val_loss: 2.5099 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3384e-06 - accuracy: 1.0000 - val_loss: 2.5232 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0783e-06 - accuracy: 1.0000 - val_loss: 2.5373 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.0915e-06 - accuracy: 1.0000 - val_loss: 2.5478 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0620e-06 - accuracy: 1.0000 - val_loss: 2.5379 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.5023e-07 - accuracy: 1.0000 - val_loss: 2.4962 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9588e-06 - accuracy: 1.0000 - val_loss: 2.5238 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.8231e-06 - accuracy: 1.0000 - val_loss: 2.5571 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.7659e-07 - accuracy: 1.0000 - val_loss: 2.4878 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.8170e-06 - accuracy: 1.0000 - val_loss: 2.4759 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4007e-06 - accuracy: 1.0000 - val_loss: 2.4861 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.6673e-07 - accuracy: 1.0000 - val_loss: 2.5001 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2706e-06 - accuracy: 1.0000 - val_loss: 2.5452 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5931e-06 - accuracy: 1.0000 - val_loss: 2.5262 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0171e-07 - accuracy: 1.0000 - val_loss: 2.4887 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0024e-06 - accuracy: 1.0000 - val_loss: 2.5202 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9398e-06 - accuracy: 1.0000 - val_loss: 2.5301 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.7215e-07 - accuracy: 1.0000 - val_loss: 2.5223 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6770e-06 - accuracy: 1.0000 - val_loss: 2.5624 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.6223e-06 - accuracy: 1.0000 - val_loss: 2.5581 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.7510e-07 - accuracy: 1.0000 - val_loss: 2.5497 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.7383e-06 - accuracy: 1.0000 - val_loss: 2.4915 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0756e-06 - accuracy: 1.0000 - val_loss: 2.5142 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.0417e-07 - accuracy: 1.0000 - val_loss: 2.4813 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7908e-06 - accuracy: 1.0000 - val_loss: 2.4688 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3492e-06 - accuracy: 1.0000 - val_loss: 2.4452 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.8396e-06 - accuracy: 1.0000 - val_loss: 2.4473 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0712e-07 - accuracy: 1.0000 - val_loss: 2.4167 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.5540e-07 - accuracy: 1.0000 - val_loss: 2.3790 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1945e-07 - accuracy: 1.0000 - val_loss: 2.4283 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.4878e-05 - accuracy: 1.0000 - val_loss: 2.4337 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5114e-06 - accuracy: 1.0000 - val_loss: 2.4819 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.8341e-06 - accuracy: 1.0000 - val_loss: 2.4089 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.0688e-07 - accuracy: 1.0000 - val_loss: 2.4462 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.6920e-07 - accuracy: 1.0000 - val_loss: 2.4595 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.8201e-07 - accuracy: 1.0000 - val_loss: 2.4703 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.5849e-06 - accuracy: 1.0000 - val_loss: 2.5231 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7989e-06 - accuracy: 1.0000 - val_loss: 2.5421 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3001e-06 - accuracy: 1.0000 - val_loss: 2.5338 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.7757e-07 - accuracy: 1.0000 - val_loss: 2.5339 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3571e-06 - accuracy: 1.0000 - val_loss: 2.5528 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3002e-06 - accuracy: 1.0000 - val_loss: 2.5627 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.6180e-07 - accuracy: 1.0000 - val_loss: 2.5801 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2625e-06 - accuracy: 1.0000 - val_loss: 2.5536 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0984e-07 - accuracy: 1.0000 - val_loss: 2.5614 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5954e-06 - accuracy: 1.0000 - val_loss: 2.5766 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.5270e-07 - accuracy: 1.0000 - val_loss: 2.5511 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0994e-06 - accuracy: 1.0000 - val_loss: 2.5630 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7285e-06 - accuracy: 1.0000 - val_loss: 2.5476 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0441e-07 - accuracy: 1.0000 - val_loss: 2.5446 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.6635e-06 - accuracy: 1.0000 - val_loss: 2.5230 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.6527e-07 - accuracy: 1.0000 - val_loss: 2.5579 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4332e-06 - accuracy: 1.0000 - val_loss: 2.5166 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1108e-06 - accuracy: 1.0000 - val_loss: 2.5319 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0566e-07 - accuracy: 1.0000 - val_loss: 2.4943 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0458e-06 - accuracy: 1.0000 - val_loss: 2.4897 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.3964e-07 - accuracy: 1.0000 - val_loss: 2.5115 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8911e-06 - accuracy: 1.0000 - val_loss: 2.4826 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2598e-06 - accuracy: 1.0000 - val_loss: 2.4870 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.2314e-07 - accuracy: 1.0000 - val_loss: 2.5259 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.2831e-07 - accuracy: 1.0000 - val_loss: 2.4931 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2083e-06 - accuracy: 1.0000 - val_loss: 2.4960 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7364e-07 - accuracy: 1.0000 - val_loss: 2.5309 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2517e-06 - accuracy: 1.0000 - val_loss: 2.5312 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4004e-06 - accuracy: 1.0000 - val_loss: 2.5564 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5253e-06 - accuracy: 1.0000 - val_loss: 2.4613 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.8690e-06 - accuracy: 1.0000 - val_loss: 2.4397 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7393e-06 - accuracy: 1.0000 - val_loss: 2.4093 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.8760e-06 - accuracy: 1.0000 - val_loss: 2.3975 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0160e-06 - accuracy: 1.0000 - val_loss: 2.4433 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1189e-06 - accuracy: 1.0000 - val_loss: 2.4828 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5307e-06 - accuracy: 1.0000 - val_loss: 2.4540 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0861e-06 - accuracy: 1.0000 - val_loss: 2.4463 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5551e-06 - accuracy: 1.0000 - val_loss: 2.4510 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.6721e-07 - accuracy: 1.0000 - val_loss: 2.4419 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.2043e-07 - accuracy: 1.0000 - val_loss: 2.4728 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.2028e-06 - accuracy: 1.0000 - val_loss: 2.5026 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.8864e-07 - accuracy: 1.0000 - val_loss: 2.5152 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9507e-07 - accuracy: 1.0000 - val_loss: 2.5658 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.9924e-07 - accuracy: 1.0000 - val_loss: 2.5236 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1105e-06 - accuracy: 1.0000 - val_loss: 2.5205 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.9063e-07 - accuracy: 1.0000 - val_loss: 2.5600 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.2560e-07 - accuracy: 1.0000 - val_loss: 2.5443 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.1501e-07 - accuracy: 1.0000 - val_loss: 2.5249 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.2595e-06 - accuracy: 1.0000 - val_loss: 2.4819 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4166e-06 - accuracy: 1.0000 - val_loss: 2.4922 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.2351e-06 - accuracy: 1.0000 - val_loss: 2.5342 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.8719e-07 - accuracy: 1.0000 - val_loss: 2.4388 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3763e-06 - accuracy: 1.0000 - val_loss: 2.4650 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.5812e-07 - accuracy: 1.0000 - val_loss: 2.4663 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1620e-06 - accuracy: 1.0000 - val_loss: 2.4957 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7635e-07 - accuracy: 1.0000 - val_loss: 2.5308 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.6241e-06 - accuracy: 1.0000 - val_loss: 2.5005 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2219e-06 - accuracy: 1.0000 - val_loss: 2.5324 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6364e-06 - accuracy: 1.0000 - val_loss: 2.5005 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.5638e-07 - accuracy: 1.0000 - val_loss: 2.4458 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.4210e-07 - accuracy: 1.0000 - val_loss: 2.4564 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.7535e-07 - accuracy: 1.0000 - val_loss: 2.4989 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.9558e-06 - accuracy: 1.0000 - val_loss: 2.5319 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4221e-06 - accuracy: 1.0000 - val_loss: 2.4773 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1921e-06 - accuracy: 1.0000 - val_loss: 2.5112 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7688e-06 - accuracy: 1.0000 - val_loss: 2.5023 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.9851e-07 - accuracy: 1.0000 - val_loss: 2.4902 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3303e-06 - accuracy: 1.0000 - val_loss: 2.4974 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3384e-06 - accuracy: 1.0000 - val_loss: 2.4948 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.3107e-06 - accuracy: 1.0000 - val_loss: 2.4917 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.3717e-07 - accuracy: 1.0000 - val_loss: 2.5020 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.1671e-06 - accuracy: 1.0000 - val_loss: 2.4930 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4793e-06 - accuracy: 1.0000 - val_loss: 2.4342 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1891e-06 - accuracy: 1.0000 - val_loss: 2.4242 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0563e-06 - accuracy: 1.0000 - val_loss: 2.4340 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2029e-06 - accuracy: 1.0000 - val_loss: 2.4360 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0051e-06 - accuracy: 1.0000 - val_loss: 2.4509 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.1574e-07 - accuracy: 1.0000 - val_loss: 2.4543 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.0910e-07 - accuracy: 1.0000 - val_loss: 2.4365 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.9285e-07 - accuracy: 1.0000 - val_loss: 2.4189 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0837e-06 - accuracy: 1.0000 - val_loss: 2.3982 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3113e-06 - accuracy: 1.0000 - val_loss: 2.4308 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2977e-06 - accuracy: 1.0000 - val_loss: 2.4475 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7712e-05 - accuracy: 1.0000 - val_loss: 2.4801 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Classification accuracy: 0.512465 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.95184946e-01 3.04815054e-01]\n",
      " [4.21892613e-01 5.78107417e-01]\n",
      " [7.89708734e-01 2.10291266e-01]\n",
      " [9.99535143e-01 4.64851793e-04]\n",
      " [1.59405246e-01 8.40594828e-01]\n",
      " [9.02964830e-01 9.70351920e-02]\n",
      " [1.18944913e-01 8.81055117e-01]\n",
      " [1.71721101e-01 8.28278899e-01]\n",
      " [9.67139781e-01 3.28601785e-02]\n",
      " [7.46518612e-01 2.53481418e-01]\n",
      " [2.98798531e-01 7.01201379e-01]\n",
      " [1.78000614e-10 9.99999940e-01]\n",
      " [8.16536665e-01 1.83463335e-01]\n",
      " [2.43162528e-01 7.56837428e-01]\n",
      " [1.04919985e-01 8.95080090e-01]\n",
      " [9.93469775e-01 6.53020665e-03]\n",
      " [9.06261265e-01 9.37386826e-02]\n",
      " [6.60291553e-01 3.39708477e-01]\n",
      " [7.47849047e-01 2.52150923e-01]]\n",
      "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[7 4]\n",
      " [4 4]]\n",
      "[57.89 63.64 50.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "C:\\Users\\annej\\AppData\\Local\\Temp\\ipykernel_14032\\2687471855.py:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  [2.9218609e-02, 9.7078133e-01]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14032\\2687471855.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m7.1874863e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.8125137e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m9.9979991e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0014797e-04\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                             \u001b[1;33m[\u001b[0m\u001b[1;36m2.9218609e-02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9.7078133e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m9.9454159e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.4584546e-03\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m5.5008806e-02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9.4499117e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# init data, 300 epochs, softmax activation\n",
    "probs_Deep_init_soft = np.array([[7.0045400e-01, 2.9954603e-01],\n",
    "                            [7.8413427e-01, 2.1586572e-01],\n",
    "                            [7.1874863e-01, 2.8125137e-01],\n",
    "                            [9.9979991e-01, 2.0014797e-04],\n",
    "                            [2.9218609e-02, 9.7078133e-01]\n",
    "                            [9.9454159e-01, 5.4584546e-03],\n",
    "                            [5.5008806e-02, 9.4499117e-01],\n",
    "                            [9.6587259e-01, 3.4127403e-02],\n",
    "                            [6.3271725e-01, 3.6728275e-01],\n",
    "                            [9.8455411e-01, 1.5445878e-02],\n",
    "                            [8.1000119e-01, 1.8999882e-01],\n",
    "                            [9.9752015e-01, 2.4798173e-03],\n",
    "                            [9.8586375e-01, 1.4136297e-02],\n",
    "                            [4.3618846e-01, 5.6381154e-01],\n",
    "                            [1.4959927e-01, 8.5040075e-01],\n",
    "                            [9.5763546e-01, 4.2364582e-02],\n",
    "                            [9.9997401e-01, 2.5972882e-05],\n",
    "                            [9.9117404e-01, 8.8259308e-03],\n",
    "                            [9.9719328e-01, 2.8067816e-03]])\n",
    "\n",
    "\n",
    "probs_Deep_init_sigm = np.array([[6.95184946e-01 3.04815054e-01]\n",
    " [4.21892613e-01 5.78107417e-01]\n",
    " [7.89708734e-01 2.10291266e-01]\n",
    " [9.99535143e-01 4.64851793e-04]\n",
    " [1.59405246e-01 8.40594828e-01]\n",
    " [9.02964830e-01 9.70351920e-02]\n",
    " [1.18944913e-01 8.81055117e-01]\n",
    " [1.71721101e-01 8.28278899e-01]\n",
    " [9.67139781e-01 3.28601785e-02]\n",
    " [7.46518612e-01 2.53481418e-01]\n",
    " [2.98798531e-01 7.01201379e-01]\n",
    " [1.78000614e-10 9.99999940e-01]\n",
    " [8.16536665e-01 1.83463335e-01]\n",
    " [2.43162528e-01 7.56837428e-01]\n",
    " [1.04919985e-01 8.95080090e-01]\n",
    " [9.93469775e-01 6.53020665e-03]\n",
    " [9.06261265e-01 9.37386826e-02]\n",
    " [6.60291553e-01 3.39708477e-01]\n",
    " [7.47849047e-01 2.52150923e-01]]\n",
    "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
    "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 43.95264, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 10s - loss: 78.8688 - accuracy: 0.4318 - val_loss: 43.9526 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 86.1343 - accuracy: 0.5455 - val_loss: 149.1919 - val_accuracy: 0.4375 - 9s/epoch - 4s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 202.2070 - accuracy: 0.5000 - val_loss: 94.7610 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 131.3749 - accuracy: 0.5227 - val_loss: 96.1171 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 80.6944 - accuracy: 0.5227 - val_loss: 112.3041 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 123.3803 - accuracy: 0.5227 - val_loss: 92.6022 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 78.4258 - accuracy: 0.6136 - val_loss: 60.1742 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 43.95264 to 32.75097, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 37.4668 - accuracy: 0.5909 - val_loss: 32.7510 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 32.75097\n",
      "2/2 - 8s - loss: 10.1157 - accuracy: 0.7273 - val_loss: 39.9827 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 32.75097 to 18.14144, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 14.9758 - accuracy: 0.7955 - val_loss: 18.1414 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 18.14144\n",
      "2/2 - 9s - loss: 0.2918 - accuracy: 0.9318 - val_loss: 26.9031 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 18.14144 to 11.75048, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 0.6369 - accuracy: 0.9318 - val_loss: 11.7505 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 11.75048 to 5.23638, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.2364 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 13.6759 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 17.3170 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0734 - accuracy: 0.9773 - val_loss: 5.6160 - val_accuracy: 0.6250 - 9s/epoch - 4s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2106 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 6.6937e-06 - accuracy: 1.0000 - val_loss: 14.5230 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.1760 - accuracy: 0.9773 - val_loss: 14.6603 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.0862e-04 - accuracy: 1.0000 - val_loss: 14.4515 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 4.8248e-06 - accuracy: 1.0000 - val_loss: 14.5977 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.1406e-06 - accuracy: 1.0000 - val_loss: 14.6366 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 5.6895e-08 - accuracy: 1.0000 - val_loss: 14.5806 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.4129e-06 - accuracy: 1.0000 - val_loss: 14.3573 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.8667e-06 - accuracy: 1.0000 - val_loss: 14.3160 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 7.5860e-08 - accuracy: 1.0000 - val_loss: 14.3574 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.8887e-07 - accuracy: 1.0000 - val_loss: 14.2316 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.3777e-04 - accuracy: 1.0000 - val_loss: 14.2435 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.4928e-05 - accuracy: 1.0000 - val_loss: 13.5583 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.2007e-04 - accuracy: 1.0000 - val_loss: 13.1742 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.8075e-07 - accuracy: 1.0000 - val_loss: 12.2424 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.3644e-07 - accuracy: 1.0000 - val_loss: 11.7089 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.4088e-07 - accuracy: 1.0000 - val_loss: 11.1886 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.5876e-06 - accuracy: 1.0000 - val_loss: 10.8942 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.9967e-06 - accuracy: 1.0000 - val_loss: 10.4596 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.1540e-05 - accuracy: 1.0000 - val_loss: 10.0898 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 9.4825e-08 - accuracy: 1.0000 - val_loss: 10.0081 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.8989e-07 - accuracy: 1.0000 - val_loss: 9.8631 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.9802e-08 - accuracy: 1.0000 - val_loss: 9.8093 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 3.1970e-07 - accuracy: 1.0000 - val_loss: 9.7251 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 3.7388e-07 - accuracy: 1.0000 - val_loss: 9.5806 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 6.7461e-07 - accuracy: 1.0000 - val_loss: 9.4852 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0890e-04 - accuracy: 1.0000 - val_loss: 9.6460 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.1081e-06 - accuracy: 1.0000 - val_loss: 9.4020 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.5182e-04 - accuracy: 1.0000 - val_loss: 9.4752 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.4384e-08 - accuracy: 1.0000 - val_loss: 9.4618 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 9.5348 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.5908 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.7490e-04 - accuracy: 1.0000 - val_loss: 9.7065 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.5074 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2909 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2726 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7931e-06 - accuracy: 1.0000 - val_loss: 9.3026 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1273 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1026 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0024e-07 - accuracy: 1.0000 - val_loss: 9.1139 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1670 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.0145 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9424 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 8.9655 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9433 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9245 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9671 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9791 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8804 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7777 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6653 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8902 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0153 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9295 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.8300 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7876 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.6144 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 3.4731e-06 - accuracy: 1.0000 - val_loss: 8.8174 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8093 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7647 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.6120 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 4.5787e-07 - accuracy: 1.0000 - val_loss: 8.8001 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8321 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8810 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7359 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8110 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7339 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8234 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8674 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7895 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6460 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.8965e-08 - accuracy: 1.0000 - val_loss: 8.5151 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7676 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.0530e-05 - accuracy: 1.0000 - val_loss: 8.9580 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8350 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9693 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.3547e-08 - accuracy: 1.0000 - val_loss: 8.8560 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7519 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7127 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7903 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8287 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.8558e-06 - accuracy: 1.0000 - val_loss: 9.0365 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0554 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9630 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9653 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8779 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7542 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8986 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9681 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8959 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8613 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8340 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 3.2512e-08 - accuracy: 1.0000 - val_loss: 8.9947 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8548 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9198 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1150 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.1674e-08 - accuracy: 1.0000 - val_loss: 9.1509 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.0877 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0857 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9535 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0496 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1655 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0184 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0595 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0199 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.8965e-08 - accuracy: 1.0000 - val_loss: 8.7150 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.5471 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5566 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5877 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.5136 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4329 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5572 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5789 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6248 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7148 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5713 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.8243e-05 - accuracy: 1.0000 - val_loss: 8.7888 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6289 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5697 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4816 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.1674e-08 - accuracy: 1.0000 - val_loss: 8.3858 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2311 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.6256e-08 - accuracy: 1.0000 - val_loss: 8.0355 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 7.9159 - val_accuracy: 0.7500 - 8s/epoch - 4s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.0702 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.1583 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.1835 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3623 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4772 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5796 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6740 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7145 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7010 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.5664 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6989 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6751 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.8993 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9435 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8638 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7466 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6676 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7362 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6957 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7325 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7529 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6407 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6157 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5053 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7343 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.5932 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5362 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5821 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.4958 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.4555e-05 - accuracy: 1.0000 - val_loss: 8.7158 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7682 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6320 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-08 - accuracy: 1.0000 - val_loss: 8.8310 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8947 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1853 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9329 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9411 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8251 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.6959 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7030 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5961 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.4484 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4800 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6906 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7517 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7304 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7526 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 3.7930e-08 - accuracy: 1.0000 - val_loss: 8.5095 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4647 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4847 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6004 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.7193 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6185 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5995 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5489 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5584 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7258 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7826 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.5461 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6360 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7663 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7166 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.4976 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.9231 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8732 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8057 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9107 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7900 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7256 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4846 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4224 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4187 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4363 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6469 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5497 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.4536 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5227 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7794 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8171 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8289 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7928 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.5438 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7125 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7728 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0039 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0306 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.2177 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.3714 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2687 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1853 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0514 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8101 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7849 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8168 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9166 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 9.0517 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0985 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2701 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2126 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9590 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6692 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5467 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3182 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.1280 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3570 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3575 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4283 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5639 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7421 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7618 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7808 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7804 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8035 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7550 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6544 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5220 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6227 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5433 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5511 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3593 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.5080 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4296 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4512 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5369 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4706 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6046 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4727 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4619 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4738 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5712 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6644 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 6.2314e-08 - accuracy: 1.0000 - val_loss: 8.5080 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6817 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6193 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6880 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7285 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8661 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8302 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7311 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.8965e-08 - accuracy: 1.0000 - val_loss: 8.9242 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6679 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6933 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7067 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6168 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7254 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8799 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8450 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8845 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9049 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.6136 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7037 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7865 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7707 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8302 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0004 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9952 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9706 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8857 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.5555 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3591 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "Classification accuracy: 0.520776 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.7226576e-03 9.9227726e-01]\n",
      " [8.9589642e-13 9.9999994e-01]\n",
      " [9.9999994e-01 9.9106887e-14]\n",
      " [9.9999803e-01 1.9031139e-06]\n",
      " [9.9999994e-01 2.5240918e-13]\n",
      " [9.9999994e-01 4.0680857e-13]\n",
      " [1.8764605e-14 9.9999994e-01]\n",
      " [9.9999994e-01 9.1274961e-09]\n",
      " [1.2797143e-12 9.9999994e-01]\n",
      " [9.9999875e-01 1.2368912e-06]\n",
      " [6.0726139e-11 9.9999994e-01]\n",
      " [9.9989623e-01 1.0368826e-04]\n",
      " [9.8513472e-01 1.4865178e-02]\n",
      " [1.5122936e-02 9.8487711e-01]\n",
      " [9.9999994e-01 3.5599435e-15]\n",
      " [9.9999994e-01 2.3553793e-13]\n",
      " [1.0905969e-05 9.9998915e-01]\n",
      " [1.0000000e+00 3.4641903e-12]\n",
      " [1.0000000e+00 1.7420365e-09]]\n",
      "[1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[9 2]\n",
      " [3 5]]\n",
      "[73.68 75.   71.43]\n"
     ]
    }
   ],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_Shallow_init_sigmoid = np.array([[6.95184946e-01 3.04815054e-01]\n",
    " [4.21892613e-01 5.78107417e-01]\n",
    " [7.89708734e-01 2.10291266e-01]\n",
    " [9.99535143e-01 4.64851793e-04]\n",
    " [1.59405246e-01 8.40594828e-01]\n",
    " [9.02964830e-01 9.70351920e-02]\n",
    " [1.18944913e-01 8.81055117e-01]\n",
    " [1.71721101e-01 8.28278899e-01]\n",
    " [9.67139781e-01 3.28601785e-02]\n",
    " [7.46518612e-01 2.53481418e-01]\n",
    " [2.98798531e-01 7.01201379e-01]\n",
    " [1.78000614e-10 9.99999940e-01]\n",
    " [8.16536665e-01 1.83463335e-01]\n",
    " [2.43162528e-01 7.56837428e-01]\n",
    " [1.04919985e-01 8.95080090e-01]\n",
    " [9.93469775e-01 6.53020665e-03]\n",
    " [9.06261265e-01 9.37386826e-02]\n",
    " [6.60291553e-01 3.39708477e-01]\n",
    " [7.47849047e-01 2.52150923e-01]]\n",
    "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
    "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
