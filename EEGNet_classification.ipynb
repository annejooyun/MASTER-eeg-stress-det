{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.data import extract_eeg_data, multi_to_binary_classification, split_dataset, dict_to_arr\n",
    "from utils.labels import get_stai_labels\n",
    "from utils.valid_recs import get_valid_recs\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification, TSGLEEGNet\n",
    "import utils.variables as v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/ICA_data\\sub-P010_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P013_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P013_ses-S001_run-002.mat not valid\n",
      "Data/ICA_data\\sub-P020_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P023_ses-S002_run-002.mat not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n"
     ]
    }
   ],
   "source": [
    "valid_recs = get_valid_recs(data_type='ica', output_type = 'np')\n",
    "print(f'Valid recs {valid_recs}')\n",
    "\n",
    "x_dict_ = extract_eeg_data(valid_recs, data_type='ica', output_type='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n"
     ]
    }
   ],
   "source": [
    "y_dict_ = get_stai_labels(valid_recs) \n",
    "#y_dict = get_pss_labels(valid_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing invalid labels: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing invalid labels: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n"
     ]
    }
   ],
   "source": [
    "x_dict, y_dict = multi_to_binary_classification(x_dict_, y_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing mildly stressed subjects: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing  mildly stressed subjects: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict, test_data_dict, val_data_dict, train_labels_dict, test_labels_dict, val_labels_dict = split_dataset(x_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train data set: {len(train_data_dict)}\")\n",
    "print(f\"Length of validation data set: {len(val_data_dict)}\")\n",
    "print(f\"Length of test data set: {len(test_data_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (44, 8, 75000)\n",
      "Shape of validation data set: (16, 8, 75000)\n",
      "Shape of test data set: (19, 8, 75000)\n",
      "Shape of train labels set: (44, 1)\n",
      "Shape of validation labels set: (16, 1)\n",
      "Shape of test labels set: (19, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = dict_to_arr(train_data_dict)\n",
    "test_data = dict_to_arr(test_data_dict)\n",
    "val_data = dict_to_arr(val_data_dict)\n",
    "\n",
    "train_labels = np.reshape(np.array(list(train_labels_dict.values())), (len(train_data),1))\n",
    "test_labels = np.reshape(np.array(list(test_labels_dict.values())), (len(test_data),1))\n",
    "val_labels = np.reshape(np.array(list(val_labels_dict.values())), (len(val_data),1))\n",
    "\n",
    "print(f\"Shape of train data set: {train_data.shape}\")\n",
    "print(f\"Shape of validation data set: {val_data.shape}\")\n",
    "print(f\"Shape of test data set: {test_data.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Shape of train labels set: {train_labels.shape}\")\n",
    "print(f\"Shape of validation labels set: {val_labels.shape}\")\n",
    "print(f\"Shape of test labels set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.71064, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 11s - loss: 0.9489 - accuracy: 0.4545 - val_loss: 0.7106 - val_accuracy: 0.3125 - 11s/epoch - 11s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.5629 - accuracy: 0.6818 - val_loss: 0.7657 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.3071 - accuracy: 0.8409 - val_loss: 0.7491 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1756 - accuracy: 0.9773 - val_loss: 0.7269 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1320 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.3750 - 9s/epoch - 9s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.7117 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.0876 - accuracy: 1.0000 - val_loss: 0.7109 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 0.71064 to 0.70986, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.7099 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 0.70986 to 0.70700, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.7070 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 0.70700 to 0.70200, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.7020 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 0.70200 to 0.69611, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 0.69611 to 0.68972, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 0.68972 to 0.68410, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.68410 to 0.67905, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.67905 to 0.67509, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.67509 to 0.67224, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.67224 to 0.67001, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.6700 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.67001 to 0.66768, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.6677 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 0.66768 to 0.66546, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.6655 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.66546 to 0.66335, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.66335 to 0.66125, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.66125 to 0.65953, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.6595 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.65953 to 0.65809, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss improved from 0.65809 to 0.65643, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss improved from 0.65643 to 0.65470, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss improved from 0.65470 to 0.65297, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.6530 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss improved from 0.65297 to 0.65119, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.6512 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 0.65119 to 0.64929, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.64929 to 0.64748, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 10s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.6250 - 10s/epoch - 10s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss improved from 0.64748 to 0.64566, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 11s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.6457 - val_accuracy: 0.6250 - 11s/epoch - 11s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss improved from 0.64566 to 0.64393, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 12s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.6439 - val_accuracy: 0.6875 - 12s/epoch - 12s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss improved from 0.64393 to 0.64230, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.6423 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss improved from 0.64230 to 0.64093, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss improved from 0.64093 to 0.63976, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss improved from 0.63976 to 0.63852, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.6385 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss improved from 0.63852 to 0.63742, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss improved from 0.63742 to 0.63661, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss improved from 0.63661 to 0.63567, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss improved from 0.63567 to 0.63472, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss improved from 0.63472 to 0.63351, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.6335 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss improved from 0.63351 to 0.63230, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss improved from 0.63230 to 0.63113, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.6311 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss improved from 0.63113 to 0.63003, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6300 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss improved from 0.63003 to 0.62889, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss improved from 0.62889 to 0.62770, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.6277 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss improved from 0.62770 to 0.62656, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss improved from 0.62656 to 0.62544, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss improved from 0.62544 to 0.62431, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.62431 to 0.62322, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss improved from 0.62322 to 0.62220, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.62220 to 0.62119, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss improved from 0.62119 to 0.62010, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss improved from 0.62010 to 0.61895, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss improved from 0.61895 to 0.61802, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss improved from 0.61802 to 0.61704, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss improved from 0.61704 to 0.61608, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss improved from 0.61608 to 0.61516, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss improved from 0.61516 to 0.61417, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss improved from 0.61417 to 0.61316, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss improved from 0.61316 to 0.61204, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss improved from 0.61204 to 0.61090, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss improved from 0.61090 to 0.60971, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss improved from 0.60971 to 0.60867, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss improved from 0.60867 to 0.60787, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 0.60787 to 0.60701, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss improved from 0.60701 to 0.60634, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss improved from 0.60634 to 0.60557, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss improved from 0.60557 to 0.60476, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss improved from 0.60476 to 0.60401, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss improved from 0.60401 to 0.60309, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss improved from 0.60309 to 0.60220, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss improved from 0.60220 to 0.60128, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss improved from 0.60128 to 0.60033, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss improved from 0.60033 to 0.59933, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss improved from 0.59933 to 0.59831, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss improved from 0.59831 to 0.59742, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5974 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss improved from 0.59742 to 0.59663, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss improved from 0.59663 to 0.59588, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.5959 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss improved from 0.59588 to 0.59515, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss improved from 0.59515 to 0.59454, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5945 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss improved from 0.59454 to 0.59406, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss improved from 0.59406 to 0.59354, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5935 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss improved from 0.59354 to 0.59277, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5928 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss improved from 0.59277 to 0.59194, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss improved from 0.59194 to 0.59118, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5912 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss improved from 0.59118 to 0.59035, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5903 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss improved from 0.59035 to 0.58935, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5893 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss improved from 0.58935 to 0.58838, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5884 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss improved from 0.58838 to 0.58752, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss improved from 0.58752 to 0.58672, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5867 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss improved from 0.58672 to 0.58592, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss improved from 0.58592 to 0.58500, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss improved from 0.58500 to 0.58396, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.5840 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss improved from 0.58396 to 0.58298, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5830 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss improved from 0.58298 to 0.58201, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.5820 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss improved from 0.58201 to 0.58121, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5812 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss improved from 0.58121 to 0.58055, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5805 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss improved from 0.58055 to 0.57995, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5799 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss improved from 0.57995 to 0.57959, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5796 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss improved from 0.57959 to 0.57911, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5791 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss improved from 0.57911 to 0.57875, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.5787 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss improved from 0.57875 to 0.57842, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5784 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 0.57842 to 0.57811, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5781 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 0.57811 to 0.57753, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss improved from 0.57753 to 0.57697, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5770 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss improved from 0.57697 to 0.57632, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5763 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss improved from 0.57632 to 0.57562, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5756 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss improved from 0.57562 to 0.57481, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5748 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss improved from 0.57481 to 0.57412, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss improved from 0.57412 to 0.57358, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss improved from 0.57358 to 0.57302, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss improved from 0.57302 to 0.57257, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss improved from 0.57257 to 0.57224, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss improved from 0.57224 to 0.57193, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5719 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss improved from 0.57193 to 0.57155, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5715 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss improved from 0.57155 to 0.57117, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss improved from 0.57117 to 0.57093, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 545s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5709 - val_accuracy: 0.7500 - 545s/epoch - 545s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss improved from 0.57093 to 0.57081, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss improved from 0.57081 to 0.57056, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 10s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5706 - val_accuracy: 0.7500 - 10s/epoch - 10s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss improved from 0.57056 to 0.57030, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5703 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss improved from 0.57030 to 0.57006, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5701 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss improved from 0.57006 to 0.56977, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5698 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss improved from 0.56977 to 0.56953, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.56953\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss improved from 0.56953 to 0.56948, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.5697 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5699 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5698 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss improved from 0.56948 to 0.56925, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.5693 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss improved from 0.56925 to 0.56909, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss improved from 0.56909 to 0.56870, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss improved from 0.56870 to 0.56844, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss improved from 0.56844 to 0.56825, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5683 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss improved from 0.56825 to 0.56816, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5682 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss improved from 0.56816 to 0.56805, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss improved from 0.56805 to 0.56801, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5680 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5702 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5709 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5727 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5735 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5749 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.56801\n",
      "1/1 - 10s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5756 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.56801\n",
      "1/1 - 10s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5761 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5769 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5781 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5788 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5792 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5797 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5804 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5813 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5825 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5834 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5842 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5846 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5851 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5863 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5891 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5902 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5911 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5921 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5934 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5957 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6000 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6208 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6244 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6259 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6256 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6246 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6279 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6307 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6332 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6405 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6417 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6448 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6505 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6670 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6693 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6795 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6738 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6756 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6903 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6960 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6997 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7001 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6994 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6970 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6962 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6965 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6983 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7019 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7041 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7071 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7096 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7130 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7161 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7184 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7212 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7259 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7260 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7267 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7272 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7291 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7308 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7326 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7345 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x137038a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x137038a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 414ms/step\n",
      "Classification accuracy: 0.562327 \n"
     ]
    }
   ],
   "source": [
    "probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[9 2]\n",
      " [8 0]]\n",
      "[47.37 52.94  0.  ]\n"
     ]
    }
   ],
   "source": [
    "# with init_filter data, 300 epochs, sigmoid\n",
    "probs_EEGNet_init_sigmoid = np.array([\n",
    "                  [0.7138277, 0.28617287],\n",
    "                  [0.5637057, 0.43629473],\n",
    "                  [0.6218500, 0.37815124],\n",
    "                  [0.7166857, 0.28331438],\n",
    "                  [0.6754987, 0.32450426],\n",
    "                  [0.8090515, 0.19095036],\n",
    "                  [0.5339635, 0.46603800],\n",
    "                  [0.2460488, 0.75395140],\n",
    "                  [0.4467932, 0.55320940],\n",
    "                  [0.6290466, 0.37095330],\n",
    "                  [0.5762418, 0.42375806],\n",
    "                  [0.1861840, 0.81381420],\n",
    "                  [0.6687245, 0.33127743],\n",
    "                  [0.7082854, 0.29171503],\n",
    "                  [0.5183024, 0.48169836],\n",
    "                  [0.7217397, 0.27826140],\n",
    "                  [0.6201925, 0.37980822],\n",
    "                  [0.6769989, 0.32300153],\n",
    "                  [0.4880893, 0.51191190]])\n",
    "\n",
    "\n",
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6622658  0.33773378]\n",
      " [0.6305761  0.3694238 ]\n",
      " [0.59458554 0.40541294]\n",
      " [0.58825123 0.41174808]\n",
      " [0.61485636 0.38514262]\n",
      " [0.6281727  0.37182656]\n",
      " [0.6707296  0.32926738]\n",
      " [0.7152163  0.28478536]\n",
      " [0.3621873  0.63781214]\n",
      " [0.56972075 0.43027884]\n",
      " [0.629088   0.37091017]\n",
      " [0.4353447  0.5646552 ]\n",
      " [0.56697416 0.43302345]\n",
      " [0.6457108  0.35428876]\n",
      " [0.6471834  0.35281634]\n",
      " [0.5027475  0.497253  ]\n",
      " [0.62844676 0.37155262]\n",
      " [0.57276446 0.42723358]\n",
      " [0.5813717  0.41862816]]\n"
     ]
    }
   ],
   "source": [
    "print(probs_EEGNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SSVEP IS TAKING TOO LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_SSVEP = EEGNet_SSVEP_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(probs_SSVEP)\\npreds_SSVEP = probs_SSVEP.argmax(axis = -1)  \\nprint(preds_SSVEP)\\nprint(test_labels.T)\\n\\nperformance_SSVEP = compute_metrics(test_labels, preds_SSVEP)\\nprint(performance_SSVEP)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(probs_SSVEP)\n",
    "preds_SSVEP = probs_SSVEP.argmax(axis = -1)  \n",
    "print(preds_SSVEP)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_SSVEP = compute_metrics(test_labels, preds_SSVEP)\n",
    "print(performance_SSVEP)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 3.11173, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 5.4610 - accuracy: 0.6364 - val_loss: 3.1117 - val_accuracy: 0.6875 - 16s/epoch - 8s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 3.11173 to 2.49090, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 4.6506 - accuracy: 0.9091 - val_loss: 2.4909 - val_accuracy: 0.6250 - 14s/epoch - 7s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 2.49090 to 2.29405, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 3.9703 - accuracy: 0.9773 - val_loss: 2.2941 - val_accuracy: 0.6875 - 14s/epoch - 7s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 2.29405 to 2.02687, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 3.6702 - accuracy: 1.0000 - val_loss: 2.0269 - val_accuracy: 0.6875 - 15s/epoch - 7s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 2.02687 to 1.70249, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 3.5965 - accuracy: 1.0000 - val_loss: 1.7025 - val_accuracy: 0.7500 - 15s/epoch - 7s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 1.70249 to 1.56236, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 3.1744 - accuracy: 1.0000 - val_loss: 1.5624 - val_accuracy: 0.6250 - 15s/epoch - 7s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 1.56236 to 1.45458, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 3.0077 - accuracy: 1.0000 - val_loss: 1.4546 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 1.45458 to 1.37095, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 22s - loss: 2.9564 - accuracy: 1.0000 - val_loss: 1.3710 - val_accuracy: 0.6250 - 22s/epoch - 11s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 1.37095 to 1.30142, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 2.8821 - accuracy: 1.0000 - val_loss: 1.3014 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 1.30142 to 1.21975, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.8249 - accuracy: 1.0000 - val_loss: 1.2198 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 1.21975 to 1.17340, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.5147 - accuracy: 1.0000 - val_loss: 1.1734 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 1.17340 to 1.13818, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.5574 - accuracy: 1.0000 - val_loss: 1.1382 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 1.13818 to 1.09508, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.3313 - accuracy: 1.0000 - val_loss: 1.0951 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 1.09508 to 1.06390, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.2435 - accuracy: 1.0000 - val_loss: 1.0639 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 1.06390 to 1.03523, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.0886 - accuracy: 1.0000 - val_loss: 1.0352 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 1.03523 to 1.01599, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.1659 - accuracy: 1.0000 - val_loss: 1.0160 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 1.01599 to 0.99513, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.9363 - accuracy: 1.0000 - val_loss: 0.9951 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.99513 to 0.97134, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.0848 - accuracy: 1.0000 - val_loss: 0.9713 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 0.97134 to 0.95665, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.7910 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.95665 to 0.94738, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.7413 - accuracy: 1.0000 - val_loss: 0.9474 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.94738 to 0.93409, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 1.6984 - accuracy: 1.0000 - val_loss: 0.9341 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.93409 to 0.92415, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 1.5893 - accuracy: 1.0000 - val_loss: 0.9241 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.92415 to 0.91913, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 1.5340 - accuracy: 1.0000 - val_loss: 0.9191 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss improved from 0.91913 to 0.91142, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 1.3750 - accuracy: 1.0000 - val_loss: 0.9114 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss improved from 0.91142 to 0.89636, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 1.3587 - accuracy: 1.0000 - val_loss: 0.8964 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss improved from 0.89636 to 0.87773, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 1.2354 - accuracy: 1.0000 - val_loss: 0.8777 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss improved from 0.87773 to 0.86176, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 1.1766 - accuracy: 1.0000 - val_loss: 0.8618 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 0.86176 to 0.84943, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 1.1106 - accuracy: 1.0000 - val_loss: 0.8494 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.84943 to 0.83864, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 1.1246 - accuracy: 1.0000 - val_loss: 0.8386 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss improved from 0.83864 to 0.82869, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 0.9856 - accuracy: 1.0000 - val_loss: 0.8287 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss improved from 0.82869 to 0.81872, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 0.9653 - accuracy: 1.0000 - val_loss: 0.8187 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss improved from 0.81872 to 0.81083, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 0.8832 - accuracy: 1.0000 - val_loss: 0.8108 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss improved from 0.81083 to 0.80595, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.8301 - accuracy: 1.0000 - val_loss: 0.8059 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss improved from 0.80595 to 0.80259, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.7914 - accuracy: 1.0000 - val_loss: 0.8026 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss improved from 0.80259 to 0.80099, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.7492 - accuracy: 1.0000 - val_loss: 0.8010 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss improved from 0.80099 to 0.80007, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.7002 - accuracy: 1.0000 - val_loss: 0.8001 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.80007\n",
      "2/2 - 19s - loss: 0.6879 - accuracy: 1.0000 - val_loss: 0.8022 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.80007\n",
      "2/2 - 19s - loss: 0.6890 - accuracy: 1.0000 - val_loss: 0.8096 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.80007\n",
      "2/2 - 19s - loss: 0.6182 - accuracy: 1.0000 - val_loss: 0.8150 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.80007\n",
      "2/2 - 19s - loss: 0.5455 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.80007\n",
      "2/2 - 18s - loss: 0.5057 - accuracy: 1.0000 - val_loss: 0.8153 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.80007\n",
      "2/2 - 18s - loss: 0.5185 - accuracy: 1.0000 - val_loss: 0.8074 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss improved from 0.80007 to 0.79951, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.4502 - accuracy: 1.0000 - val_loss: 0.7995 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss improved from 0.79951 to 0.79669, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.4266 - accuracy: 1.0000 - val_loss: 0.7967 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss improved from 0.79669 to 0.79212, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.3973 - accuracy: 1.0000 - val_loss: 0.7921 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss improved from 0.79212 to 0.78575, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.4024 - accuracy: 1.0000 - val_loss: 0.7858 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss improved from 0.78575 to 0.78131, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.3526 - accuracy: 1.0000 - val_loss: 0.7813 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss improved from 0.78131 to 0.75989, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.3359 - accuracy: 1.0000 - val_loss: 0.7599 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.75989 to 0.73250, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.3103 - accuracy: 1.0000 - val_loss: 0.7325 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss improved from 0.73250 to 0.71821, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 0.2921 - accuracy: 1.0000 - val_loss: 0.7182 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.71821 to 0.71361, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 0.2727 - accuracy: 1.0000 - val_loss: 0.7136 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.71361\n",
      "2/2 - 17s - loss: 0.2829 - accuracy: 1.0000 - val_loss: 0.7190 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.71361\n",
      "2/2 - 18s - loss: 0.2439 - accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.71361\n",
      "2/2 - 17s - loss: 0.2303 - accuracy: 1.0000 - val_loss: 0.7254 - val_accuracy: 0.5000 - 17s/epoch - 9s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.71361\n",
      "2/2 - 17s - loss: 0.2165 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.71361\n",
      "2/2 - 17s - loss: 0.2149 - accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.71361\n",
      "2/2 - 17s - loss: 0.1909 - accuracy: 1.0000 - val_loss: 0.7211 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.71361\n",
      "2/2 - 18s - loss: 0.2002 - accuracy: 1.0000 - val_loss: 0.7137 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss improved from 0.71361 to 0.70833, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.1705 - accuracy: 1.0000 - val_loss: 0.7083 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.70833\n",
      "2/2 - 17s - loss: 0.1581 - accuracy: 1.0000 - val_loss: 0.7148 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.70833\n",
      "2/2 - 17s - loss: 0.1503 - accuracy: 1.0000 - val_loss: 0.7242 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.70833\n",
      "2/2 - 17s - loss: 0.1410 - accuracy: 1.0000 - val_loss: 0.7261 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.70833\n",
      "2/2 - 18s - loss: 0.1331 - accuracy: 1.0000 - val_loss: 0.7294 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.70833\n",
      "2/2 - 18s - loss: 0.1329 - accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.70833\n",
      "2/2 - 19s - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.7136 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.70833\n",
      "2/2 - 18s - loss: 0.1107 - accuracy: 1.0000 - val_loss: 0.7089 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.70833\n",
      "2/2 - 18s - loss: 0.1055 - accuracy: 1.0000 - val_loss: 0.7200 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.70833\n",
      "2/2 - 18s - loss: 0.1063 - accuracy: 1.0000 - val_loss: 0.7320 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.70833\n",
      "2/2 - 18s - loss: 0.0922 - accuracy: 1.0000 - val_loss: 0.7312 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.70833\n",
      "2/2 - 17s - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.7175 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss improved from 0.70833 to 0.69151, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 0.0821 - accuracy: 1.0000 - val_loss: 0.6915 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss improved from 0.69151 to 0.69069, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 0.0829 - accuracy: 1.0000 - val_loss: 0.6907 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.69069\n",
      "2/2 - 17s - loss: 0.0785 - accuracy: 1.0000 - val_loss: 0.7085 - val_accuracy: 0.5000 - 17s/epoch - 9s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.69069\n",
      "2/2 - 19s - loss: 0.0715 - accuracy: 1.0000 - val_loss: 0.7298 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.69069\n",
      "2/2 - 17s - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 0.5000 - 17s/epoch - 8s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.69069\n",
      "2/2 - 17s - loss: 0.0588 - accuracy: 1.0000 - val_loss: 0.7091 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.69069\n",
      "2/2 - 18s - loss: 0.0549 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.69069\n",
      "2/2 - 19s - loss: 0.0543 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.69069\n",
      "2/2 - 18s - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.7020 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.69069\n",
      "2/2 - 18s - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.7173 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.69069\n",
      "2/2 - 19s - loss: 0.0419 - accuracy: 1.0000 - val_loss: 0.7353 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.69069\n",
      "2/2 - 19s - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.7391 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.69069\n",
      "2/2 - 19s - loss: 0.0369 - accuracy: 1.0000 - val_loss: 0.7291 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.69069\n",
      "2/2 - 19s - loss: 0.0355 - accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.69069\n",
      "2/2 - 18s - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.7317 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.69069\n",
      "2/2 - 17s - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.7363 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.69069\n",
      "2/2 - 17s - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.7926 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.69069\n",
      "2/2 - 17s - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.7391 - val_accuracy: 0.3750 - 17s/epoch - 9s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.69069\n",
      "2/2 - 18s - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.7008 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss improved from 0.69069 to 0.68730, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.6873 - val_accuracy: 0.6250 - 21s/epoch - 11s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss improved from 0.68730 to 0.68336, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.6834 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss improved from 0.68336 to 0.66693, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.6669 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.66693\n",
      "2/2 - 18s - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss improved from 0.66693 to 0.59645, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.0780 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1107 - accuracy: 1.0000 - val_loss: 1.5384 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 1.0732 - accuracy: 0.7273 - val_loss: 1.7043 - val_accuracy: 0.2500 - 18s/epoch - 9s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 1.0423 - accuracy: 0.7273 - val_loss: 1.4397 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 1.7040 - accuracy: 0.8409 - val_loss: 3.3890 - val_accuracy: 0.1875 - 18s/epoch - 9s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 1.2244 - accuracy: 0.7500 - val_loss: 1.8741 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.3510 - accuracy: 0.9773 - val_loss: 1.3976 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.3049 - accuracy: 1.0000 - val_loss: 1.3488 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.3078 - accuracy: 1.0000 - val_loss: 1.3711 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.3219 - accuracy: 1.0000 - val_loss: 1.3397 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.3370 - accuracy: 1.0000 - val_loss: 1.2753 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.3376 - accuracy: 1.0000 - val_loss: 1.1980 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.3433 - accuracy: 1.0000 - val_loss: 1.1481 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.3451 - accuracy: 1.0000 - val_loss: 1.1074 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.3522 - accuracy: 1.0000 - val_loss: 1.0723 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.3277 - accuracy: 1.0000 - val_loss: 1.0324 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.3274 - accuracy: 1.0000 - val_loss: 0.9970 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.3391 - accuracy: 1.0000 - val_loss: 0.9647 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2974 - accuracy: 1.0000 - val_loss: 0.9335 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2893 - accuracy: 1.0000 - val_loss: 0.9038 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2888 - accuracy: 1.0000 - val_loss: 0.8784 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2688 - accuracy: 1.0000 - val_loss: 0.8569 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2920 - accuracy: 1.0000 - val_loss: 0.8426 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2496 - accuracy: 1.0000 - val_loss: 0.8266 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2410 - accuracy: 1.0000 - val_loss: 0.8172 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2323 - accuracy: 1.0000 - val_loss: 0.8031 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2242 - accuracy: 1.0000 - val_loss: 0.7856 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2132 - accuracy: 1.0000 - val_loss: 0.7766 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.2070 - accuracy: 1.0000 - val_loss: 0.7709 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1963 - accuracy: 1.0000 - val_loss: 0.7746 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.2062 - accuracy: 1.0000 - val_loss: 0.8090 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1859 - accuracy: 1.0000 - val_loss: 0.8209 - val_accuracy: 0.3750 - 17s/epoch - 9s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1768 - accuracy: 1.0000 - val_loss: 0.7829 - val_accuracy: 0.5625 - 17s/epoch - 8s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1711 - accuracy: 1.0000 - val_loss: 0.7560 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1695 - accuracy: 1.0000 - val_loss: 0.7522 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1675 - accuracy: 1.0000 - val_loss: 0.7878 - val_accuracy: 0.4375 - 17s/epoch - 9s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1528 - accuracy: 1.0000 - val_loss: 0.9532 - val_accuracy: 0.3750 - 17s/epoch - 9s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1495 - accuracy: 1.0000 - val_loss: 0.7851 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1512 - accuracy: 1.0000 - val_loss: 1.2103 - val_accuracy: 0.6250 - 17s/epoch - 8s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1669 - accuracy: 1.0000 - val_loss: 1.8562 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1640 - accuracy: 1.0000 - val_loss: 1.2662 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1590 - accuracy: 1.0000 - val_loss: 1.0832 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1393 - accuracy: 1.0000 - val_loss: 1.1699 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1565 - accuracy: 1.0000 - val_loss: 0.9915 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1313 - accuracy: 1.0000 - val_loss: 1.0076 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1413 - accuracy: 1.0000 - val_loss: 1.1351 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.59645\n",
      "2/2 - 21s - loss: 0.1312 - accuracy: 1.0000 - val_loss: 1.2442 - val_accuracy: 0.2500 - 21s/epoch - 10s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.5075 - accuracy: 0.9318 - val_loss: 4.4126 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 2.5263 - accuracy: 0.5000 - val_loss: 0.9541 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.3133 - accuracy: 0.9773 - val_loss: 1.8299 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.4644 - accuracy: 0.9318 - val_loss: 0.9546 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.4965 - accuracy: 0.9545 - val_loss: 1.3535 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2579 - accuracy: 1.0000 - val_loss: 1.3127 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2542 - accuracy: 1.0000 - val_loss: 1.2012 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2909 - accuracy: 1.0000 - val_loss: 1.1317 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2655 - accuracy: 1.0000 - val_loss: 1.1122 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2896 - accuracy: 1.0000 - val_loss: 1.1056 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2660 - accuracy: 1.0000 - val_loss: 1.0976 - val_accuracy: 0.5000 - 17s/epoch - 9s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.2584 - accuracy: 1.0000 - val_loss: 1.0851 - val_accuracy: 0.5000 - 17s/epoch - 9s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2709 - accuracy: 1.0000 - val_loss: 1.0937 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2732 - accuracy: 1.0000 - val_loss: 1.0616 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.2738 - accuracy: 1.0000 - val_loss: 1.0053 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.2380 - accuracy: 1.0000 - val_loss: 0.9830 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.2292 - accuracy: 1.0000 - val_loss: 0.9817 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.2194 - accuracy: 1.0000 - val_loss: 0.9810 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.2109 - accuracy: 1.0000 - val_loss: 0.9619 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.59645\n",
      "2/2 - 21s - loss: 0.2075 - accuracy: 1.0000 - val_loss: 0.9319 - val_accuracy: 0.6250 - 21s/epoch - 10s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.59645\n",
      "2/2 - 21s - loss: 0.2090 - accuracy: 1.0000 - val_loss: 0.9128 - val_accuracy: 0.6250 - 21s/epoch - 11s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1924 - accuracy: 1.0000 - val_loss: 0.8998 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.2022 - accuracy: 1.0000 - val_loss: 0.8920 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1792 - accuracy: 1.0000 - val_loss: 0.8844 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1732 - accuracy: 1.0000 - val_loss: 0.8920 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1650 - accuracy: 1.0000 - val_loss: 0.9198 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1700 - accuracy: 1.0000 - val_loss: 0.9401 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1538 - accuracy: 1.0000 - val_loss: 0.9460 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1601 - accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1431 - accuracy: 1.0000 - val_loss: 0.8093 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.1384 - accuracy: 1.0000 - val_loss: 0.7823 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.1339 - accuracy: 1.0000 - val_loss: 0.7732 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.59645\n",
      "2/2 - 21s - loss: 0.1484 - accuracy: 1.0000 - val_loss: 0.9254 - val_accuracy: 0.3125 - 21s/epoch - 11s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1246 - accuracy: 1.0000 - val_loss: 1.6297 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1401 - accuracy: 1.0000 - val_loss: 1.1613 - val_accuracy: 0.2500 - 19s/epoch - 9s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1167 - accuracy: 1.0000 - val_loss: 0.8774 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1126 - accuracy: 1.0000 - val_loss: 0.7830 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1227 - accuracy: 1.0000 - val_loss: 0.7939 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1113 - accuracy: 1.0000 - val_loss: 0.8487 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.1132 - accuracy: 1.0000 - val_loss: 0.9274 - val_accuracy: 0.2500 - 18s/epoch - 9s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.1082 - accuracy: 1.0000 - val_loss: 1.0121 - val_accuracy: 0.2500 - 17s/epoch - 8s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0988 - accuracy: 1.0000 - val_loss: 1.0682 - val_accuracy: 0.2500 - 17s/epoch - 9s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0912 - accuracy: 1.0000 - val_loss: 1.0926 - val_accuracy: 0.2500 - 18s/epoch - 9s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0883 - accuracy: 1.0000 - val_loss: 1.0828 - val_accuracy: 0.2500 - 18s/epoch - 9s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0851 - accuracy: 1.0000 - val_loss: 1.0227 - val_accuracy: 0.2500 - 18s/epoch - 9s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0901 - accuracy: 1.0000 - val_loss: 0.9641 - val_accuracy: 0.2500 - 18s/epoch - 9s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.1002 - accuracy: 1.0000 - val_loss: 0.9578 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0803 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0742 - accuracy: 1.0000 - val_loss: 1.0446 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0721 - accuracy: 1.0000 - val_loss: 1.0350 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0721 - accuracy: 1.0000 - val_loss: 0.9977 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0747 - accuracy: 1.0000 - val_loss: 0.8857 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.7882 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.7761 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0617 - accuracy: 1.0000 - val_loss: 0.7866 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.8260 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.8749 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.9372 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.9644 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.8572 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0513 - accuracy: 1.0000 - val_loss: 0.7848 - val_accuracy: 0.3750 - 19s/epoch - 9s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0520 - accuracy: 1.0000 - val_loss: 0.8180 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0521 - accuracy: 1.0000 - val_loss: 0.8605 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.8964 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0461 - accuracy: 1.0000 - val_loss: 0.8981 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.8646 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.8196 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.7787 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.7572 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0383 - accuracy: 1.0000 - val_loss: 0.7505 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.7573 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.7770 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.7998 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.8152 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0341 - accuracy: 1.0000 - val_loss: 0.8180 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0318 - accuracy: 1.0000 - val_loss: 0.8129 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.8125 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.8124 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.8242 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.59645\n",
      "2/2 - 22s - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.8158 - val_accuracy: 0.3125 - 22s/epoch - 11s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.7963 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0254 - accuracy: 1.0000 - val_loss: 0.7833 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.7764 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.7786 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.7954 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.8182 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.8183 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.7936 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.7786 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.7785 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.7902 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.7926 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.7719 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.7430 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.7285 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.7347 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.7471 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.7540 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.7527 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.7478 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.7327 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.7148 - val_accuracy: 0.3750 - 17s/epoch - 8s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.59645\n",
      "2/2 - 17s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.7064 - val_accuracy: 0.5000 - 17s/epoch - 8s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.7038 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.59645\n",
      "2/2 - 18s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.3125 - 19s/epoch - 9s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.7129 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.6662 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.6645 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.6924 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.59645\n",
      "2/2 - 20s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.7418 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.59645\n",
      "2/2 - 19s - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.6520 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss improved from 0.59645 to 0.59380, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.5938 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.6238 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0248 - accuracy: 1.0000 - val_loss: 0.6333 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.59380\n",
      "2/2 - 18s - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.6463 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.59380\n",
      "2/2 - 17s - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.6509 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.59380\n",
      "2/2 - 17s - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.6495 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.59380\n",
      "2/2 - 17s - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.6476 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.59380\n",
      "2/2 - 17s - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.6444 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.59380\n",
      "2/2 - 18s - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.59380\n",
      "2/2 - 18s - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.6345 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.6306 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.59380\n",
      "2/2 - 20s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.6278 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.59380\n",
      "2/2 - 22s - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.6228 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.59380\n",
      "2/2 - 20s - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.59380\n",
      "2/2 - 19s - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.5991 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.59380\n",
      "2/2 - 18s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.5961 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss improved from 0.59380 to 0.59324, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.5932 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss improved from 0.59324 to 0.58588, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss improved from 0.58588 to 0.58010, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.5801 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss improved from 0.58010 to 0.57731, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.5773 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.57731\n",
      "2/2 - 17s - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.5783 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.57731\n",
      "2/2 - 18s - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.5845 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.57731\n",
      "2/2 - 18s - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.5920 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.5937 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.5932 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.57731\n",
      "2/2 - 20s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.5933 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.57731\n",
      "2/2 - 20s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.5936 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.57731\n",
      "2/2 - 20s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.5936 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.57731\n",
      "2/2 - 20s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.5956 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6067 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6045 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.57731\n",
      "2/2 - 18s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.57731\n",
      "2/2 - 17s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.57731\n",
      "2/2 - 17s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.5953 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.57731\n",
      "2/2 - 17s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5868 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.57731\n",
      "2/2 - 17s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.5847 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.5904 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.57731\n",
      "2/2 - 20s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.6121 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.57731\n",
      "2/2 - 19s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.6104 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Classification accuracy: 0.578947 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\annej\\anaconda3\\envs\\MNE\\lib\\site-packages\\pyriemann\\utils\\viz.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  cm = 100 * cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAHFCAYAAABb+zt/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMZUlEQVR4nO3deVxU1f8/8NeAMAy7qAxiyu6OyeKGFaa4lBpmpaYtmuaWC1lqZCqmgmiapWlmpdg3U9MwKzXNBfcU3NHEBVwKwgVFBVnP7w9+3o8TqCD3emeY17PHfTycc8898x6EfPM+59yrEUIIEBERESnEQu0AiIiIqGpjskFERESKYrJBREREimKyQURERIpiskFERESKYrJBREREimKyQURERIpiskFERESKYrJBREREimKyQVRBS5cuhUajue+xfft2AICnp+d9+7Rr167UuEePHsXAgQPh4+MDnU4HnU4HPz8/DBkyBImJiQZ9o6KioNFo4Orqips3b5Yay9PTE926dXukz7dgwQIsXbq0QtdkZ2djwoQJqF+/PmxtbVGnTh288sorSE5Ofui16enp+Oijj9CmTRvUrFkTjo6OCAoKwldffYWioqJH+gxEZFyqqR0AkalasmQJGjZsWKq9cePG0p/btm2LTz75pFQfR0dHg9eLFi3CiBEj0KBBA4wePRpNmjSBRqPByZMn8cMPP6BFixY4c+YMfHx8DK67fPkyZs6cialTp8r0qUqSjZo1a6J///7lvqZ79+5ITExEVFQUgoODcenSJXz88cdo06YNjh07Bg8Pj/tem5SUhGXLluGNN97AxIkTYWVlhQ0bNmDYsGHYt28fvv32Wxk+FRGpShBRhSxZskQAEAcOHHhgPw8PD9G1a9eHjrdr1y5hYWEhunfvLvLy8srss2rVKvH3339LrydPniwAiC5dugg7OzuRnp7+SO9dliZNmojQ0NBy9z99+rQAID766COD9j179ggAYs6cOQ+8/tq1ayI/P79U+zvvvCMAiAsXLpQ7FiIyTpxGIVJZdHQ0LC0tsWjRIlhbW5fZ55VXXoG7u3up9mnTpqGwsBBRUVEPfZ/8/HxMmzYNDRs2hFarRa1atTBgwABcvnxZ6uPp6Ynk5GQkJCRIUz6enp4PHNfKygoA4OTkZNDu7OwMALCxsXng9dWrV5fGuFfLli0BAJcuXXrYRyMiI8dkg+gRFRUVobCw0OD47xoDIUSpPoWFhRD//2HLRUVF2LZtG4KDg1G7du0Kx+Dh4YHhw4fjm2++QUpKyn37FRcXIzw8HDNmzEDfvn3x22+/YcaMGdi8eTPatWuH3NxcAEB8fDy8vb0REBCAvXv3Yu/evYiPj39oDOHh4fj000+xbds23Lp1C3/99RdGjRqFevXqoU+fPhX+XACwdetWVKtWDfXr13+k64nIiKhdWiEyNXenUco6LC0tpX4eHh737Td16lQhhBAZGRkCgOjTp0+p9yksLBQFBQXSUVxcLJ27O41y+fJlceXKFeHk5CReeuklg/e+dxrlhx9+EADEmjVrDN7jwIEDAoBYsGCB1FbRaRQhhMjPzxdvv/22wWds1qyZSE1NrdA4d/3+++/CwsJCvPvuu490PREZFy4QJXpEy5YtQ6NGjQzaNBqNweunnnoKn376aalr69Sp89Dxg4KCcOTIEen1rFmz8P7775fqV6NGDYwfPx4ffvgh/vzzT7Rq1apUn19//RXOzs7o3r07CgsLpfbmzZvDzc0N27dvx7Bhwx4YT1FRkVSRAQALCwtYWJQUR4cNG4b4+Hh8+umnCAwMREZGBmbNmoX27dtj27ZtD1wg+l8HDx5Er1690Lp1a8TExJT7OiIyXkw2iB5Ro0aNEBwc/MA+Tk5OD+xTs2ZN6HQ6nD9/vtS55cuXIycnB+np6XjhhRce+D4RERGYP38+xo0bh4SEhFLn//33X1y/fv2+a0KuXLnywPEBwMfHxyDOyZMnIyoqChs3bsQ333yDH3/8ES+//LJ0vlOnTvD09ERUVBSWLFny0PEB4NChQ+jYsSP8/Pywfv16aLXacl1HRMaNyQaRiiwtLdG+fXts2rQJ6enpBus27m6hTUtLe+g4Op0OUVFRGDx4MH777bdS52vWrIkaNWpg48aNZV7v4ODw0Pf45ZdfkJeXJ72+u2D18OHDAIAWLVoY9Hd2doavry+OHz/+0LGBkkQjLCwMHh4e2LRpU6kFp0RkuphsEKksMjISGzZswNChQ7F69eoyd2aUx1tvvYVPP/0UH3zwAYqLiw3OdevWDStWrEBRUVGZ0yz30mq10oLRe/n7+5fZ/27SsW/fPoPpkqtXryIlJQUdOnR4aOyHDx9GWFgYnnjiCWzevBnVq1d/6DVEZDqYbBA9ouPHjxusf7jLx8cHtWrVAgBcv34d+/btK9VHq9UiICAAQMmNv7744guMHDkSgYGBGDx4MJo0aQILCwukp6djzZo1AErfCOy/LC0tER0djRdffBEA0KxZM+lcnz598P333+P555/H6NGj0bJlS1hZWeHSpUvYtm0bwsPDpev8/f2xYsUKrFy5Et7e3rCxsblvogEAPXv2xKRJkzBs2DBcunQJgYGBSE9Px6xZs5CTk4PRo0cb9NdoNAgNDZXutHrq1CmEhYUBAKZPn47Tp0/j9OnTZX49ichEqb1ClcjUPGg3CgCxePFiIcSDd6PUqVOn1LiHDx8WAwYMEF5eXkKr1QobGxvh6+sr3njjDbFlyxaDvvfuRvmvkJAQAaDUTb0KCgrEJ598Ip588klhY2Mj7O3tRcOGDcWQIUPE6dOnpX5paWmiU6dOwsHBQQAQHh4eD/2apKenixEjRghfX19hY2Mj3N3dRdeuXcXevXsN+t28ebPU7puHfT2XLFny0PcnIuOmEeKe5eVERApav349unXrhiNHjjywWkJEVQtv6kVEj822bdvQp08fJhpEZoaVDSIiIlIUKxtERESkKCYbREREVdSOHTvQvXt3uLu7Q6PRYO3atQbnhRCIioqCu7s7dDod2rVrh+TkZIM+eXl5GDlyJGrWrAk7Ozu88MILFX5AIpMNIiKiKur27dt48sknMX/+/DLPz5w5E3PmzMH8+fNx4MABuLm5oWPHjrh586bUJyIiAvHx8VixYgV27dqFW7duoVu3bqUePPkgXLNBRERkBjQaDeLj49GjRw8AJVUNd3d3REREYPz48QBKqhh6vR6xsbEYMmQIbty4gVq1auG7775D7969AQD//PMP6tati/Xr16Nz587lem9WNoiIiExEXl4esrOzDY57HyNQEampqcjIyECnTp2kNq1Wi9DQUOzZswcAkJSUhIKCAoM+7u7uaNq0qdSnPHgHUSIiIoXpAkbIMs748JqYMmWKQdvdhyJWVEZGBgBAr9cbtOv1eumhixkZGbC2ti71CAG9Xi9dXx5VNtno+U2S2iEQGZ2fBgZhdsI5tcMgMirvhXqrHUK5RUZGYsyYMQZtlX06skajMXgthCjV9l/l6XMvTqMQEREpTWMhy6HVauHo6GhwPGqy4ebmBgClKhSZmZlStcPNzQ35+fnIysq6b5/yYLJBRESkNI1GnkNGXl5ecHNzw+bNm6W2/Px8JCQkICQkBAAQFBQEKysrgz7p6ek4fvy41Kc8quw0ChERkdHQqPO7/a1bt3DmzBnpdWpqKg4fPgwXFxfUq1cPERERiI6Ohp+fH/z8/BAdHQ1bW1v07dsXAODk5ISBAwfivffeQ40aNeDi4oL3338f/v7+0tOay4PJBhERURWVmJiIZ599Vnp9d73Hm2++iaVLl2LcuHHIzc3F8OHDkZWVhVatWmHTpk1wcHCQrvn0009RrVo19OrVC7m5uejQoQOWLl0KS0vLcsdRZe+zwQWiRKVxgShRaY9jgaiuxZiHdyqH3ANzZBnncWNlg4iISGkqTaMYC/P+9ERERKQ4VjaIiIiUJvNOElPDZIOIiEhpnEYhIiIiUg4rG0RERErjNAoREREpitMoRERERMphZYOIiEhpnEYhIiIiRZn5NAqTDSIiIqWZeWXDvFMtIiIiUhwrG0RERErjNAoREREpysyTDfP+9ERERKQ4VjaIiIiUZmHeC0SZbBARESmN0yhEREREymFlg4iISGlmfp8NJhtERERK4zQKERERkXJY2SAiIlIap1GIiIhIUWY+jcJkg4iISGlmXtkw71SLiIiIFMfKBhERkdI4jUJERESK4jQKERERkXJY2SAiIlIap1GIiIhIUZxGISIiIlIOKxtERERK4zQKERERKcrMkw3z/vRERESkOFY2iIiIlGbmC0SZbBARESnNzKdRmGwQEREpzcwrG+adahEREZHiWNkgIiJSGqdRiIiISFGcRiEiIiJSDisbRERECtOYeWWDyQYREZHCzD3Z4DQKERERKYqVDSIiIqWZd2GDyQYREZHSOI1CREREpCBWNoiIiBRm7pUNJhtEREQKY7JBREREijL3ZINrNoiIiEhRrGwQEREpzbwLG0w2iIiIlMZpFCIiIiIFsbJBRESkMHOvbDDZICIiUpi5JxucRiEiIiJFsbJBRESkMHOvbDDZICIiUpp55xrqJBtjxowpd985c+YoGAkREREpTZVk49ChQwavk5KSUFRUhAYNGgAAUlJSYGlpiaCgIDXCIyIikhWnUVSwbds26c9z5syBg4MD4uLiUL16dQBAVlYWBgwYgKefflqN8IiIiGRl7smG6rtRZs+ejZiYGCnRAIDq1atj2rRpmD17toqRERERyUOj0chymCrVk43s7Gz8+++/pdozMzNx8+ZNFSIiIiIyfYWFhfjoo4/g5eUFnU4Hb29vfPzxxyguLpb6CCEQFRUFd3d36HQ6tGvXDsnJybLHonqy8eKLL2LAgAFYvXo1Ll26hEuXLmH16tUYOHAgevbsqXZ4RERElaeR6aiA2NhYfPnll5g/fz5OnjyJmTNnYtasWZg3b57UZ+bMmZgzZw7mz5+PAwcOwM3NDR07dpT9l33Vt75++eWXeP/99/Haa6+hoKAAAFCtWjUMHDgQs2bNUjk6IiKiylNjCmTv3r0IDw9H165dAQCenp744YcfkJiYCKCkqjF37lxMmDBB+uU+Li4Oer0ey5cvx5AhQ2SLRfXKhq2tLRYsWICrV6/i0KFDOHjwIK5du4YFCxbAzs5O7fCIiIiMRl5eHrKzsw2OvLy8Mvs+9dRT2LJlC1JSUgAAR44cwa5du/D8888DAFJTU5GRkYFOnTpJ12i1WoSGhmLPnj2yxq16snFXeno60tPTUb9+fdjZ2UEIoXZIREREspBrgWhMTAycnJwMjpiYmDLfc/z48Xj11VfRsGFDWFlZISAgABEREXj11VcBABkZGQAAvV5vcJ1er5fOyUX1aZSrV6+iV69e2LZtGzQaDU6fPg1vb28MGjQIzs7O3JFCREQmT65plMjIyFI3xtRqtWX2XblyJf7v//4Py5cvR5MmTXD48GFERETA3d0db7755n1jE0LIPu2jemXj3XffhZWVFS5cuABbW1upvXfv3ti4caOKkRERERkXrVYLR0dHg+N+ycbYsWPxwQcfoE+fPvD398frr7+Od999V6qEuLm5AUCpKkZmZmapakdlqZ5sbNq0CbGxsXjiiScM2v38/HD+/HmVoiIiIpKPGvfZyMnJgYWF4T/zlpaW0tZXLy8vuLm5YfPmzdL5/Px8JCQkICQkpPIf+h6qT6Pcvn3boKJx15UrV+6brREREZkUFe7H1b17d0yfPh316tVDkyZNcOjQIcyZMwdvvfVWSUgaDSIiIhAdHQ0/Pz/4+fkhOjoatra26Nu3r6yxqJ5sPPPMM1i2bBmmTp0KoOTDFxcXY9asWXj22WdVjo6IiMg0zZs3DxMnTsTw4cORmZkJd3d3DBkyBJMmTZL6jBs3Drm5uRg+fDiysrLQqlUrbNq0CQ4ODrLGohEqb/s4ceIE2rVrh6CgIGzduhUvvPACkpOTce3aNezevRs+Pj6PNG7Pb5JkjpTI9P00MAizE86pHQaRUXkv1Fvx96gzLF6Wcf5e+KIs4zxuqq/ZaNy4MY4ePYqWLVuiY8eOuH37Nnr27IlDhw49cqJBRERkTMz92SiqT6MAJStip0yZonYYREREijDlREEOqlc2Nm7ciF27dkmvv/jiCzRv3hx9+/ZFVlaWipERERGRHFRPNsaOHYvs7GwAwLFjxzBmzBg8//zzOHfuXKkblxAREZkkFR7EZkxUn0ZJTU1F48aNAQBr1qxB9+7dER0djYMHD0r3byciIjJlnEZRmbW1NXJycgAAf/zxh/RAGBcXF6niQURERKZL9crGU089hTFjxqBt27bYv38/Vq5cCQBISUkpdVdRUk/vgNroHehu0JaVU4CBPxwFULKlsixx+y/h52P/lnnOUgP0fLI2nvWrARdbK/xz4w6+O/A3Dv3NJJNM06ENK3EgfimadghHSO+hKC4sxIGf43DhWCJuXkmHtc4OdRoFoGXPAbBzrnHfca79cx6JP3+HKxdO49bVTLTpNRj+Yaa55ZFKmHtlQ/VkY/78+Rg+fDhWr16NhQsXok6dOgCADRs2oEuXLipHR/e6kJWLqA0p0uvie+7Q8tbyIwZ9A59wwvCnPbAv7f6LfPsG18EzPi5YuOs8/r5xB83rOGJcmA8+/PUvpF7NlT1+IiVlpp3CXzs2wOUJL6mtMD8PVy6cRWC3V1HjCW/k5dzE3pWL8PsXU9Bzwuf3Hasw/w4ca7nBO+gp7F311eMInxTGZENl9erVw6+//lqq/dNPP1UhGnqQomKB67mFZZ77b3sLD2ccT7+Jf2/m33e8UB8XrD6SgYOXSioZv/91Bc2fcMILTfX4LCFNtriJlFZwJxfbvp6Fp18fjUPrf5DarW3t0PXdaIO+Ia8Ow9roCNy6mgn7Gq5ljufq2QCung0AAPvjlygXONFjovqajYMHD+LYsWPS659//hk9evTAhx9+iPz8+/9DRY9fbUctvu7jj4W9mmLMs17QO1iX2c/JphqC6jphy6krDxzPytICBUXFBm35hcVopLeXLWaix2HXD1+grn8LPNE44KF983NyAI0G1rZ2jyEyMhbmflMv1ZONIUOGICWlpDR/7tw59OnTB7a2tvjxxx8xbtw4laOju1Iu38bnO9Lw8e+nsXDXeTjrrBDdrSHstZal+j7rVwO5BUXYd/76A8c89Hc2ujfVo7ajFhoAT7o7oKWHM6rbWinzIYgUcGb/dlw5fxYtew54aN/Cgnzsj18C35btYK1jsmFWzHzrq+rJRkpKCpo3bw4A+PHHH/HMM89g+fLlWLp0KdasWfPQ6/Py8pCdnW1w5OXlKRy1+Tl0KRv70q7jQtYdHP3nJqZvOgOgJLH4r/b1a2LnmWsoKHrwY3e+3XcR6dl5+PylJlg1IBCD2tTD1pQrBmtBiIzZrWuXsXflIrQfOBbVrMqu9N1VXFiILV/NgCguxlN933lMERIZB9XXbAghUFxcUkr/448/0K1bNwBA3bp1ceXKg8vwABATE1PqVueTJ08G6naXP1iS5BUW40JWLmo72hi0N9Lb4wlnG8zZ9vCHfWXfKUTsH2dhZamBg7YaruUU4PUWdfDvTSaLZBqunD+N3JvX8dP0kVKbKC5G+unjSN72CwYuWAcLC0sUFxbij6+icfNqBrqNmcGqhhky5SkQOaiebAQHB2PatGkICwtDQkICFi5cCKDkZl96vf6h10dGRpa606hWq8Wr/3dckXipRDULDZ5wtsGJjFsG7R3q18CZy7eRdq38u0kKigSu5RTAUgO09nTGnnO8TT2ZBvdGzfHy5IUGbQlL58DJrS6ad3nFING4kfkPur03Azb2jipFS2pisqGyuXPnol+/fli7di0mTJgAX19fAMDq1asREhLy0Ou1Wi20Wq3SYZq9N1vWwYELN3DlVj6cdNXwcvPa0FlZYvuZq1IfnZUFQryqY+n+S2WOMeoZT1zNycf3if8AAPxq2cLF1hpp13LgYmuN3oG1oYEG8fe5LweRsbG2sYVLHU+DtmpaG9jYO8CljieKi4qwedF0XLlwBl1GTIEoLkbOjWsAAK2dAyyrlaxP2vbtJ7BzriGt+ygqLEBW+gUAJdMvt69fxZWLZ2Gl1cHJ1fB+N2QazDzXUD/ZaNasmcFulLtmzZoFS8vSiw9JHTXsrDGmnRccbKoh+04hUjJv44Nf/sLlW//bMfSUtws0Gg12nb1W5hg17a1RLP63IMPK0gJ9g9yhd9DiTmExDl68gc8S0pCTX6T45yF6HG5nXcH5I/sAAGumGq7T6PZeLNwbNAMA3LqWafCbb871a/hp6gjp9dFNa3B00xrUru+P7u/PfAyRE8lLI4RQfTne9evXsXr1apw9exZjx46Fi4sLDh48CL1eL93kq6J6fpMkc5REpu+ngUGYnfDw9TRE5uS9UG/F38Nv7EZZxjk9yzRvdql6ZePo0aPo0KEDnJ2dkZaWhrfffhsuLi6Ij4/H+fPnsWzZMrVDJCIiqhRzn0ZRfevrmDFjMGDAAJw+fRo2Nv/b2fDcc89hx44dKkZGREREclC9snHgwAEsWrSoVHudOnWQkZGhQkRERETy4m4UldnY2JT5KPlTp06hVq1aKkREREQkLzPPNdSfRgkPD8fHH3+MgoICACXZ34ULF/DBBx/gpZdeUjk6IiIiqizVk41PPvkEly9fhqurK3JzcxEaGgpfX184ODhg+vTpaodHRERUaRYWGlkOU6X6NIqjoyN27dqFrVu34uDBgyguLkZgYCDCwsLUDo2IiEgW5j6NomqyUVhYCBsbGxw+fBjt27dH+/bt1QyHiIiIFKBqslGtWjV4eHigqIh3jCQioqrL3HejqL5m46OPPkJkZCSuXSv7FtdERESmTqOR5zBVqq/Z+Pzzz3HmzBm4u7vDw8MDdnaGj14+ePCgSpERERHJw9wrG6onG+Hh4Wb/l0BERFSVqZ5sREVFqR0CERGRosz9l2rV12x4e3vj6tWrpdqvX78Ob2/ln8RHRESkNHNfs6F6spGWllbmbpS8vDxcunRJhYiIiIhITqpNo6xbt0768++//w4nJyfpdVFREbZs2QIvLy81QiMiIpKVuU+jqJZs9OjRA0DJX8Cbb75pcM7Kygqenp6YPXu2CpERERHJy8xzDfWSjeLiYgCAl5cXDhw4gJo1a6oVChERESlItTUbf/75JzZs2IDU1FQp0Vi2bBm8vLzg6uqKwYMHIy8vT63wiIiIZKPRaGQ5TJVqycbkyZNx9OhR6fWxY8cwcOBAhIWF4YMPPsAvv/yCmJgYtcIjIiKSDXejqOTIkSPo0KGD9HrFihVo1aoVFi9ejDFjxuDzzz/HqlWr1AqPiIiIZKLamo2srCzo9XrpdUJCArp06SK9btGiBS5evKhGaERERLIy5SkQOahW2dDr9UhNTQUA5Ofn4+DBg2jTpo10/ubNm7CyslIrPCIiItlwGkUlXbp0wQcffICdO3ciMjIStra2ePrpp6XzR48ehY+Pj1rhERERycbcF4iqNo0ybdo09OzZE6GhobC3t0dcXBysra2l899++y06deqkVnhEREQkE9WSjVq1amHnzp24ceMG7O3tYWlpaXD+xx9/hL29vUrRERERyceEixKyUP2pr/fepvxeLi4ujzkSIiIiZZjyFIgcVH8QGxEREVVtqlc2iIiIqjozL2ww2SAiIlIap1GIiIiIFMTKBhERkcLMvLDBZIOIiEhpnEYhIiIiUhArG0RERAoz98oGkw0iIiKFmXmuwWSDiIhIaeZe2eCaDSIiIlIUKxtEREQKM/PCBpMNIiIipXEahYiIiEhBrGwQEREpzMwLG0w2iIiIlGZh5tkGp1GIiIhIUaxsEBERKczMCxtMNoiIiJTG3ShERESkKAuNPEdF/f3333jttddQo0YN2Nraonnz5khKSpLOCyEQFRUFd3d36HQ6tGvXDsnJyTJ+8hJMNoiIiKqgrKwstG3bFlZWVtiwYQNOnDiB2bNnw9nZWeozc+ZMzJkzB/Pnz8eBAwfg5uaGjh074ubNm7LGwmkUIiIihakxjRIbG4u6detiyZIlUpunp6f0ZyEE5s6diwkTJqBnz54AgLi4OOj1eixfvhxDhgyRLRZWNoiIiBSm0chz5OXlITs72+DIy8sr8z3XrVuH4OBgvPLKK3B1dUVAQAAWL14snU9NTUVGRgY6deoktWm1WoSGhmLPnj2yfn4mG0RERCYiJiYGTk5OBkdMTEyZfc+dO4eFCxfCz88Pv//+O4YOHYpRo0Zh2bJlAICMjAwAgF6vN7hOr9dL5+TCaRQiIiKFaSDPNEpkZCTGjBlj0KbVasvsW1xcjODgYERHRwMAAgICkJycjIULF+KNN974X2z/meIRQsg+7cPKBhERkcLk2o2i1Wrh6OhocNwv2ahduzYaN25s0NaoUSNcuHABAODm5gYApaoYmZmZpaodlf78so5GRERERqFt27Y4deqUQVtKSgo8PDwAAF5eXnBzc8PmzZul8/n5+UhISEBISIissXAahYiISGFq7EZ59913ERISgujoaPTq1Qv79+/HV199ha+++kqKKSIiAtHR0fDz84Ofnx+io6Nha2uLvn37yhpLuZKNzz//vNwDjho16pGDISIiqorUuIFoixYtEB8fj8jISHz88cfw8vLC3Llz0a9fP6nPuHHjkJubi+HDhyMrKwutWrXCpk2b4ODgIGssGiGEeFgnLy+v8g2m0eDcuXOVDkoOPb9JengnIjPz08AgzE4wjp9RImPxXqi34u/R4+tEWcZZOyhYlnEet3JVNlJTU5WOg4iIqMriI+YfUX5+Pk6dOoXCwkI54yEiIqpy5Lqpl6mqcLKRk5ODgQMHwtbWFk2aNJG20IwaNQozZsyQPUAiIiJTp9FoZDlMVYWTjcjISBw5cgTbt2+HjY2N1B4WFoaVK1fKGhwRERGZvgpvfV27di1WrlyJ1q1bG2RZjRs3xtmzZ2UNjoiIqCow4aKELCqcbFy+fBmurq6l2m/fvm3SJR4iIiKlcIFoBbVo0QK//fab9PpugrF48WK0adNGvsiIiIioSqhwZSMmJgZdunTBiRMnUFhYiM8++wzJycnYu3cvEhISlIiRiIjIpJl3XeMRKhshISHYvXs3cnJy4OPjg02bNkGv12Pv3r0ICgpSIkYiIiKTZu67UR7p2Sj+/v6Ii4uTOxYiIiKqgh4p2SgqKkJ8fDxOnjwJjUaDRo0aITw8HNWq8bluRERE/2VhukUJWVQ4Ozh+/DjCw8ORkZGBBg0aACh5ZG2tWrWwbt06+Pv7yx4kERGRKTPlKRA5VHjNxqBBg9CkSRNcunQJBw8exMGDB3Hx4kU0a9YMgwcPViJGIiIiMmEVrmwcOXIEiYmJqF69utRWvXp1TJ8+HS1atJA1OCIioqrAzAsbFa9sNGjQAP/++2+p9szMTPj6+soSFBERUVXC3SjlkJ2dLf05Ojoao0aNQlRUFFq3bg0A2LdvHz7++GPExsYqEyUREZEJ4wLRcnB2djbIqIQQ6NWrl9QmhAAAdO/eHUVFRQqESURERKaqXMnGtm3blI6DiIioyjLlKRA5lCvZCA0NVToOIiKiKsu8U41HvKkXAOTk5ODChQvIz883aG/WrFmlgyIiIqKq45EeMT9gwABs2LChzPNcs0FERGSIj5ivoIiICGRlZWHfvn3Q6XTYuHEj4uLi4Ofnh3Xr1ikRIxERkUnTaOQ5TFWFKxtbt27Fzz//jBYtWsDCwgIeHh7o2LEjHB0dERMTg65duyoRJxEREZmoClc2bt++DVdXVwCAi4sLLl++DKDkSbAHDx6UNzoiIqIqwNxv6vVIdxA9deoUAKB58+ZYtGgR/v77b3z55ZeoXbu27AESERGZOk6jVFBERATS09MBAJMnT0bnzp3x/fffw9raGkuXLpU7PiIiIjJxFU42+vXrJ/05ICAAaWlp+Ouvv1CvXj3UrFlT1uCIiIiqAnPfjfLI99m4y9bWFoGBgXLEQkREVCWZea5RvmRjzJgx5R5wzpw5jxwMERFRVWTKizvlUK5k49ChQ+UazNy/mERERFSaRtx9ZCsREREpYmT8SVnGmfdiI1nGedwqvWaDiIiIHszcK/8Vvs8GERERUUWwskFERKQwC/MubDDZICIiUpq5JxucRiEiIiJFPVKy8d1336Ft27Zwd3fH+fPnAQBz587Fzz//LGtwREREVQEfxFZBCxcuxJgxY/D888/j+vXrKCoqAgA4Oztj7ty5csdHRERk8iw08hymqsLJxrx587B48WJMmDABlpaWUntwcDCOHTsma3BERERk+iq8QDQ1NRUBAQGl2rVaLW7fvi1LUERERFWJCc+AyKLClQ0vLy8cPny4VPuGDRvQuHFjOWIiIiKqUiw0GlkOU1XhysbYsWPxzjvv4M6dOxBCYP/+/fjhhx8QExODr7/+WokYiYiITJq5b/2scLIxYMAAFBYWYty4ccjJyUHfvn1Rp04dfPbZZ+jTp48SMRIREZEJq9SD2K5cuYLi4mK4urrKGRMREVGVMmFDiizjTH+uvizjPG6VuoNozZo15YqDiIioyjLl9RZyqHCy4eXl9cAbi5w7d65SAREREVHVUuFkIyIiwuB1QUEBDh06hI0bN2Ls2LFyxUVERFRlmHlho+LJxujRo8ts/+KLL5CYmFjpgIiIiKoaU777pxxk243z3HPPYc2aNXINR0RERFWEbI+YX716NVxcXOQajoiIqMrgAtEKCggIMFggKoRARkYGLl++jAULFsgaHBERUVVg5rlGxZONHj16GLy2sLBArVq10K5dOzRs2FCuuIiIiKiKqFCyUVhYCE9PT3Tu3Blubm5KxURERFSlcIFoBVSrVg3Dhg1DXl6eUvEQERFVORqZ/jNVFd6N0qpVKxw6dEiJWIiIiKokC408h6mq8JqN4cOH47333sOlS5cQFBQEOzs7g/PNmjWTLTgiIiIyfeV+ENtbb72FuXPnwtnZufQgGg2EENBoNCgqKpI7RiIiIpM2c9tZWcYZ96yPLOM8buVONiwtLZGeno7c3NwH9vPw8JAlMCIioqpi1nZ5nhs2tp23LOM8buWeRrmbkzCZICIiooqo0JqNBz3tlYiIiMpmyos75VChZKN+/foPTTiuXbtWqYCIiIiqGnP/Xb1CycaUKVPg5OSkVCxERERUBVUo2ejTpw9cXV2VioWIiKhKMvcHsZX7pl5cr0FERPRojOGmXjExMdBoNIiIiJDahBCIioqCu7s7dDod2rVrh+Tk5Mq9URnKnWyUc4csERERGZkDBw7gq6++KnXjzZkzZ2LOnDmYP38+Dhw4ADc3N3Ts2BE3b96U9f3LnWwUFxdzCoWIiOgRaDTyHI/i1q1b6NevHxYvXozq1atL7UIIzJ07FxMmTEDPnj3RtGlTxMXFIScnB8uXL5fpk5eo8LNRiIiIqGIsoJHlyMvLQ3Z2tsHxsIejvvPOO+jatSvCwsIM2lNTU5GRkYFOnTpJbVqtFqGhodizZ4/Mn5+IiIgUJVdlIyYmBk5OTgZHTEzMfd93xYoVOHjwYJl9MjIyAAB6vd6gXa/XS+fkUuEHsREREZE6IiMjMWbMGIM2rVZbZt+LFy9i9OjR2LRpE2xsbO475n83gNx91pmcmGwQEREpTK47iGq12vsmF/+VlJSEzMxMBAUFSW1FRUXYsWMH5s+fj1OnTgEoqXDUrl1b6pOZmVmq2lFZnEYhIiJSmIVGI8tRER06dMCxY8dw+PBh6QgODka/fv1w+PBheHt7w83NDZs3b5auyc/PR0JCAkJCQmT9/KxsEBERVUEODg5o2rSpQZudnR1q1KghtUdERCA6Ohp+fn7w8/NDdHQ0bG1t0bdvX1ljYbJBRESkMGO9L+a4ceOQm5uL4cOHIysrC61atcKmTZvg4OAg6/toBO/WRUREpKhv9l+QZZyBLevJMs7jxjUbREREpChOoxARESnMWKdRHhcmG0RERAoz92kEc//8REREpDBWNoiIiBQm9x05TQ2TDSIiIoWZd6rBZIOIiEhxFb37Z1WjWrLx+eefl7vvqFGjFIyEiIiIlKTaTb28vLwMXl++fBk5OTlwdnYGAFy/fh22trZwdXXFuXPnVIiQiIhIHt8nXZJlnH5BT8gyzuOm2m6U1NRU6Zg+fTqaN2+OkydP4tq1a7h27RpOnjyJwMBATJ06Va0QiYiIZKHRyHOYKqO4XbmPjw9Wr16NgIAAg/akpCS8/PLLSE1NVSkyIiKiylt+UJ7KRt9A06xsGMUC0fT0dBQUFJRqLyoqwr///qtCRERERPIx962vRnFTrw4dOuDtt99GYmIi7hZaEhMTMWTIEISFhakcHRERUeVYyHSYKqOI/dtvv0WdOnXQsmVL2NjYQKvVolWrVqhduza+/vprtcMjIiKiSjCKaZRatWph/fr1SElJwV9//QUhBBo1aoT69eurHRoREVGlmfs0ilEkG3d5enpCCAEfHx9Uq2ZUoRERET0y8041jGQaJScnBwMHDoStrS2aNGmCCxcuACi5mdeMGTNUjo6IiIgqwyiSjcjISBw5cgTbt2+HjY2N1B4WFoaVK1eqGBkREVHlaTQaWQ5TZRRzFWvXrsXKlSvRunVrgy9m48aNcfbsWRUjIyIiqjyj+M1eRUaRbFy+fBmurq6l2m/fvm3SmRwRERHABaJGkWy1aNECv/32m/T67l/K4sWL0aZNG7XCIiIiIhkYRWUjJiYGXbp0wYkTJ1BYWIjPPvsMycnJ2Lt3LxISEtQOj4iIqFLMu65hJJWNkJAQ7N69Gzk5OfDx8cGmTZug1+uxd+9eBAUFqR0eERFRpfBBbEbwIDYiIqKq7OdjGbKME+7vJss4j5tRVDYOHjyIY8eOSa9//vln9OjRAx9++CHy8/NVjIyIiKjyLKCR5TBVRpFsDBkyBCkpKQCAc+fOoXfv3rC1tcWPP/6IcePGqRwdERFR5Zj7NIpRJBspKSlo3rw5AODHH39EaGgoli9fjqVLl2LNmjXqBkdERESVYhS7UYQQKC4uBgD88ccf6NatGwCgbt26uHLlipqhERERVZrGhKdA5GAUyUZwcDCmTZuGsLAwJCQkYOHChQCA1NRU6PV6laMjIiKqHFOeApGDUUyjzJ07FwcPHsSIESMwYcIE+Pr6AgBWr16NkJAQlaMjIiKiyjDqra937tyBpaUlrKys1A6FiIjokW1MvizLOF2a1JJlnMfNKCobFy9exKVLl6TX+/fvR0REBJYtW8ZEg4iITB53oxiBvn37Ytu2bQCAjIwMdOzYEfv378eHH36Ijz/+WOXoiIiIKofJhhE4fvw4WrZsCQBYtWoVmjZtij179kjbX4mIiMh0GcVulIKCAmi1WgAlW19feOEFAEDDhg2Rnp6uZmhERESVZu5bX42istGkSRN8+eWX2LlzJzZv3owuXboAAP755x/UqFFD5eiIiIgqx0Ijz2GqjCLZiI2NxaJFi9CuXTu8+uqrePLJJwEA69atk6ZXiIiIyDQZzdbXoqIiZGdno3r16lJbWloabG1t4erqqmJkRERElbP1r6uyjNO+oWlW+42isgGU3LI8KSkJixYtws2bNwEA1tbWsLW1VTkyIiKiyjH33ShGsUD0/Pnz6NKlCy5cuIC8vDx07NgRDg4OmDlzJu7cuYMvv/xS7RCJiIjoERlFZWP06NEIDg5GVlYWdDqd1P7iiy9iy5YtKkZGRERUeRqZ/jNVRlHZ2LVrF3bv3g1ra2uDdg8PD/z9998qRUVERCQPU95JIgejqGwUFxejqKioVPulS5fg4OCgQkREREQkF6NINjp27Ii5c+dKrzUaDW7duoXJkyfj+eefVy8wIiIiGZj7NIpRbH39+++/0b59e1haWuL06dMIDg7G6dOnUbNmTezYsYNbX4mIyKTtOp0lyzhP+VV/eCcjZBTJBgDk5uZixYoVSEpKQnFxMQIDA9GvXz+DBaNERESmaLdMyUZbJhuPpqCgAA0aNMCvv/6Kxo0bqxkKERGRIsw92VB9N4qVlRXy8vKgecS7leTl5SEvL8+gTavVSg92IyIiUpuFKd+RSwZGsUB05MiRiI2NRWFhYYWvjYmJgZOTk8ERExOjQJRERESPRiPTYapUn0YB/nfzLnt7e/j7+8POzs7g/E8//XTfa1nZICIiY7fvzHVZxmnt6yzLOI+b6tMoAODs7IyXXnrpka5lYkFEREbPlMsSMjCKygYREVFV9ufZG7KM08rHSZZxHjejWLPRvn17XL9+vVR7dnY22rdv//gDIiIiItkYxTTK9u3bkZ+fX6r9zp072LlzpwoRERERycfMN6Oom2wcPXpU+vOJEyeQkZEhvS4qKsLGjRtRp04dNUIjIiKSjZnnGuomG82bN4dGo4FGoylzukSn02HevHkqREZERERyUTXZSE1NhRAC3t7e2L9/P2rVqiWds7a2hqurKywtLVWMkIiISAZmXtpQNdnw8PAAUPKIeSIioqrKlJ/YKgej2I0SFxeH3377TXo9btw4ODs7IyQkBOfPn1cxMiIiosrTaOQ5TJVRJBvR0dHS01337t2L+fPnY+bMmahZsybeffddlaMjIiKiyjCKra8XL16Er68vAGDt2rV4+eWXMXjwYLRt2xbt2rVTNzgiIqJKMuGihCyMorJhb2+Pq1evAgA2bdqEsLAwAICNjQ1yc3PVDI2IiKjyzPxJbEZR2ejYsSMGDRqEgIAApKSkoGvXrgCA5ORkeHp6qhscERERVYpRVDa++OILtGnTBpcvX8aaNWtQo0YNAEBSUhJeffVVlaMjIiKqHI1M/1VETEwMWrRoAQcHB7i6uqJHjx44deqUQR8hBKKiouDu7g6dTod27dohOTlZzo8OgA9iIyIiUtzhCzdlGad5PYdy9+3SpQv69OmDFi1aoLCwEBMmTMCxY8dw4sQJ2NnZAQBiY2Mxffp0LF26FPXr18e0adOwY8cOnDp1Cg4O5X+vhzG6ZMPf3x/r169H3bp11Q6FiIhIFmokG/91+fJluLq6IiEhAc888wyEEHB3d0dERATGjx8PAMjLy4Ner0dsbCyGDBkiS8yAkUyj3CstLQ0FBQVqh0FERCQbudaH5uXlITs72+DIy8srVww3bpQ85t7FxQVAyV28MzIy0KlTJ6mPVqtFaGgo9uzZU9mPbMDokg0iIqIqR6ZsIyYmBk5OTgZHTEzMQ99eCIExY8bgqaeeQtOmTQFAevipXq836KvX6w0ejCoHo9iNcq+nn35ausEXERER/U9kZCTGjBlj0KbVah963YgRI3D06FHs2rWr1DnNf25NKoQo1VZZRpdsrF+/Xu0QiIiIZCXXs1G0Wm25kot7jRw5EuvWrcOOHTvwxBNPSO1ubm4ASioctWvXltozMzNLVTsqy2iSjZSUFGzfvh2ZmZmlHsw2adIklaIiIiKqPDWeayKEwMiRIxEfH4/t27fDy8vL4LyXlxfc3NywefNmBAQEAADy8/ORkJCA2NhYWWMximRj8eLFGDZsGGrWrAk3NzeD8o1Go2GyQUREJk2Nm3++8847WL58OX7++Wc4ODhI6zCcnJyg0+mg0WgQERGB6Oho+Pn5wc/PD9HR0bC1tUXfvn1ljcUotr56eHhg+PDh0tYbIiKiquT4pVuyjNP0Cfty973fuoslS5agf//+AEqqH1OmTMGiRYuQlZWFVq1a4YsvvpAWkcrFKJINR0dHHD58GN7e3mqHQkREJLvjf8uUbNQpf7JhTIxi6+srr7yCTZs2qR0GERGRItS4XbkxMYo1G76+vpg4cSL27dsHf39/WFlZGZwfNWqUSpERERFRZRnFNMp/V8jeS6PR4Ny5c48xGiIiInmd+Oe2LOM0dreTZZzHzSgqG6mpqWqHQEREpBjTnQCRh1Gs2biXEAJGUGwhIiIimRhNsrFs2TL4+/tDp9NBp9OhWbNm+O6779QOi4iIqPLkehKbiTKKaZQ5c+Zg4sSJGDFiBNq2bQshBHbv3o2hQ4fiypUrePfdd9UOkYiI6JGZ8k4SORjNAtEpU6bgjTfeMGiPi4tDVFQU13QQEZFJ+ys9R5ZxGta2lWWcx80oKhvp6ekICQkp1R4SEoL09HQVIiIiIpKPGs9GMSZGsWbD19cXq1atKtW+cuVK+Pn5qRARERGRfMx8yYZxVDamTJmC3r17Y8eOHWjbti00Gg127dqFLVu2lJmEEBERmRRTzhRkYBRrNgAgKSkJc+bMwV9//QUhBBo3boz33ntPeuwtERGRqUr5V541G/X1prlmw2iSDSIioqrq9L+5sozjp9fJMs7jpuo0ioWFxX0fgXuXRqNBYWHhY4qIiIhIfua+QFTVZCM+Pv6+5/bs2YN58+bxbqJEREQmzuimUf766y9ERkbil19+Qb9+/TB16lTUq1dP7bCIiIge2dlMeaZRfFxNcxrFKLa+AsA///yDt99+G82aNUNhYSEOHz6MuLg4JhpERGT6zHzvq+rJxo0bNzB+/Hj4+voiOTkZW7ZswS+//IKmTZuqHRoRERHJQNU1GzNnzkRsbCzc3Nzwww8/IDw8XM1wiIiIFMFno6i4ZsPCwgI6nQ5hYWGwtLS8b7+ffvrpMUZFREQkr9Qrd2QZx6umjSzjPG6qVjbeeOONh259JSIiItNmdLtRiIiIqpo0mSobnqxsEBERUZnMvIjPZIOIiEhh5r5AVPWtr0RERFS1sbJBRESkMHPfC8Fkg4iISGFmnmtwGoWIiIiUxcoGERGRwjiNQkRERAoz72yD0yhERESkKFY2iIiIFMZpFCIiIlKUmecanEYhIiIiZbGyQUREpDBOoxAREZGizP3ZKEw2iIiIlGbeuQbXbBAREZGyWNkgIiJSmJkXNphsEBERKc3cF4hyGoWIiIgUxcoGERGRwrgbhYiIiJRl3rkGp1GIiIhIWaxsEBERKczMCxtMNoiIiJTG3ShERERECmJlg4iISGHcjUJERESK4jQKERERkYKYbBAREZGiOI1CRESkMHOfRmGyQUREpDBzXyDKaRQiIiJSFCsbRERECuM0ChERESnKzHMNTqMQERGRsljZICIiUpqZlzaYbBARESmMu1GIiIiIFMTKBhERkcK4G4WIiIgUZea5BqdRiIiIFKeR6XgECxYsgJeXF2xsbBAUFISdO3dW6qM8CiYbREREVdTKlSsRERGBCRMm4NChQ3j66afx3HPP4cKFC481Do0QQjzWdyQiIjIzuQXyjKOzqlj/Vq1aITAwEAsXLpTaGjVqhB49eiAmJkaeoMqBlQ0iIiKFaTTyHBWRn5+PpKQkdOrUyaC9U6dO2LNnj4yf7uG4QJSIiMhE5OXlIS8vz6BNq9VCq9WW6nvlyhUUFRVBr9cbtOv1emRkZCga53+xskGKycvLQ1RUVKkfDCJzx58N82NTTZ4jJiYGTk5OBsfDpkM0/ymJCCFKtSmNazZIMdnZ2XBycsKNGzfg6OiodjhERoM/G/SoKlLZyM/Ph62tLX788Ue8+OKLUvvo0aNx+PBhJCQkKB7vXaxsEBERmQitVgtHR0eDo6xEAwCsra0RFBSEzZs3G7Rv3rwZISEhjyNcCddsEBERVVFjxozB66+/juDgYLRp0wZfffUVLly4gKFDhz7WOJhsEBERVVG9e/fG1atX8fHHHyM9PR1NmzbF+vXr4eHh8VjjYLJBitFqtZg8efJ9S3xE5oo/G/Q4DR8+HMOHD1c1Bi4QJSIiIkVxgSgREREpiskGERERKYrJBhERESmKyQZVGe3atUNERITaYRBVKZ6enpg7d67aYZCJY7JhZjIzMzFkyBDUq1cPWq0Wbm5u6Ny5M/bu3Qug5La2a9euVTdIokfQv39/aDQazJgxw6B97dq1j/3WzPdKS0uDRqPB4cOHVYuBSG1MNszMSy+9hCNHjiAuLg4pKSlYt24d2rVrh2vXrpV7jIICmZ6VTCQzGxsbxMbGIisrS+1QKiw/P1/tEIgUw2TDjFy/fh27du1CbGwsnn32WXh4eKBly5aIjIxE165d4enpCQB48cUXodFopNdRUVFo3rw5vv32W3h7e0Or1UIIgRs3bmDw4MFwdXWFo6Mj2rdvjyNHjkjvd+TIETz77LNwcHCAo6MjgoKCkJiYCAA4f/48unfvjurVq8POzg5NmjTB+vXrpWtPnDiB559/Hvb29tDr9Xj99ddx5coV6fzt27fxxhtvwN7eHrVr18bs2bOV/wKS0QsLC4Obm9sDH0y1Zs0aNGnSBFqtFp6enqW+dzw9PREdHY233noLDg4OqFevHr766qsHvm9WVhb69euHWrVqQafTwc/PD0uWLAEAeHl5AQACAgKg0WjQrl07ACWVmB49eiAmJgbu7u6oX78+AODvv/9G7969Ub16ddSoUQPh4eFIS0uT3mv79u1o2bIl7Ozs4OzsjLZt2+L8+fMAHvwzBwB79uzBM888A51Oh7p162LUqFG4ffu2dD4zMxPdu3eHTqeDl5cXvv/++4d8xYnKh8mGGbG3t4e9vT3Wrl1b5tMmDxw4AABYsmQJ0tPTpdcAcObMGaxatQpr1qyRysFdu3ZFRkYG1q9fj6SkJAQGBqJDhw5SlaRfv3544okncODAASQlJeGDDz6AlZUVAOCdd95BXl4eduzYgWPHjiE2Nhb29vYAgPT0dISGhqJ58+ZITEzExo0b8e+//6JXr15SPGPHjsW2bdsQHx+PTZs2Yfv27UhKSlLk60amw9LSEtHR0Zg3bx4uXbpU6nxSUhJ69eqFPn364NixY4iKisLEiROxdOlSg36zZ89GcHAwDh06hOHDh2PYsGH466+/7vu+EydOxIkTJ7BhwwacPHkSCxcuRM2aNQEA+/fvBwD88ccfSE9Px08//SRdt2XLFpw8eRKbN2/Gr7/+ipycHDz77LOwt7fHjh07sGvXLtjb26NLly7Iz89HYWEhevTogdDQUBw9ehR79+7F4MGDpWmiB/3MHTt2DJ07d0bPnj1x9OhRrFy5Ert27cKIESOkePr374+0tDRs3boVq1evxoIFC5CZmflofxlE9xJkVlavXi2qV68ubGxsREhIiIiMjBRHjhyRzgMQ8fHxBtdMnjxZWFlZiczMTKlty5YtwtHRUdy5c8egr4+Pj1i0aJEQQggHBwexdOnSMuPw9/cXUVFRZZ6bOHGi6NSpk0HbxYsXBQBx6tQpcfPmTWFtbS1WrFghnb969arQ6XRi9OjRD/0aUNX05ptvivDwcCGEEK1btxZvvfWWEEKI+Ph4cfd/dX379hUdO3Y0uG7s2LGicePG0msPDw/x2muvSa+Li4uFq6urWLhw4X3fu3v37mLAgAFlnktNTRUAxKFDh0rFq9frRV5entT2zTffiAYNGoji4mKpLS8vT+h0OvH777+Lq1evCgBi+/btZb7Xg37mXn/9dTF48GCDtp07dwoLCwuRm5srTp06JQCIffv2SedPnjwpAIhPP/30vp+dqDxY2TAzL730Ev755x+sW7cOnTt3xvbt2xEYGFjqN7v/8vDwQK1ataTXSUlJuHXrFmrUqCFVTOzt7ZGamoqzZ88CKHkA0KBBgxAWFoYZM2ZI7QAwatQoTJs2DW3btsXkyZNx9OhRg7G3bdtmMG7Dhg0BAGfPnsXZs2eRn5+PNm3aSNe4uLigQYMGcnyJqAqIjY1FXFwcTpw4YdB+8uRJtG3b1qCtbdu2OH36NIqKiqS2Zs2aSX/WaDRwc3OTfsN/7rnnpO/LJk2aAACGDRuGFStWoHnz5hg3bhz27NlTrjj9/f1hbW0tvU5KSsKZM2fg4OAgvYeLiwvu3LmDs2fPwsXFBf3790fnzp3RvXt3fPbZZ0hPT5euf9DPXFJSEpYuXWrwc9W5c2cUFxcjNTUVJ0+eRLVq1RAcHCxd07BhQzg7O5frsxA9CJMNM2RjY4OOHTti0qRJ2LNnD/r374/Jkyc/8Bo7OzuD18XFxahduzYOHz5scJw6dQpjx44FULLWIzk5GV27dsXWrVvRuHFjxMfHAwAGDRqEc+fO4fXXX8exY8cQHByMefPmSWN379691NinT5/GM888A8E77NNDPPPMM+jcuTM+/PBDg3YhRKmdKWV9P92derhLo9GguLgYAPD1119L35N31xk999xzOH/+PCIiIvDPP/+gQ4cOeP/99x8aZ1k/V0FBQaW+91NSUtC3b18AJdOce/fuRUhICFauXIn69etj3759AB78M1dcXIwhQ4YYjHvkyBGcPn0aPj4+0tdBzZ07VHXxQWyExo0bS9tdraysDH7Du5/AwEBkZGSgWrVq0kLSstSvXx/169fHu+++i1dffRVLlizBiy++CACoW7cuhg4diqFDhyIyMhKLFy/GyJEjERgYiDVr1sDT0xPVqpX+FvX19YWVlRX27duHevXqAShZoJeSkoLQ0NCKfwGoSpoxYwaaN28uLbwESr7Xd+3aZdBvz549qF+/PiwtLcs1bp06dcpsr1WrFvr374/+/fvj6aefxtixY/HJJ59IlYvy/lytXLlSWnR9PwEBAQgICEBkZCTatGmD5cuXo3Xr1gDu/zMXGBiI5ORk+Pr6ljlmo0aNUFhYiMTERLRs2RIAcOrUKVy/fv2hcRM9DCsbZuTq1ato3749/u///g9Hjx5FamoqfvzxR8ycORPh4eEASlbib9myBRkZGQ/cPhgWFoY2bdqgR48e+P3335GWloY9e/bgo48+QmJiInJzczFixAhs374d58+fx+7du3HgwAE0atQIABAREYHff/8dqampOHjwILZu3Sqde+edd3Dt2jW8+uqr2L9/P86dO4dNmzbhrbfeQlFREezt7TFw4ECMHTsWW7ZswfHjx9G/f39YWPDbmf7H398f/fr1kypmAPDee+9hy5YtmDp1KlJSUhAXF4f58+eXqwrxIJMmTcLPP/+MM2fOIDk5Gb/++qv0/ezq6gqdTictdL5x48Z9x+nXrx9q1qyJ8PBw7Ny5E6mpqUhISMDo0aNx6dIlpKamIjIyEnv37sX58+exadMmpKSkoFGjRg/9mRs/fjz27t2Ld955R6oUrlu3DiNHjgQANGjQAF26dMHbb7+NP//8E0lJSRg0aBB0Ol2lvjZEALhA1JzcuXNHfPDBByIwMFA4OTkJW1tb0aBBA/HRRx+JnJwcIYQQ69atE76+vqJatWrCw8NDCFGyQPTJJ58sNV52drYYOXKkcHd3F1ZWVqJu3bqiX79+4sKFCyIvL0/06dNH1K1bV1hbWwt3d3cxYsQIkZubK4QQYsSIEcLHx0dotVpRq1Yt8frrr4srV65IY6ekpIgXX3xRODs7C51OJxo2bCgiIiKkhXM3b94Ur732mrC1tRV6vV7MnDlThIaGcoGoGbt3gehdaWlpQqvVinv/V7d69WrRuHFjYWVlJerVqydmzZplcI2Hh0epBZFPPvmkmDx58n3fe+rUqaJRo0ZCp9MJFxcXER4eLs6dOyedX7x4sahbt66wsLAQoaGh941XCCHS09PFG2+8IWrWrCm0Wq3w9vYWb7/9trhx44bIyMgQPXr0ELVr1xbW1tbCw8NDTJo0SRQVFT30Z04IIfbv3y86duwo7O3thZ2dnWjWrJmYPn26wXt37dpVaLVaUa9ePbFs2bIyvx5EFcVHzBMREZGiWHcmIiIiRTHZICIiIkUx2SAiIiJFMdkgIiIiRTHZICIiIkUx2SAiIiJFMdkgIiIiRTHZIDIiUVFRaN68ufS6f//+6NGjx2OPIy0tDRqNBocPH75vH09PT8ydO7fcYy5dulSWh3ppNBrp9vpEZBqYbBA9RP/+/aHRaKDRaGBlZQVvb2+8//77uH37tuLv/dlnnz30ibx3lSdBICJSAx/ERlQOXbp0wZIlS1BQUICdO3di0KBBuH37NhYuXFiqb0FBQamnhj4qJycnWcYhIlITKxtE5aDVauHm5oa6deuib9++6Nevn1TKvzv18e2338Lb2xtarRZCCNy4cQODBw+WnuDZvn17HDlyxGDcGTNmQK/Xw8HBAQMHDsSdO3cMzv93GqW4uBixsbHw9fWFVqtFvXr1MH36dACAl5cXgJIngmo0GrRr1066bsmSJWjUqBFsbGzQsGFDLFiwwOB99u/fj4CAANjY2CA4OBiHDh2q8Ndozpw58Pf3h52dHerWrYvhw4fj1q1bpfqtXbsW9evXh42NDTp27IiLFy8anP/ll18QFBQEGxsbeHt7Y8qUKSgsLKxwPERkPJhsED0CnU6HgoIC6fWZM2ewatUqrFmzRprG6Nq1KzIyMrB+/XokJSUhMDAQHTp0wLVr1wAAq1atwuTJkzF9+nQkJiaidu3apZKA/4qMjERsbCwmTpyIEydOYPny5dDr9QBKEgYA+OOPP5Ceno6ffvoJALB48WJMmDAB06dPx8mTJxEdHY2JEyciLi4OAHD79m1069YNDRo0QFJSEqKioh7pKagWFhb4/PPPcfz4ccTFxWHr1q0YN26cQZ+cnBxMnz4dcXFx2L17N7Kzs9GnTx/p/O+//47XXnsNo0aNwokTJ7Bo0SIsXbpUSqiIyESp/CA4IqP336dz/vnnn6JGjRqiV69eQoiSp+JaWVmJzMxMqc+WLVuEo6OjuHPnjsFYPj4+YtGiRUIIIdq0aSOGDh1qcL5Vq1YGT9i9972zs7OFVqsVixcvLjPO1NRUAUAcOnTIoL1u3bpi+fLlBm1Tp04Vbdq0EUIIsWjRIuHi4iJu374tnV+4cGGZY93rYU8DXbVqlahRo4b0esmSJQKA2Ldvn9R28uRJAUD8+eefQgghnn76aREdHW0wznfffSdq164tvQYg4uPj7/u+RGR8uGaDqBx+/fVX2Nvbo7CwEAUFBQgPD8e8efOk8x4eHqhVq5b0OikpCbdu3UKNGjUMxsnNzcXZs2cBACdPnsTQoUMNzrdp0wbbtm0rM4aTJ08iLy8PHTp0KHfcly9fxsWLFzFw4EC8/fbbUnthYaG0HuTkyZN48sknYWtraxBHRW3btg3R0dE4ceIEsrOzUVhYiDt37uD27duws7MDAFSrVg3BwcHSNQ0bNoSzszNOnjyJli1bIikpCQcOHDCoZBQVFeHOnTvIyckxiJGITAeTDaJyePbZZ7Fw4UJYWVnB3d291ALQu/+Y3lVcXIzatWtj+/btpcZ61O2fOp2uwtcUFxcDKJlKadWqlcE5S0tLAIAQ4pHiudf58+fx/PPPY+jQoZg6dSpcXFywa9cuDBw40GC6CSjZuvpfd9uKi4sxZcoU9OzZs1QfGxubSsdJROpgskFUDnZ2dvD19S13/8DAQGRkZKBatWrw9PQss0+jRo2wb98+vPHGG1Lbvn377jumn58fdDodtmzZgkGDBpU6b21tDaCkEnCXXq9HnTp1cO7cOfTr16/McRs3bozvvvsOubm5UkLzoDjKkpiYiMLCQsyePRsWFiVLwVatWlWqX2FhIRITE9GyZUsAwKlTp3D9+nU0bNgQQMnX7dSpUxX6WhOR8WOyQaSAsLAwtGnTBj169EBsbCwaNGiAf/75B+vXr0ePHj0QHByM0aNH480330RwcDCeeuopfP/990hOToa3t3eZY9rY2GD8+PEYN24crK2t0bZtW1y+fBnJyckYOHAgXF1dodPpsHHjRjzxxBOwsbGBk5MToqKiMGrUKDg6OuK5555DXl4eEhMTkZWVhTFjxqBv376YMGECBg4ciI8++ghpaWn45JNPKvR5fXx8UFhYiHnz5qF79+7YvXs3vvzyy1L9rKysMHLkSHz++eewsrLCiBEj0Lp1ayn5mDRpErp164a6devilVdegYWFBY4ePYpjx45h2rRpFf+LICKjwN0oRArQaDRYv349nnnmGbz11luoX78++vTpg7S0NGn3SO/evTFp0iSMHz8eQUFBOH/+PIYNG/bAcSdOnIj33nsPkyZNQqNGjdC7d29kZmYCKFkP8fnnn2PRokVwd3dHeHg4AGDQoEH4+uuvsXTpUvj7+yM0NBRLly6Vtsra29vjl19+wYkTJxAQEIAJEyYgNja2Qp+3efPmmDNnDmJjY9G0aVN8//33iImJKdXP1tYW48ePR9++fdGmTRvodDqsWLFCOt+5c2f8+uuv2Lx5M1q0aIHWrVtjzpw58PDwqFA8RGRcNEKOCVsiIiKi+2Blg4iIiBTFZIOIiIgUxWSDiIiIFMVkg4iIiBTFZIOIiIgUxWSDiIiIFMVkg4iIiBTFZIOIiIgUxWSDiIiIFMVkg4iIiBTFZIOIiIgUxWSDiIiIFPX/APBqrBRIF5jZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64331865 0.3566838 ]\n",
      " [0.6146636  0.38533852]\n",
      " [0.58371186 0.41628984]\n",
      " [0.56707406 0.43292838]\n",
      " [0.6612734  0.33872905]\n",
      " [0.6875879  0.3124139 ]\n",
      " [0.607306   0.3926963 ]\n",
      " [0.60564953 0.39435235]\n",
      " [0.6652483  0.33475387]\n",
      " [0.64642864 0.35357326]\n",
      " [0.62921125 0.37079108]\n",
      " [0.6381108  0.36189175]\n",
      " [0.55244595 0.44755626]\n",
      " [0.6248526  0.37514943]\n",
      " [0.6027489  0.39725298]\n",
      " [0.6343474  0.36565548]\n",
      " [0.6637554  0.3362469 ]\n",
      " [0.61393946 0.38606283]\n",
      " [0.6709218  0.32907963]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[11  0]\n",
      " [ 8  0]]\n",
      "Null error in specificity\n",
      "[57.89 57.89  0.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with init data, 300 epochs, softmax\n",
    "probs_TSGL_init = np.array([[0.34468523, 0.65531474],\n",
    "                            [0.4223048,  0.5776952 ],\n",
    "                            [0.66058546, 0.33941454],\n",
    "                            [0.82226074, 0.1777392 ],\n",
    "                            [0.85768410, 0.142316  ],\n",
    "                            [0.79356056, 0.2064394 ],\n",
    "                            [0.43697017, 0.5630298 ],\n",
    "                            [0.23311326, 0.7668868 ],\n",
    "                            [0.06507578, 0.93492424],\n",
    "                            [0.48482734, 0.5151727 ],\n",
    "                            [0.50462850, 0.49537155],\n",
    "                            [0.68665516, 0.3133448 ],\n",
    "                            [0.46980935, 0.5301907 ],\n",
    "                            [0.55152243, 0.4484776 ],\n",
    "                            [0.11421819, 0.8857818 ],\n",
    "                            [0.25776443, 0.74223554],\n",
    "                            [0.45055872, 0.5494413 ],\n",
    "                            [0.75034060, 0.24965943],\n",
    "                            [0.59446220, 0.40553778]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 26.70509, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 2.2621 - accuracy: 0.6136 - val_loss: 26.7051 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 26.70509 to 3.67555, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 58.2839 - accuracy: 0.4318 - val_loss: 3.6756 - val_accuracy: 0.6875 - 16s/epoch - 8s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 3.67555 to 1.50265, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 26.4931 - accuracy: 0.5682 - val_loss: 1.5026 - val_accuracy: 0.3750 - 15s/epoch - 8s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.50265\n",
      "2/2 - 16s - loss: 2.3248 - accuracy: 0.8636 - val_loss: 3.3243 - val_accuracy: 0.7500 - 16s/epoch - 8s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.50265\n",
      "2/2 - 15s - loss: 5.0684 - accuracy: 0.7727 - val_loss: 8.1943 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 1.50265 to 1.20018, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 9.2698 - accuracy: 0.5682 - val_loss: 1.2002 - val_accuracy: 0.6250 - 15s/epoch - 8s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.20018\n",
      "2/2 - 15s - loss: 1.2084 - accuracy: 0.8636 - val_loss: 2.5351 - val_accuracy: 0.6875 - 15s/epoch - 7s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.20018\n",
      "2/2 - 15s - loss: 2.5028 - accuracy: 0.7045 - val_loss: 3.8003 - val_accuracy: 0.5625 - 15s/epoch - 7s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.20018\n",
      "2/2 - 16s - loss: 0.2133 - accuracy: 0.9545 - val_loss: 4.2192 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 1.20018 to 1.14865, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 0.0390 - accuracy: 0.9773 - val_loss: 1.1486 - val_accuracy: 0.6250 - 16s/epoch - 8s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 0.0571 - accuracy: 0.9773 - val_loss: 2.7125 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 0.0243 - accuracy: 1.0000 - val_loss: 4.0032 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 1.7855 - val_accuracy: 0.4375 - 16s/epoch - 8s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.4872 - val_accuracy: 0.6875 - 16s/epoch - 8s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 0.0443 - accuracy: 0.9773 - val_loss: 2.6054 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 7.7914 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 0.0934 - accuracy: 0.9545 - val_loss: 1.2697 - val_accuracy: 0.6875 - 16s/epoch - 8s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 0.3044 - accuracy: 0.9545 - val_loss: 3.1916 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 20.0612 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 1.5893 - accuracy: 0.7955 - val_loss: 1.4669 - val_accuracy: 0.6875 - 15s/epoch - 8s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.14865\n",
      "2/2 - 17s - loss: 0.1682 - accuracy: 0.9545 - val_loss: 1.7332 - val_accuracy: 0.6875 - 17s/epoch - 8s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 0.0381 - accuracy: 0.9773 - val_loss: 12.8400 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 0.2251 - accuracy: 0.9318 - val_loss: 9.8100 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 6.6173 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.1635 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.14865\n",
      "2/2 - 15s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.6162 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.14865\n",
      "2/2 - 16s - loss: 3.9710e-04 - accuracy: 1.0000 - val_loss: 1.5530 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 1.14865 to 0.91193, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 2.7623e-04 - accuracy: 1.0000 - val_loss: 0.9119 - val_accuracy: 0.4375 - 15s/epoch - 8s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.91193 to 0.62420, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 6.1465e-04 - accuracy: 1.0000 - val_loss: 0.6242 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss improved from 0.62420 to 0.53499, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 7.5924e-04 - accuracy: 1.0000 - val_loss: 0.5350 - val_accuracy: 0.6250 - 14s/epoch - 7s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss improved from 0.53499 to 0.52471, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 9.3223e-04 - accuracy: 1.0000 - val_loss: 0.5247 - val_accuracy: 0.6250 - 15s/epoch - 7s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 5.4150e-04 - accuracy: 1.0000 - val_loss: 0.5557 - val_accuracy: 0.6250 - 15s/epoch - 8s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.5482e-04 - accuracy: 1.0000 - val_loss: 0.6435 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.3086e-04 - accuracy: 1.0000 - val_loss: 0.7533 - val_accuracy: 0.4375 - 16s/epoch - 8s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.2213e-04 - accuracy: 1.0000 - val_loss: 0.8887 - val_accuracy: 0.4375 - 16s/epoch - 8s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.6547e-04 - accuracy: 1.0000 - val_loss: 1.0839 - val_accuracy: 0.4375 - 16s/epoch - 8s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.6186e-04 - accuracy: 1.0000 - val_loss: 1.2302 - val_accuracy: 0.4375 - 16s/epoch - 8s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.0857e-04 - accuracy: 1.0000 - val_loss: 1.4955 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 6.7335e-04 - accuracy: 1.0000 - val_loss: 1.5908 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 8.6675e-05 - accuracy: 1.0000 - val_loss: 1.5170 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 8.9841e-05 - accuracy: 1.0000 - val_loss: 1.5266 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.3742e-04 - accuracy: 1.0000 - val_loss: 1.5045 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.2545e-05 - accuracy: 1.0000 - val_loss: 1.4752 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.6774e-05 - accuracy: 1.0000 - val_loss: 1.4767 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 8.8120e-05 - accuracy: 1.0000 - val_loss: 1.4916 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.1161e-04 - accuracy: 1.0000 - val_loss: 1.5956 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 6.5169e-05 - accuracy: 1.0000 - val_loss: 1.6340 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.0535e-05 - accuracy: 1.0000 - val_loss: 1.7203 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.0734e-05 - accuracy: 1.0000 - val_loss: 1.7773 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.4835e-05 - accuracy: 1.0000 - val_loss: 1.8294 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.4696e-05 - accuracy: 1.0000 - val_loss: 1.8779 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.3802e-05 - accuracy: 1.0000 - val_loss: 1.9179 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.3476e-05 - accuracy: 1.0000 - val_loss: 1.9682 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.7041e-05 - accuracy: 1.0000 - val_loss: 2.0058 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 5.1287e-05 - accuracy: 1.0000 - val_loss: 2.1018 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.0164e-05 - accuracy: 1.0000 - val_loss: 2.1271 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.0227e-05 - accuracy: 1.0000 - val_loss: 2.1735 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.0943e-05 - accuracy: 1.0000 - val_loss: 2.2255 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.0453e-04 - accuracy: 1.0000 - val_loss: 2.1823 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.8621e-05 - accuracy: 1.0000 - val_loss: 1.8890 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.5465e-05 - accuracy: 1.0000 - val_loss: 1.6588 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 4.4523e-05 - accuracy: 1.0000 - val_loss: 1.5225 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 3.3395e-05 - accuracy: 1.0000 - val_loss: 1.3991 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.4329e-05 - accuracy: 1.0000 - val_loss: 1.2759 - val_accuracy: 0.3750 - 15s/epoch - 8s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 5.1030e-05 - accuracy: 1.0000 - val_loss: 1.2343 - val_accuracy: 0.3750 - 15s/epoch - 8s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 9.9026e-05 - accuracy: 1.0000 - val_loss: 1.2555 - val_accuracy: 0.3750 - 17s/epoch - 8s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 4.0405e-05 - accuracy: 1.0000 - val_loss: 1.2717 - val_accuracy: 0.3750 - 17s/epoch - 8s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 3.4170e-05 - accuracy: 1.0000 - val_loss: 1.2638 - val_accuracy: 0.3750 - 17s/epoch - 8s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 3.4523e-05 - accuracy: 1.0000 - val_loss: 1.2615 - val_accuracy: 0.3750 - 17s/epoch - 8s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.8226e-05 - accuracy: 1.0000 - val_loss: 1.3186 - val_accuracy: 0.3750 - 16s/epoch - 8s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.8635e-04 - accuracy: 1.0000 - val_loss: 1.4474 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.5333e-05 - accuracy: 1.0000 - val_loss: 1.5201 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.8711e-05 - accuracy: 1.0000 - val_loss: 1.5978 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.1462e-05 - accuracy: 1.0000 - val_loss: 1.6697 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.4865e-05 - accuracy: 1.0000 - val_loss: 1.7271 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.3097e-05 - accuracy: 1.0000 - val_loss: 1.7607 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.6344e-05 - accuracy: 1.0000 - val_loss: 1.8947 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.4537e-05 - accuracy: 1.0000 - val_loss: 1.9618 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 1.2351e-05 - accuracy: 1.0000 - val_loss: 2.0350 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.52471\n",
      "2/2 - 18s - loss: 1.8032e-05 - accuracy: 1.0000 - val_loss: 2.0651 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.52471\n",
      "2/2 - 20s - loss: 1.7466e-05 - accuracy: 1.0000 - val_loss: 2.0979 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 1.7262e-05 - accuracy: 1.0000 - val_loss: 2.1161 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.4595e-05 - accuracy: 1.0000 - val_loss: 2.1360 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.8392e-05 - accuracy: 1.0000 - val_loss: 2.1524 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.2698e-05 - accuracy: 1.0000 - val_loss: 2.1887 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.4709e-04 - accuracy: 1.0000 - val_loss: 2.3030 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 9.1137e-06 - accuracy: 1.0000 - val_loss: 2.2329 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.0618e-05 - accuracy: 1.0000 - val_loss: 2.1698 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.5632e-05 - accuracy: 1.0000 - val_loss: 2.1668 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.3353e-05 - accuracy: 1.0000 - val_loss: 2.1258 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.0292e-05 - accuracy: 1.0000 - val_loss: 2.1214 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 9.2953e-06 - accuracy: 1.0000 - val_loss: 2.0811 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.4253e-05 - accuracy: 1.0000 - val_loss: 2.0661 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 3.3644e-05 - accuracy: 1.0000 - val_loss: 2.0962 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.7997e-05 - accuracy: 1.0000 - val_loss: 2.0849 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.9134e-05 - accuracy: 1.0000 - val_loss: 2.0538 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.6938e-05 - accuracy: 1.0000 - val_loss: 2.0533 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.1185e-05 - accuracy: 1.0000 - val_loss: 2.0528 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.7520e-05 - accuracy: 1.0000 - val_loss: 2.0788 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.7629e-05 - accuracy: 1.0000 - val_loss: 2.0993 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.6168e-05 - accuracy: 1.0000 - val_loss: 2.1209 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.2255e-06 - accuracy: 1.0000 - val_loss: 2.1389 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.1027e-05 - accuracy: 1.0000 - val_loss: 2.1286 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.6333e-05 - accuracy: 1.0000 - val_loss: 2.1577 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.2277e-05 - accuracy: 1.0000 - val_loss: 2.2019 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.0971e-05 - accuracy: 1.0000 - val_loss: 2.2080 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.9314e-05 - accuracy: 1.0000 - val_loss: 2.2241 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.5922e-05 - accuracy: 1.0000 - val_loss: 2.2155 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.1677e-05 - accuracy: 1.0000 - val_loss: 2.2225 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.0468e-05 - accuracy: 1.0000 - val_loss: 2.2396 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.4472e-05 - accuracy: 1.0000 - val_loss: 2.2547 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.8376e-05 - accuracy: 1.0000 - val_loss: 2.2884 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.3183e-05 - accuracy: 1.0000 - val_loss: 2.2917 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.4700e-05 - accuracy: 1.0000 - val_loss: 2.2983 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 9.3847e-06 - accuracy: 1.0000 - val_loss: 2.3139 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 7.5208e-06 - accuracy: 1.0000 - val_loss: 2.3316 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.0073e-05 - accuracy: 1.0000 - val_loss: 2.3901 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.9581e-05 - accuracy: 1.0000 - val_loss: 2.4058 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.1617e-05 - accuracy: 1.0000 - val_loss: 2.4185 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.2703e-05 - accuracy: 1.0000 - val_loss: 2.4690 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.3800e-06 - accuracy: 1.0000 - val_loss: 2.4752 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.2505e-05 - accuracy: 1.0000 - val_loss: 2.4942 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.0969e-05 - accuracy: 1.0000 - val_loss: 2.5612 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 6.0173e-06 - accuracy: 1.0000 - val_loss: 2.5389 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 9.1355e-06 - accuracy: 1.0000 - val_loss: 2.5338 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.6813e-06 - accuracy: 1.0000 - val_loss: 2.5215 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.4074e-05 - accuracy: 1.0000 - val_loss: 2.5111 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.0059e-05 - accuracy: 1.0000 - val_loss: 2.5216 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.3262e-05 - accuracy: 1.0000 - val_loss: 2.5106 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 9.0379e-06 - accuracy: 1.0000 - val_loss: 2.5058 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.5404e-05 - accuracy: 1.0000 - val_loss: 2.5538 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.5677e-05 - accuracy: 1.0000 - val_loss: 2.5805 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.1755e-05 - accuracy: 1.0000 - val_loss: 2.5356 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.1476e-05 - accuracy: 1.0000 - val_loss: 2.5202 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.0566e-05 - accuracy: 1.0000 - val_loss: 2.4999 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.5074e-05 - accuracy: 1.0000 - val_loss: 2.4650 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.2566e-05 - accuracy: 1.0000 - val_loss: 2.5666 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.1795e-05 - accuracy: 1.0000 - val_loss: 2.5218 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 9.6960e-06 - accuracy: 1.0000 - val_loss: 2.5230 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 9.3088e-06 - accuracy: 1.0000 - val_loss: 2.5279 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.1533e-05 - accuracy: 1.0000 - val_loss: 2.5331 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 6.8571e-06 - accuracy: 1.0000 - val_loss: 2.4942 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.4862e-05 - accuracy: 1.0000 - val_loss: 2.4890 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 8.0355e-06 - accuracy: 1.0000 - val_loss: 2.4648 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.0270e-05 - accuracy: 1.0000 - val_loss: 2.5188 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.8752e-05 - accuracy: 1.0000 - val_loss: 2.5241 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.7185e-05 - accuracy: 1.0000 - val_loss: 2.5676 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.4614e-06 - accuracy: 1.0000 - val_loss: 2.5082 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.3353e-05 - accuracy: 1.0000 - val_loss: 2.4956 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.6100e-05 - accuracy: 1.0000 - val_loss: 2.4933 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 6.6945e-06 - accuracy: 1.0000 - val_loss: 2.4343 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.8540e-06 - accuracy: 1.0000 - val_loss: 2.4057 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.0002e-05 - accuracy: 1.0000 - val_loss: 2.3866 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.0799e-05 - accuracy: 1.0000 - val_loss: 2.3704 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 8.8183e-06 - accuracy: 1.0000 - val_loss: 2.3384 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.0818e-05 - accuracy: 1.0000 - val_loss: 2.3339 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.8167e-06 - accuracy: 1.0000 - val_loss: 2.3632 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.1253e-06 - accuracy: 1.0000 - val_loss: 2.3268 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.2039e-06 - accuracy: 1.0000 - val_loss: 2.3180 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 9.5690e-06 - accuracy: 1.0000 - val_loss: 2.3206 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 8.5476e-06 - accuracy: 1.0000 - val_loss: 2.3383 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.2468e-05 - accuracy: 1.0000 - val_loss: 2.3728 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.3262e-05 - accuracy: 1.0000 - val_loss: 2.4094 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.8192e-06 - accuracy: 1.0000 - val_loss: 2.3771 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 3.1146e-05 - accuracy: 1.0000 - val_loss: 2.3716 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.6864e-06 - accuracy: 1.0000 - val_loss: 2.3135 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 8.6749e-06 - accuracy: 1.0000 - val_loss: 2.2606 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.0235e-05 - accuracy: 1.0000 - val_loss: 2.2367 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 8.4880e-06 - accuracy: 1.0000 - val_loss: 2.2307 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.1576e-05 - accuracy: 1.0000 - val_loss: 2.2972 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.8764e-06 - accuracy: 1.0000 - val_loss: 2.2514 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.2105e-05 - accuracy: 1.0000 - val_loss: 2.2230 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.1524e-06 - accuracy: 1.0000 - val_loss: 2.2086 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.0089e-05 - accuracy: 1.0000 - val_loss: 2.1820 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.9979e-06 - accuracy: 1.0000 - val_loss: 2.2873 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.3914e-06 - accuracy: 1.0000 - val_loss: 2.2724 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.4713e-05 - accuracy: 1.0000 - val_loss: 2.2659 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.2066e-06 - accuracy: 1.0000 - val_loss: 2.2172 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.0823e-05 - accuracy: 1.0000 - val_loss: 2.2102 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.1795e-06 - accuracy: 1.0000 - val_loss: 2.2162 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.1595e-05 - accuracy: 1.0000 - val_loss: 2.2320 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 6.4911e-06 - accuracy: 1.0000 - val_loss: 2.3089 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 6.5347e-06 - accuracy: 1.0000 - val_loss: 2.2956 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 8.9675e-06 - accuracy: 1.0000 - val_loss: 2.2984 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 5.8845e-06 - accuracy: 1.0000 - val_loss: 2.2747 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.3167e-05 - accuracy: 1.0000 - val_loss: 2.3984 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.9709e-06 - accuracy: 1.0000 - val_loss: 2.4065 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 8.6722e-06 - accuracy: 1.0000 - val_loss: 2.3909 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 9.5554e-06 - accuracy: 1.0000 - val_loss: 2.3687 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.1858e-06 - accuracy: 1.0000 - val_loss: 2.3608 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.1105e-05 - accuracy: 1.0000 - val_loss: 2.3594 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.9627e-06 - accuracy: 1.0000 - val_loss: 2.3543 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.9489e-06 - accuracy: 1.0000 - val_loss: 2.3378 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.3695e-05 - accuracy: 1.0000 - val_loss: 2.3506 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.7301e-06 - accuracy: 1.0000 - val_loss: 2.3649 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.52471\n",
      "2/2 - 18s - loss: 9.4523e-06 - accuracy: 1.0000 - val_loss: 2.4003 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.0252e-05 - accuracy: 1.0000 - val_loss: 2.4279 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.8327e-06 - accuracy: 1.0000 - val_loss: 2.3933 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.9247e-05 - accuracy: 1.0000 - val_loss: 2.4193 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.1292e-05 - accuracy: 1.0000 - val_loss: 2.4273 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.7618e-06 - accuracy: 1.0000 - val_loss: 2.4116 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 3.5789e-06 - accuracy: 1.0000 - val_loss: 2.4137 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 4.3565e-06 - accuracy: 1.0000 - val_loss: 2.3991 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 6.8652e-06 - accuracy: 1.0000 - val_loss: 2.3966 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 1.5903e-05 - accuracy: 1.0000 - val_loss: 2.4211 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 4.4107e-06 - accuracy: 1.0000 - val_loss: 2.3845 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 3.1644e-06 - accuracy: 1.0000 - val_loss: 2.3802 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 7.1930e-06 - accuracy: 1.0000 - val_loss: 2.3748 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 2.2813e-05 - accuracy: 1.0000 - val_loss: 2.4544 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.52471\n",
      "2/2 - 14s - loss: 7.1902e-06 - accuracy: 1.0000 - val_loss: 2.4616 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.1198e-06 - accuracy: 1.0000 - val_loss: 2.4639 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.5157e-06 - accuracy: 1.0000 - val_loss: 2.4967 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.9823e-06 - accuracy: 1.0000 - val_loss: 2.5000 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 7.4284e-06 - accuracy: 1.0000 - val_loss: 2.4790 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 1.2562e-05 - accuracy: 1.0000 - val_loss: 2.5234 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 3.9203e-06 - accuracy: 1.0000 - val_loss: 2.5191 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.52471\n",
      "2/2 - 18s - loss: 8.5612e-06 - accuracy: 1.0000 - val_loss: 2.5448 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.52471\n",
      "2/2 - 18s - loss: 4.7060e-06 - accuracy: 1.0000 - val_loss: 2.5216 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.52471\n",
      "2/2 - 18s - loss: 9.6956e-06 - accuracy: 1.0000 - val_loss: 2.5292 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 6.4533e-06 - accuracy: 1.0000 - val_loss: 2.5343 - val_accuracy: 0.3125 - 17s/epoch - 9s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.6112e-06 - accuracy: 1.0000 - val_loss: 2.5516 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.5211e-06 - accuracy: 1.0000 - val_loss: 2.5485 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 8.8562e-06 - accuracy: 1.0000 - val_loss: 2.5800 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.2728e-06 - accuracy: 1.0000 - val_loss: 2.5933 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.9724e-06 - accuracy: 1.0000 - val_loss: 2.6028 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.1111e-05 - accuracy: 1.0000 - val_loss: 2.6192 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.9989e-06 - accuracy: 1.0000 - val_loss: 2.5814 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 6.5507e-06 - accuracy: 1.0000 - val_loss: 2.6007 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.1844e-05 - accuracy: 1.0000 - val_loss: 2.6354 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.8139e-05 - accuracy: 1.0000 - val_loss: 2.7045 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.2640e-06 - accuracy: 1.0000 - val_loss: 2.6527 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.9881e-05 - accuracy: 1.0000 - val_loss: 2.6891 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.0503e-06 - accuracy: 1.0000 - val_loss: 2.6611 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.7490e-06 - accuracy: 1.0000 - val_loss: 2.6427 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.4510e-06 - accuracy: 1.0000 - val_loss: 2.6375 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.1527e-05 - accuracy: 1.0000 - val_loss: 2.6627 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 2.9775e-06 - accuracy: 1.0000 - val_loss: 2.6675 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.3429e-06 - accuracy: 1.0000 - val_loss: 2.6803 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.4480e-06 - accuracy: 1.0000 - val_loss: 2.6795 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.52471\n",
      "2/2 - 17s - loss: 7.2091e-06 - accuracy: 1.0000 - val_loss: 2.6776 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.6356e-06 - accuracy: 1.0000 - val_loss: 2.6744 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.4131e-06 - accuracy: 1.0000 - val_loss: 2.6957 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.1612e-04 - accuracy: 1.0000 - val_loss: 2.6837 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.0709e-05 - accuracy: 1.0000 - val_loss: 2.5040 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.52471\n",
      "2/2 - 18s - loss: 7.8865e-06 - accuracy: 1.0000 - val_loss: 2.3348 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.9552e-06 - accuracy: 1.0000 - val_loss: 2.2041 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.2719e-06 - accuracy: 1.0000 - val_loss: 2.1046 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.5295e-06 - accuracy: 1.0000 - val_loss: 2.0930 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 7.2391e-06 - accuracy: 1.0000 - val_loss: 2.0166 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.7772e-05 - accuracy: 1.0000 - val_loss: 1.9974 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 9.8232e-06 - accuracy: 1.0000 - val_loss: 1.9472 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.2990e-06 - accuracy: 1.0000 - val_loss: 1.9167 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.52471\n",
      "2/2 - 18s - loss: 5.6732e-06 - accuracy: 1.0000 - val_loss: 1.9020 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.5539e-06 - accuracy: 1.0000 - val_loss: 1.8860 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.6051e-06 - accuracy: 1.0000 - val_loss: 1.8931 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.5998e-05 - accuracy: 1.0000 - val_loss: 1.9703 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.6373e-06 - accuracy: 1.0000 - val_loss: 1.9802 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.9595e-06 - accuracy: 1.0000 - val_loss: 2.0003 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.7836e-06 - accuracy: 1.0000 - val_loss: 2.0346 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.6292e-06 - accuracy: 1.0000 - val_loss: 2.0458 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.2126e-06 - accuracy: 1.0000 - val_loss: 2.0351 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.4368e-06 - accuracy: 1.0000 - val_loss: 2.0550 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 9.3841e-06 - accuracy: 1.0000 - val_loss: 2.1150 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.1069e-06 - accuracy: 1.0000 - val_loss: 2.1420 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.3101e-06 - accuracy: 1.0000 - val_loss: 2.1331 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.2417e-06 - accuracy: 1.0000 - val_loss: 2.1468 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.0628e-05 - accuracy: 1.0000 - val_loss: 2.1790 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.6379e-06 - accuracy: 1.0000 - val_loss: 2.1606 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.9610e-06 - accuracy: 1.0000 - val_loss: 2.1555 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.5055e-06 - accuracy: 1.0000 - val_loss: 2.1668 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.4509e-06 - accuracy: 1.0000 - val_loss: 2.1762 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.2749e-06 - accuracy: 1.0000 - val_loss: 2.2185 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.2674e-06 - accuracy: 1.0000 - val_loss: 2.2313 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.7815e-06 - accuracy: 1.0000 - val_loss: 2.2392 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.2234e-06 - accuracy: 1.0000 - val_loss: 2.2445 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.2308e-06 - accuracy: 1.0000 - val_loss: 2.2724 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.9925e-06 - accuracy: 1.0000 - val_loss: 2.2665 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.7659e-06 - accuracy: 1.0000 - val_loss: 2.2633 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 5.1449e-06 - accuracy: 1.0000 - val_loss: 2.2366 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 1.0530e-05 - accuracy: 1.0000 - val_loss: 2.2869 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.5591e-06 - accuracy: 1.0000 - val_loss: 2.3097 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 3.3460e-06 - accuracy: 1.0000 - val_loss: 2.3557 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.2291e-06 - accuracy: 1.0000 - val_loss: 2.3541 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 8.0468e-05 - accuracy: 1.0000 - val_loss: 2.4388 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.3637e-06 - accuracy: 1.0000 - val_loss: 2.3093 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 7.1794e-06 - accuracy: 1.0000 - val_loss: 2.2204 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.7920e-06 - accuracy: 1.0000 - val_loss: 2.1496 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.6166e-06 - accuracy: 1.0000 - val_loss: 2.0945 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 4.7195e-06 - accuracy: 1.0000 - val_loss: 2.0423 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 6.5832e-06 - accuracy: 1.0000 - val_loss: 2.0788 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 1.5049e-05 - accuracy: 1.0000 - val_loss: 2.0610 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.1610e-06 - accuracy: 1.0000 - val_loss: 2.0823 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.9829e-06 - accuracy: 1.0000 - val_loss: 2.0698 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 2.3053e-05 - accuracy: 1.0000 - val_loss: 2.1098 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.52471\n",
      "2/2 - 15s - loss: 5.8574e-06 - accuracy: 1.0000 - val_loss: 2.0858 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.4405e-06 - accuracy: 1.0000 - val_loss: 2.0706 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.9281e-06 - accuracy: 1.0000 - val_loss: 2.0572 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.2885e-06 - accuracy: 1.0000 - val_loss: 2.0544 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 5.7246e-06 - accuracy: 1.0000 - val_loss: 2.0481 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.52471\n",
      "2/2 - 16s - loss: 4.5624e-06 - accuracy: 1.0000 - val_loss: 2.0665 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Classification accuracy: 0.554017 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAHFCAYAAABb+zt/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSXUlEQVR4nO3deVgVZfsH8O9hOxx2kU0Q2V1QVBQXsIJU1DJDrdS0zH1f0HLhdaNUEC00NU2tFHsz1zQzF8wFc1dwRQQX3AoEFUUBWef3Bz/P2wkUkDPOOZzvp2uuy3nmmefcg5A39/PMjEwQBAFEREREItGTOgAiIiKq2ZhsEBERkaiYbBAREZGomGwQERGRqJhsEBERkaiYbBAREZGomGwQERGRqJhsEBERkaiYbBAREZGomGwQVdGaNWsgk8meux08eBAA4Orq+tw+QUFBZcY9f/48Bg8eDA8PDygUCigUCnh5eWH48OE4ffq0St/w8HDIZDLY2dnh8ePHZcZydXXFO++881LXt2zZMqxZs6ZK52RnZ2PatGmoX78+TExM4OTkhA8++ACJiYkVnpuWlobp06fD398fNjY2sLCwQMuWLbFy5UoUFxe/1DUQkWYxkDoAIm21evVqNGzYsEy7t7e38s/t2rXDl19+WaaPhYWFyv6KFSswZswYNGjQAOPHj0fjxo0hk8mQlJSEn3/+Ga1atcLVq1fh4eGhcl5mZibmz5+P2bNnq+mqSpMNGxsbDBgwoNLndOvWDadPn0Z4eDj8/Pxw584dfPHFF/D398eFCxfg4uLy3HPj4+Oxdu1a9O/fHzNmzIChoSF27dqFkSNH4vjx4/jhhx/UcFVEJCmBiKpk9erVAgDh1KlTL+zn4uIidO3atcLxDh8+LOjp6QndunUT8vPzy+2zceNG4a+//lLuz5o1SwAgdOnSRTA1NRXS0tJe6rPL07hxYyEwMLDS/a9cuSIAEKZPn67SfvToUQGAEB0d/cLzHzx4IBQUFJRpHz16tABAuHXrVqVjISLNxGkUIolFRERAX18fK1asgJGRUbl9PvjgAzg6OpZpnzNnDoqKihAeHl7h5xQUFGDOnDlo2LAh5HI5bG1tMXDgQGRmZir7uLq6IjExEXFxccopH1dX1xeOa2hoCACwtLRUabeysgIAGBsbv/D8WrVqKcf4p9atWwMA7ty5U9GlEZGGY7JB9JKKi4tRVFSksv17jYEgCGX6FBUVQfj/ly0XFxfjwIED8PPzQ506daocg4uLC0aNGoXvv/8eKSkpz+1XUlKCkJAQzJs3D3379sXvv/+OefPmYe/evQgKCkJeXh4AYOvWrXB3d4evry+OHTuGY8eOYevWrRXGEBISgoULF+LAgQN48uQJLl++jHHjxqFevXro06dPla8LAPbv3w8DAwPUr1//pc4nIg0idWmFSNs8m0Ypb9PX11f2c3FxeW6/2bNnC4IgCOnp6QIAoU+fPmU+p6ioSCgsLFRuJSUlymPPplEyMzOFe/fuCZaWlsJ7772n8tn/nEb5+eefBQDCli1bVD7j1KlTAgBh2bJlyraqTqMIgiAUFBQIQ4cOVbnGpk2bCqmpqVUa55k9e/YIenp6woQJE17qfCLSLFwgSvSS1q5di0aNGqm0yWQylf3XXnsNCxcuLHOuk5NTheO3bNkS586dU+4vWLAAn332WZl+tWvXxpQpU/Cf//wHJ06cQJs2bcr02bFjB6ysrNCtWzcUFRUp25s3bw4HBwccPHgQI0eOfGE8xcXFyooMAOjp6UFPr7Q4OnLkSGzduhULFy5EixYtkJ6ejgULFqB9+/Y4cODACxeI/ltCQgJ69eqFtm3bIjIystLnEZHmYrJB9JIaNWoEPz+/F/axtLR8YR8bGxsoFArcvHmzzLF169YhNzcXaWlpePfdd1/4OaGhoVi6dCkmT56MuLi4Msfv3r2Lhw8fPndNyL179144PgB4eHioxDlr1iyEh4dj9+7d+P7777Fp0ya8//77yuOdOnWCq6srwsPDsXr16grHB4AzZ84gODgYXl5e2LlzJ+RyeaXOIyLNxmSDSEL6+vpo3749YmNjkZaWprJu49kttDdu3KhwHIVCgfDwcAwbNgy///57meM2NjaoXbs2du/eXe755ubmFX7Gb7/9hvz8fOX+swWrZ8+eBQC0atVKpb+VlRU8PT1x8eLFCscGShONjh07wsXFBbGxsWUWnBKR9mKyQSSxsLAw7Nq1CyNGjMDmzZvLvTOjMgYNGoSFCxdi6tSpKCkpUTn2zjvvYP369SguLi53muWf5HK5csHoP/n4+JTb/1nScfz4cZXpkvv37yMlJQUdOnSoMPazZ8+iY8eOqFu3Lvbu3YtatWpVeA4RaQ8mG0Qv6eLFiyrrH57x8PCAra0tAODhw4c4fvx4mT5yuRy+vr4ASh/89c0332Ds2LFo0aIFhg0bhsaNG0NPTw9paWnYsmULgLIPAvs3fX19REREoEePHgCApk2bKo/16dMHP/30E95++22MHz8erVu3hqGhIe7cuYMDBw4gJCREeZ6Pjw/Wr1+PDRs2wN3dHcbGxs9NNACgZ8+emDlzJkaOHIk7d+6gRYsWSEtLw4IFC5Cbm4vx48er9JfJZAgMDFQ+aTU5ORkdO3YEAMydOxdXrlzBlStXyv16EpGWknqFKpG2edHdKACEVatWCYLw4rtRnJycyox79uxZYeDAgYKbm5sgl8sFY2NjwdPTU+jfv7+wb98+lb7/vBvl3wICAgQAZR7qVVhYKHz55ZdCs2bNBGNjY8HMzExo2LChMHz4cOHKlSvKfjdu3BA6deokmJubCwAEFxeXCr8maWlpwpgxYwRPT0/B2NhYcHR0FLp27SocO3ZMpd/jx4/L3H1T0ddz9erVFX4+EWk2mSD8Y3k5EZGIdu7ciXfeeQfnzp17YbWEiGoWPtSLiF6ZAwcOoE+fPkw0iHQMKxtEREQkKlY2iIiISFRMNoiIiGqoQ4cOoVu3bnB0dIRMJsO2bdtUjguCgPDwcDg6OkKhUCAoKAiJiYkqffLz8zF27FjY2NjA1NQU7777bpVfkMhkg4iIqIbKyclBs2bNsHTp0nKPz58/H9HR0Vi6dClOnToFBwcHBAcH4/Hjx8o+oaGh2Lp1K9avX4/Dhw/jyZMneOedd8q8ePJFuGaDiIhIB8hkMmzduhXdu3cHUFrVcHR0RGhoKKZMmQKgtIphb2+PqKgoDB8+HI8ePYKtrS1+/PFH9O7dGwDw999/w9nZGTt37kTnzp0r9dmsbBAREWmJ/Px8ZGdnq2z/fI1AVaSmpiI9PR2dOnVStsnlcgQGBuLo0aMAgPj4eBQWFqr0cXR0RJMmTZR9KoNPECUiIhKZwneMWsaZEmKDzz//XKXt2UsRqyo9PR0AYG9vr9Jub2+vfOlieno6jIyMyrxCwN7eXnl+ZdTYZKP94mNSh0CkcfaP88cPp25JHQaRRhnUqp7UIVRaWFgYJk6cqNJW3bcjy2QylX1BEMq0/Vtl+vwTp1GIiIjEJtNTyyaXy2FhYaGyvWyy4eDgAABlKhQZGRnKaoeDgwMKCgqQlZX13D6VwWSDiIhIbDKZejY1cnNzg4ODA/bu3atsKygoQFxcHAICAgAALVu2hKGhoUqftLQ0XLx4UdmnMmrsNAoREZHGkEnzu/2TJ09w9epV5X5qairOnj0La2tr1KtXD6GhoYiIiICXlxe8vLwQEREBExMT9O3bFwBgaWmJwYMH49NPP0Xt2rVhbW2Nzz77DD4+Psq3NVcGkw0iIqIa6vTp03jzzTeV+8/We3zyySdYs2YNJk+ejLy8PIwaNQpZWVlo06YNYmNjYW5urjxn4cKFMDAwQK9evZCXl4cOHTpgzZo10NfXr3QcNfY5G1wgSlQWF4gSlfUqFogqWk2suFMl5J2KVss4rxorG0RERGKTaBpFU+j21RMREZHoWNkgIiISm5rvJNE2TDaIiIjExmkUIiIiIvGwskFERCQ2TqMQERGRqDiNQkRERCQeVjaIiIjExmkUIiIiEpWOT6Mw2SAiIhKbjlc2dDvVIiIiItGxskFERCQ2TqMQERGRqHQ82dDtqyciIiLRsbJBREQkNj3dXiDKZIOIiEhsnEYhIiIiEg8rG0RERGLT8edsMNkgIiISG6dRiIiIiMTDygYREZHYOI1CREREotLxaRQmG0RERGLT8cqGbqdaREREJDpWNoiIiMTGaRQiIiISFadRiIiIiMTDygYREZHYOI1CREREouI0ChEREZF4WNkgIiISG6dRiIiISFQ6nmzo9tUTERGR6FjZICIiEpuOLxBlskFERCQ2HZ9GYbJBREQkNh2vbOh2qkVERESiY2WDiIhIbJxGISIiIlFxGoWIiIhIPKxsEBERiUym45UNJhtEREQi0/Vkg9MoREREJCpWNoiIiMSm24UNJhtERERi4zQKERERkYhY2SAiIhKZrlc2mGwQERGJjMkGERERiUrXkw2u2SAiIiJRsbJBREQkNt0ubDDZICIiEhunUYiIiIhExMoGERGRyHS9ssFkg4iISGS6nmxwGoWIiIhExcoGERGRyHS9ssFkg4iISGy6nWtIk2xMnDix0n2jo6NFjISIiIjEJkmycebMGZX9+Ph4FBcXo0GDBgCAlJQU6Ovro2XLllKER0REpFacRpHAgQMHlH+Ojo6Gubk5YmJiUKtWLQBAVlYWBg4ciNdff12K8IiIiNRK15MNye9G+eqrrxAZGalMNACgVq1amDNnDr766isJIyMiIlIPmUymlk1bSZ5sZGdn4+7du2XaMzIy8PjxYwkiIiIi0n5FRUWYPn063NzcoFAo4O7uji+++AIlJSXKPoIgIDw8HI6OjlAoFAgKCkJiYqLaY5E82ejRowcGDhyIzZs3486dO7hz5w42b96MwYMHo2fPnlKHR0REVH0yNW1VEBUVhW+//RZLly5FUlIS5s+fjwULFmDJkiXKPvPnz0d0dDSWLl2KU6dOwcHBAcHBwWr/ZV/yW1+//fZbfPbZZ/joo49QWFgIADAwMMDgwYOxYMECiaMjIiKqPimmQI4dO4aQkBB07doVAODq6oqff/4Zp0+fBlBa1Vi0aBGmTZum/OU+JiYG9vb2WLduHYYPH662WCSvbJiYmGDZsmW4f/8+zpw5g4SEBDx48ADLli2Dqamp1OERERFpjPz8fGRnZ6ts+fn55fZ97bXXsG/fPqSkpAAAzp07h8OHD+Ptt98GAKSmpiI9PR2dOnVSniOXyxEYGIijR4+qNW7Jk41n0tLSkJaWhvr168PU1BSCIEgdEhERkVqoa4FoZGQkLC0tVbbIyMhyP3PKlCn48MMP0bBhQxgaGsLX1xehoaH48MMPAQDp6ekAAHt7e5Xz7O3tlcfURfJplPv376NXr144cOAAZDIZrly5And3dwwZMgRWVla8I4WIiLSeuqZRwsLCyjwYUy6Xl9t3w4YN+O9//4t169ahcePGOHv2LEJDQ+Ho6IhPPvnkubEJgqD2aR/JKxsTJkyAoaEhbt26BRMTE2V77969sXv3bgkjIyIi0ixyuRwWFhYq2/OSjUmTJmHq1Kno06cPfHx88PHHH2PChAnKSoiDgwMAlKliZGRklKl2VJfkyUZsbCyioqJQt25dlXYvLy/cvHlToqiIiIjUR4rnbOTm5kJPT/WfeX19feWtr25ubnBwcMDevXuVxwsKChAXF4eAgIDqX/Q/SD6NkpOTo1LReObevXvPzdaIiIi0igTP4+rWrRvmzp2LevXqoXHjxjhz5gyio6MxaNCg0pBkMoSGhiIiIgJeXl7w8vJCREQETExM0LdvX7XGInmy8cYbb2Dt2rWYPXs2gNKLLykpwYIFC/Dmm29KHB0REZF2WrJkCWbMmIFRo0YhIyMDjo6OGD58OGbOnKnsM3nyZOTl5WHUqFHIyspCmzZtEBsbC3Nzc7XGIhMkvu3j0qVLCAoKQsuWLbF//368++67SExMxIMHD3DkyBF4eHi81LjtFx9Tc6RE2m//OH/8cOqW1GEQaZRBreqJ/hlOI7eqZZy/lvdQyzivmuRrNry9vXH+/Hm0bt0awcHByMnJQc+ePXHmzJmXTjSIiIg0ia6/G0XyaRSgdEXs559/LnUYREREotDmREEdJK9s7N69G4cPH1buf/PNN2jevDn69u2LrKwsCSMjIiIidZA82Zg0aRKys7MBABcuXMDEiRPx9ttv4/r162UeXEJERKSVJHgRmyaRfBolNTUV3t7eAIAtW7agW7duiIiIQEJCgvL57URERNqM0ygSMzIyQm5uLgDgjz/+UL4QxtraWlnxICIiIu0leWXjtddew8SJE9GuXTucPHkSGzZsAACkpKSUeaooScvG1AhD29VDaxcryA30cOfhUyz44xquZOZAX0+GQW2d0ca1FupYypGTX4yE24+w6uhN3M8pfO6YnRvZYkqwZ9n2b46jsJgv4yPNduaP33Bm3294lHkXAGBT1wUBPT6CR7PWAICoj4LLPS+oz1C0eadXuccuHNqDnSu/LNP+6Q+/w8DISE2R06um65UNyZONpUuXYtSoUdi8eTOWL18OJycnAMCuXbvQpUsXiaOjZ8zk+lj8QWOcvZONsO2XkZVbCEdLOXIKigAAxgZ68LIzxY+n7uB6Zg7MjA0w+g1XzHmnIUZuuPDCsZ/kF+GTH8+qtDHRIG1gbm2DwN6DUcu+9P9bF/+MxS/RszBg7nLY1nXF6KUbVPpfP3cSu76LRoPWr79wXCOFCYYuWK3SxkRDuzHZkFi9evWwY8eOMu0LFy6UIBp6ng9bOiHjcQHm/3FN2Xb3cb7yzzkFxZi8LUnlnCUHU7G8T1PYmRkh40nBC8fPyn1+9YNIU3m28FfZf6PXIJzZtwN/X02CbV1XmFlZqxy/mnAMLo2awcquzgvHlclkZc4l0maSJxsJCQkwNDSEj48PAODXX3/F6tWr4e3tjfDwcBgxm9cI/u61cPrmQ8x6qz6aOlngXk4Btp9Px++JGc89x1RugBJBwJOC4heOrTDUx88DWkBPD7iamYvVx2/hamauui+BSFQlJcW4fOIQCvOfwsnLu8zxnEdZuHb2BLoOn1zhWAVP87B8fD+UlJTA3sUDr78/APauZacbSXvoemVD8gWiw4cPR0pKCgDg+vXr6NOnD0xMTLBp0yZMnlzxDyW9Go4WxnjXxwF3HuZhyq+X8NuFdIwJdENwQ5ty+xvqyzA0oB72Jd9D7guSjVtZeYjaexXTdlzGnN1XUFBcgsXvN4GTpbFYl0KkVpm3UxE9uBu+HPA2Yld/jR6hs2Dj5FKm38U/Y2FkbIL6fq+9cDxrR2d0HTYJ7038Au+O/g/0DY3w3y9C8SD9jliXQK+Cjt/6KnmykZKSgubNmwMANm3ahDfeeAPr1q3DmjVrsGXLlgrPz8/PR3Z2tsqWn59f4XlUNTIZcCUzB98fu42rmbnYcTEDv1+8i3d9HMr01deTYUaX+tCTAV8fTH3huEnpT/BH8j1cv5eLC38/xhc7U3Dn4VP0aFZ2XCJNZF2nLgbO/RYfhy+Gb4du+H3FAtz762aZfufj9sA7oH2Fay+cPL3R+LWOsHPxgHNDH3QfOx3WDk5IiP1VrEsgEp3kyYYgCCgpKQFQeuvrs2drODs74969exWeHxkZCUtLS5UtMjJS1Jh10YOcQtx4oDq1cSsrD/bmcpU2fT0ZZr1VH3Us5Ji0LemFVY3yCACS7z6BkxUrG6Qd9A0MUcvBCXXcGyCw92DY1XPH6d2qL926ffkCHqTdRrOgt6o8vkxPDw7uDfAg/S91hUwS0PV3o0iebPj5+WHOnDn48ccfERcXh65duwIofdiXvb19heeHhYXh0aNHKltYWJjYYeuci2mP4WylUGmra2Wsskj0WaLhZGWMz7ZdQvbTopf6LA9bUzx4we2yRBpNEFBcpLog+nzcLji4ecHOpeovlxQEARk3r3HBqJZjsiGxRYsWISEhAWPGjMG0adPg6Vm6CGrz5s0ICAio8Hy5XA4LCwuVTS6XV3geVc3mM3/D28EMff2c4GhpjPb1bdC1iT22nU8HAOjJgPC366O+nSnm7rkCPZkMtUwMUcvEEAZ6//sBmRrsiSEB/3udc//WdeFXzxJ1LOTwsDHBpA4e8LQxwW8X777yaySqqrgN3+P25Qt4lJmOzNupOLTxB9xKOg/vgA7KPvm5OUg++SeaPqeqsePbKMRt+F65f/iXH3H9/Ck8zEjD3ZtXsWvVV8i4dQ3NO7wj+vWQeGQy9WzaSvK7UZo2bYoLF8o+h2HBggXQ19eXICIqT3JGDmb+nowhAS7o37ou0rKfYtmhG9iXXDrVZWsmRzv30t+8vuvbTOXcCVsSce6v0qfB2pkboUT43zM0zOT6+LS9B2qZGiInvxhXM3MQuiURl+8+eUVXRvTycrIfYse3Uch5+AByE1PYOrvhg8kRcPNpqeyTdPwgBEGAt3/7csfIvpeh8htrfu4T7Pl+EXIeZUFuYgo7Fw/0nR4NR4+Gol8PkVhkgiBI/vSkhw8fYvPmzbh27RomTZoEa2trJCQkwN7eXvmQr6pqv/iYmqMk0n77x/njh1O3pA6DSKMMalWv4k7V5DVpt1rGubJAOx92KXll4/z58+jQoQOsrKxw48YNDB06FNbW1ti6dStu3ryJtWvXSh0iERFRtWjzFIg6SL5mY+LEiRg4cCCuXLkCY+P/3YHw1ltv4dChQxJGRkREROogeWXj1KlTWLFiRZl2JycnpKenSxARERGRemnznSTqIHmyYWxsXO6r5JOTk2FraytBREREROql47mG9NMoISEh+OKLL1BYWPpcBZlMhlu3bmHq1Kl47733JI6OiIiIqkvyZOPLL79EZmYm7OzskJeXh8DAQHh6esLc3Bxz586VOjwiIqJq09OTqWXTVpJPo1hYWODw4cPYv38/EhISUFJSghYtWqBjx45Sh0ZERKQWuj6NImmyUVRUBGNjY5w9exbt27dH+/blP/SGiIiItJekyYaBgQFcXFxQXFy1l3URERFpE12/G0XyNRvTp09HWFgYHjx4IHUoREREouC7USS2ePFiXL16FY6OjnBxcYGpqanK8YSEBIkiIyIiUg9dr2xInmyEhITo/F8CERFRTSZ5shEeHi51CERERKLS9V+qJV+z4e7ujvv375dpf/jwIdzd3SWIiIiISL10fc2G5MnGjRs3yr0bJT8/H3fu3JEgIiIiIlInyaZRtm/frvzznj17YGlpqdwvLi7Gvn374ObmJkVoREREaqXr0yiSJRvdu3cHUPoX8Mknn6gcMzQ0hKurK7766isJIiMiIlIvHc81pEs2SkpKAABubm44deoUbGxspAqFiIiIRCTZmo0TJ05g165dSE1NVSYaa9euhZubG+zs7DBs2DDk5+dLFR4REZHayGQytWzaSrJkY9asWTh//rxy/8KFCxg8eDA6duyIqVOn4rfffkNkZKRU4REREakN70aRyLlz59ChQwfl/vr169GmTRusWrUKEydOxOLFi7Fx40apwiMiIiI1kWzNRlZWFuzt7ZX7cXFx6NKli3K/VatWuH37thShERERqZU2T4Gog2SVDXt7e6SmpgIACgoKkJCQAH9/f+Xxx48fw9DQUKrwiIiI1IbTKBLp0qULpk6dij///BNhYWEwMTHB66+/rjx+/vx5eHh4SBUeERGR2uj6AlHJplHmzJmDnj17IjAwEGZmZoiJiYGRkZHy+A8//IBOnTpJFR4RERGpiWTJhq2tLf788088evQIZmZm0NfXVzm+adMmmJmZSRQdERGR+mhxUUItJH/r6z8fU/5P1tbWrzgSIiIicWjzFIg6SP4iNiIiIqrZJK9sEBER1XQ6XthgskFERCQ2TqMQERERiYiVDSIiIpHpeGGDyQYREZHYOI1CREREJCJWNoiIiESm65UNJhtEREQi0/Fcg8kGERGR2HS9ssE1G0RERCQqVjaIiIhEpuOFDSYbREREYuM0ChEREZGIWNkgIiISmY4XNphsEBERiU1Px7MNTqMQERGRqFjZICIiEpmOFzaYbBAREYmNd6MQERGRqPRk6tmq6q+//sJHH32E2rVrw8TEBM2bN0d8fLzyuCAICA8Ph6OjIxQKBYKCgpCYmKjGKy/FZIOIiKgGysrKQrt27WBoaIhdu3bh0qVL+Oqrr2BlZaXsM3/+fERHR2Pp0qU4deoUHBwcEBwcjMePH6s1Fk6jEBERiUyKaZSoqCg4Oztj9erVyjZXV1flnwVBwKJFizBt2jT07NkTABATEwN7e3usW7cOw4cPV1ssrGwQERGJTCZTz5afn4/s7GyVLT8/v9zP3L59O/z8/PDBBx/Azs4Ovr6+WLVqlfJ4amoq0tPT0alTJ2WbXC5HYGAgjh49qtbrZ7JBRESkJSIjI2FpaamyRUZGltv3+vXrWL58Oby8vLBnzx6MGDEC48aNw9q1awEA6enpAAB7e3uV8+zt7ZXH1IXTKERERCKTQT3TKGFhYZg4caJKm1wuL7dvSUkJ/Pz8EBERAQDw9fVFYmIili9fjv79+/8vtn9N8QiCoPZpH1Y2iIiIRKauu1HkcjksLCxUtuclG3Xq1IG3t7dKW6NGjXDr1i0AgIODAwCUqWJkZGSUqXZU+/rVOhoRERFphHbt2iE5OVmlLSUlBS4uLgAANzc3ODg4YO/evcrjBQUFiIuLQ0BAgFpj4TQKERGRyKS4G2XChAkICAhAREQEevXqhZMnT2LlypVYuXKlMqbQ0FBERETAy8sLXl5eiIiIgImJCfr27avWWCqVbCxevLjSA44bN+6lgyEiIqqJpHiAaKtWrbB161aEhYXhiy++gJubGxYtWoR+/fop+0yePBl5eXkYNWoUsrKy0KZNG8TGxsLc3FytscgEQRAq6uTm5la5wWQyXL9+vdpBqUP7xcekDoFI4+wf548fTt2SOgwijTKoVT3RP6P7d6fVMs62IX5qGedVq1RlIzU1Vew4iIiIaiy+Yv4lFRQUIDk5GUVFReqMh4iIqMZR10O9tFWVk43c3FwMHjwYJiYmaNy4sfIWmnHjxmHevHlqD5CIiEjbyWQytWzaqsrJRlhYGM6dO4eDBw/C2NhY2d6xY0ds2LBBrcERERGR9qvyra/btm3Dhg0b0LZtW5Usy9vbG9euXVNrcERERDWBFhcl1KLKyUZmZibs7OzKtOfk5Gh1iYeIiEgsXCBaRa1atcLvv/+u3H+WYKxatQr+/v7qi4yIiIhqhCpXNiIjI9GlSxdcunQJRUVF+Prrr5GYmIhjx44hLi5OjBiJiIi0mm7XNV6ishEQEIAjR44gNzcXHh4eiI2Nhb29PY4dO4aWLVuKESMREZFW0/W7UV7q3Sg+Pj6IiYlRdyxERERUA71UslFcXIytW7ciKSkJMpkMjRo1QkhICAwM+F43IiKif9PT3qKEWlQ5O7h48SJCQkKQnp6OBg0aACh9Za2trS22b98OHx8ftQdJRESkzbR5CkQdqrxmY8iQIWjcuDHu3LmDhIQEJCQk4Pbt22jatCmGDRsmRoxERESkxapc2Th37hxOnz6NWrVqKdtq1aqFuXPnolWrVmoNjoiIqCbQ8cJG1SsbDRo0wN27d8u0Z2RkwNPTUy1BERER1SS8G6USsrOzlX+OiIjAuHHjEB4ejrZt2wIAjh8/ji+++AJRUVHiRElERKTFuEC0EqysrFQyKkEQ0KtXL2WbIAgAgG7duqG4uFiEMImIiEhbVSrZOHDggNhxEBER1VjaPAWiDpVKNgIDA8WOg4iIqMbS7VTjJR/qBQC5ubm4desWCgoKVNqbNm1a7aCIiIio5nipV8wPHDgQu3btKvc412wQERGp4ivmqyg0NBRZWVk4fvw4FAoFdu/ejZiYGHh5eWH79u1ixEhERKTVZDL1bNqqypWN/fv349dff0WrVq2gp6cHFxcXBAcHw8LCApGRkejatasYcRIREZGWqnJlIycnB3Z2dgAAa2trZGZmAih9E2xCQoJ6oyMiIqoBdP2hXi/1BNHk5GQAQPPmzbFixQr89ddf+Pbbb1GnTh21B0hERKTtOI1SRaGhoUhLSwMAzJo1C507d8ZPP/0EIyMjrFmzRt3xERERkZarcrLRr18/5Z99fX1x48YNXL58GfXq1YONjY1agyMiIqoJdP1ulJd+zsYzJiYmaNGihTpiISIiqpF0PNeoXLIxceLESg8YHR390sEQERHVRNq8uFMdKpVsnDlzplKD6foXk4iIiMqSCc9e2UpERESiGLs1SS3jLOnRSC3jvGrVXrOhqTafS5M6BCKN836zOmg7L07qMIg0yvGp4r9sVNcr/1V+zgYRERFRVdTYygYREZGm0NPtwgaTDSIiIrHperLBaRQiIiIS1UslGz/++CPatWsHR0dH3Lx5EwCwaNEi/Prrr2oNjoiIqCbgi9iqaPny5Zg4cSLefvttPHz4EMXFxQAAKysrLFq0SN3xERERaT09mXo2bVXlZGPJkiVYtWoVpk2bBn19fWW7n58fLly4oNbgiIiISPtVeYFoamoqfH19y7TL5XLk5OSoJSgiIqKaRItnQNSiypUNNzc3nD17tkz7rl274O3trY6YiIiIahQ9mUwtm7aqcmVj0qRJGD16NJ4+fQpBEHDy5En8/PPPiIyMxHfffSdGjERERFpN12/9rHKyMXDgQBQVFWHy5MnIzc1F37594eTkhK+//hp9+vQRI0YiIiLSYi/1UK+hQ4di6NChuHfvHkpKSmBnZ6fuuIiIiGoMLZ4BUYtqPUHUxsZGXXEQERHVWNq83kIdqpxsuLm5vfDBItevX69WQERERFSzVDnZCA0NVdkvLCzEmTNnsHv3bkyaNEldcREREdUYOl7YqHqyMX78+HLbv/nmG5w+fbraAREREdU02vz0T3VQ2904b731FrZs2aKu4YiIiKiGUNsr5jdv3gxra2t1DUdERFRjcIFoFfn6+qosEBUEAenp6cjMzMSyZcvUGhwREVFNoOO5RtWTje7du6vs6+npwdbWFkFBQWjYsKG64iIiIqIaokrJRlFREVxdXdG5c2c4ODiIFRMREVGNwgWiVWBgYICRI0ciPz9frHiIiIhqHJma/tNWVb4bpU2bNjhz5owYsRAREdVIejL1bNqqyms2Ro0ahU8//RR37txBy5YtYWpqqnK8adOmaguOiIiItF+lk41BgwZh0aJF6N27NwBg3LhxymMymQyCIEAmk6G4uFj9URIREWkxba5KqEOlk42YmBjMmzcPqampYsZDRERU47zonWK6oNLJhiAIAAAXFxfRgiEiIqKap0prNnQ9MyMiInoZnEapgvr161eYcDx48KBaAREREdU0uv67epWSjc8//xyWlpZixUJEREQ1UJWSjT59+sDOzk6sWIiIiGokXX8RW6Uf6sX1GkRERC9HEx7qFRkZCZlMhtDQUGWbIAgIDw+Ho6MjFAoFgoKCkJiYWL0PKkelk41nd6MQERGRdjl16hRWrlxZ5sGb8+fPR3R0NJYuXYpTp07BwcEBwcHBePz4sVo/v9LJRklJCadQiIiIXoJMpp7tZTx58gT9+vXDqlWrUKtWLWW7IAhYtGgRpk2bhp49e6JJkyaIiYlBbm4u1q1bp6YrL1Xld6MQERFR1ehBppYtPz8f2dnZKltFL0cdPXo0unbtio4dO6q0p6amIj09HZ06dVK2yeVyBAYG4ujRo2q+fiIiIhKVuiobkZGRsLS0VNkiIyOf+7nr169HQkJCuX3S09MBAPb29irt9vb2ymPqUuUXsREREZE0wsLCMHHiRJU2uVxebt/bt29j/PjxiI2NhbGx8XPH/PcNIM/edaZOTDaIiIhEpq4niMrl8ucmF/8WHx+PjIwMtGzZUtlWXFyMQ4cOYenSpUhOTgZQWuGoU6eOsk9GRkaZakd1cRqFiIhIZHoymVq2qujQoQMuXLiAs2fPKjc/Pz/069cPZ8+ehbu7OxwcHLB3717lOQUFBYiLi0NAQIBar5+VDSIiohrI3NwcTZo0UWkzNTVF7dq1le2hoaGIiIiAl5cXvLy8EBERARMTE/Tt21etsTDZICIiEpmmPhdz8uTJyMvLw6hRo5CVlYU2bdogNjYW5ubmav0cJhtEREQi05THlR88eFBlXyaTITw8HOHh4aJ+LtdsEBERkahY2SAiIhKZhhQ2JMNkg4iISGS6Po2g69dPREREImNlg4iISGTqfiKntmGyQUREJDLdTjWYbBAREYlOU259lYpkycbixYsr3XfcuHEiRkJERERikizZWLhwocp+ZmYmcnNzYWVlBQB4+PAhTExMYGdnx2SDiIi0mm7XNSS8GyU1NVW5zZ07F82bN0dSUhIePHiABw8eICkpCS1atMDs2bOlCpGIiEgtZDL1bNpKI259nTFjBpYsWYIGDRoo2xo0aICFCxdi+vTpEkZGRERE1aURC0TT0tJQWFhYpr24uBh3796VICIiIiL10fVbXzWistGhQwcMHToUp0+fhiAIAIDTp09j+PDh6Nixo8TRERERVY+emjZtpRGx//DDD3ByckLr1q1hbGwMuVyONm3aoE6dOvjuu++kDo+IiIiqQSOmUWxtbbFz506kpKTg8uXLEAQBjRo1Qv369aUOjYiIqNp0fRpFI5KNZ1xdXSEIAjw8PGBgoFGhERERvTTdTjU0ZBolNzcXgwcPhomJCRo3boxbt24BKH2Y17x58ySOjoiIiKpDI5KNsLAwnDt3DgcPHoSxsbGyvWPHjtiwYYOEkREREVWfTCZTy6atNGKuYtu2bdiwYQPatm2r8sX09vbGtWvXJIyMiIio+jTiN3sJaUSykZmZCTs7uzLtOTk5Wp3JERERAVwgqhHJVqtWrfD7778r95/9paxatQr+/v5ShUVERERqoBGVjcjISHTp0gWXLl1CUVERvv76ayQmJuLYsWOIi4uTOjwiIqJq0e26hoZUNgICAnDkyBHk5ubCw8MDsbGxsLe3x7Fjx9CyZUupwyMiIqoWXX8Rm0ZUNgDAx8cHMTExUodBREREaqYRlY2EhARcuHBBuf/rr7+ie/fu+M9//oOCggIJIyMiIqo+PcjUsmkrjUg2hg8fjpSUFADA9evX0bt3b5iYmGDTpk2YPHmyxNERERFVj65Po2hEspGSkoLmzZsDADZt2oTAwECsW7cOa9aswZYtW6QNjoiIiKpFI9ZsCIKAkpISAMAff/yBd955BwDg7OyMe/fuSRkaERFRtcm0eApEHTQi2fDz88OcOXPQsWNHxMXFYfny5QCA1NRU2NvbSxwdERFR9WjzFIg6aMQ0yqJFi5CQkIAxY8Zg2rRp8PT0BABs3rwZAQEBEkdHRERE1aERlY2mTZuq3I3yzIIFC6Cvry9BREREROqjzXeSqINGVDZu376NO3fuKPdPnjyJ0NBQrF27FoaGhhJGRkREVH28G0UD9O3bFwcOHAAApKenIzg4GCdPnsR//vMffPHFFxJHR0REVD1MNjTAxYsX0bp1awDAxo0b0aRJExw9elR5+ysRERFpL41Ys1FYWAi5XA6g9NbXd999FwDQsGFDpKWlSRkaERFRten6ra8aUdlo3Lgxvv32W/z555/Yu3cvunTpAgD4+++/Ubt2bYmjIyIiqh49mXo2baURyUZUVBRWrFiBoKAgfPjhh2jWrBkAYPv27crpFSIiItJOGjGNEhQUhHv37iE7Oxu1atVStg8bNgwmJiYSRkZERFR9nEbREIIgID4+HitWrMDjx48BAEZGRkw2iIhI6+n63SgaUdm4efMmunTpglu3biE/Px/BwcEwNzfH/Pnz8fTpU3z77bdSh0hEREQvSSMqG+PHj4efnx+ysrKgUCiU7T169MC+ffskjIyIiKj6ZGr6T1tpRGXj8OHDOHLkCIyMjFTaXVxc8Ndff0kUFRERkXpo850k6qARlY2SkhIUFxeXab9z5w7Mzc0liIiIiIjURSMqG8HBwVi0aBFWrlwJAJDJZHjy5AlmzZqFt99+W+LoCABOxP6KE7G/4mFmOgDArq4r3nz/EzTwbQMA2LdxNc4f3Y9H9zOhb2AAJ/f6CO4zBM5e3s8dM/HEIRzc+l88SP8LxcXFqO3ghNe69YbvG51eyTURqYOtmRFGB7nD38MacgM93HqQh7k7k5F894myj2ttE4wOcoOvsxVkMiD1Xi6m/XoJd7Pzyx1zWd9maFHPqkz7kav38enmi2JdColIm6dA1EEjko3o6Gi0b98e3t7eePr0Kfr27YsrV67AxsYGP//8s9ThEQALa1t07jsMtR2cAAAJcXvw0/xpGD1/Feyd3WDj6Ixug8bD2t4RhQX5OPL7JqyeMwmfLvkJphZW5Y6pMDNHUM+PYetYD/oGBkhOOIZfls2DmYUVvJrz+Sqk+czlBlj5sS/ibz7EhI0XkJVbACcrBZ7kFyn7OFkZY8VHzfHbuXSsOnwTT54WwdXGBAVFJc8dd+oviTDQ/98/TpYKQ/w4yA/7kzNFvR4SjzbfSaIOGpFsODk54ezZs1i/fj3i4+NRUlKCwYMHo1+/fioLRkk6jfwCVPY7fTgEJ2N/xe0rl2Dv7IZmr3VUOf52/9GI378T6TevwcOnZbljujf2VdkPePt9JMTtwY3LF5hskFb4uK0z7mbnY87OZGVb2iPVasWIN9xw9NoDLD14Xdn296OnLxw3+2mRyn5wIzvkFxZj32UmG9pKx3MN6ZONwsJCNGjQADt27MDAgQMxcOBAqUOiCpSUFOPisYMoyH+KevUblzleVFSIU3/8BmMTUzi4eFRqTEEQcP1iAu79fRtd+g1Xd8hEonjdqzaOp2Zhbndv+DpbIvNJPn5J+Bu/niudbpQBCPCwxn9P3MaiXj6ob2+GtEdPEXPsFg5duV/pz+nW1AF7kzLwtPD51RAiTSZ5smFoaIj8/HzIXrLGlJ+fj/x81d8knr3UjdQr/dZ1rJg2CkWFBTAyVqDfZ7NhV9dVefxy/FFsWPQFCgvyYWZVGwOnf/XcKZRnnuY+QdTw91FUVAg9PT10GzwBnk39xL0QIjVxtFKgp68CP5+8g5hjt+BdxxwTOnqioFjArot3UcvUEKZyA/RvWw8r/kzFNwevo627Neb1bIzR687hzO1HFX6Gdx1zeNqZIWJXyiu4IhKLno7Po2jE3Shjx45FVFQUioqKKu78L5GRkbC0tFTZIiMjRYiSbBydMWbBdxg+dxladwrB5m8ikXHnhvK4e2NfjFnwHYbNXor6zVtj/cJwPHmU9cIxjYxNMGbBdxgZ+S2C+wzBrrXf4HriGZGvhEg99GRAcvpjfHsoFSl3n2Db2TRsP5eGnr6O/3+89B+YQ1fuYf2pv3AlIwc/Hr+NI1fvo8f/96lIt6YOuJrxBJfSHot2HSQ+mZo2bSV5ZQMATpw4gX379iE2NhY+Pj4wNTVVOf7LL78899ywsDBMnDhRpU0ul+O3yw9EiVWXGRgYorZDXQBAXY+G+OvaZRzduQXdh30KADAyVqC2Q13UdqiLevUbI3pcP8Tv34nAHv2eO6aenp5yTEdXL2T8dRNx29aVWc9BpInuPSnAjfu5Km037uciqIEtAOBhbiGKikvK7dOsrmWF48sN9BDcyA4rD99QW8xEUtCIZMPKygrvvffeS50rl8s5bSIRQQCKCgtecFx44fHyTwKKq3oOkUTO33mEetaq729ytjZB+v8vAC0qEXAp7XG5fdIqWCQKAB0b2cLQQA+7L95VX9AkDW0uS6iBRiQbq1evljoEqkDsulWo79sGlrVtkf80D+eP7Edq4lkMmDYfBU/zcPCX/6KhXwDMa9VG7uNsnIjdhuwHmWjiH6QcY9PSCFhY26Bz32EAgLitP8HJowGs7R1RXFSI5DMncObQHrw7ZIJEV0lUNetP/YVVHzfHJ/71sC8pA96OFujerA7m7f7f+oqfTt7GnBBvnL39EPE3H6KtuzVe86yN0evOKvvMfKcBMh8XYHlcqsr43ZrWwaGUe2XuTiHtw+dsaID27dvjl19+gZWVlUp7dnY2unfvjv3790sTGCk9eZSFTUvn4nHWg/+/y8QdA6bNh2dTPxQW5CPz71tI+GoPch8/gom5BZw8GmLo50tg7+ymHOPRvbsqC4EL8vOw/buFeHQ/E4ZGctg61cMHY6ehaUB7KS6RqMqS0h9jyi+JGBnohkHtXJD2MA+L9l3FnksZyj5xKfcRtecKPmnrjAkdPXHrQR7Ctibi3J1sZR8HC2MIgurYzrUUaO5siXHrz7+qyyESjUwQ/v0t/urp6ekhPT0ddnZ2Ku0ZGRlwcnJCYWFhlcfcfC5NXeER1RjvN6uDtvPipA6DSKMcnxoo+mecvF7xnUeV0dq94rU+mkjSysb58//L2C9duoT09HTlfnFxMXbv3g0nJycpQiMiIlIb3Z5EkTjZaN68OWQyGWQyGdq3L1s6VygUWLJkiQSRERERkbpImmykpqZCEAS4u7vj5MmTsLW1VR4zMjKCnZ0d9PX1JYyQiIhIDXS8tCFpsuHi4gKg9BXzRERENZWu342iEU8QjYmJwe+//67cnzx5MqysrBAQEICbN29KGBkREVH1yWTq2bSVRiQbERERyre7Hjt2DEuXLsX8+fNhY2ODCRP4zAUiIiJtphHP2bh9+zY8PT0BANu2bcP777+PYcOGoV27dggKCpI2OCIiomrS4qKEWmhEZcPMzAz375e+bjk2NhYdO3YEABgbGyMvL0/K0IiIiKpPx9/EphGVjeDgYAwZMgS+vr5ISUlB165dAQCJiYlwdXWVNjgiIiKqFo2obHzzzTfw9/dHZmYmtmzZgtq1awMA4uPj8eGHH0ocHRERUfXI1PRfVURGRqJVq1YwNzeHnZ0dunfvjuTkZJU+giAgPDwcjo6OUCgUCAoKQmJiojovHYCGVDasrKywdOnSMu2ff/65BNEQERGplxR3ksTFxWH06NFo1aoVioqKMG3aNHTq1AmXLl2CqakpAGD+/PmIjo7GmjVrUL9+fcyZMwfBwcFITk6Gubm52mLRiMrGP/n4+OD27dtSh0FERKTVdu/ejQEDBqBx48Zo1qwZVq9ejVu3biE+Ph5AaVVj0aJFmDZtGnr27IkmTZogJiYGubm5WLdunVpj0bhk48aNGy/14jUiIiJNpa71ofn5+cjOzlbZ8vPzKxXDo0elL4OztrYGUPoU7/T0dHTq1EnZRy6XIzAwEEePHq3uJavQuGSDiIioxlFTthEZGQlLS0uVLTIyssKPFwQBEydOxGuvvYYmTZoAgPLlp/b29ip97e3tVV6Mqg4asWbjn15//XXlA76IiIjof8LCwjBx4kSVNrlcXuF5Y8aMwfnz53H48OEyx2T/WlAiCEKZturSuGRj586dUodARESkVup6N4pcLq9UcvFPY8eOxfbt23Ho0CHUrVtX2e7g4ACgtMJRp04dZXtGRkaZakd1aUyykZKSgoMHDyIjI6PMi9lmzpwpUVRERETVJ8XdKIIgYOzYsdi6dSsOHjwINzc3leNubm5wcHDA3r174evrCwAoKChAXFwcoqKi1BqLRiQbq1atwsiRI2FjYwMHBweV8o1MJmOyQUREWk2Kh3+OHj0a69atw6+//gpzc3PlOgxLS0soFArIZDKEhoYiIiICXl5e8PLyQkREBExMTNC3b1+1xqIRycacOXMwd+5cTJkyRepQiIiIaoTly5cDQJl3jK1evRoDBgwAUPqW9by8PIwaNQpZWVlo06YNYmNj1fqMDUBDko2srCx88MEHUodBREQkDommUSoik8kQHh6O8PBwUWPRiFtfP/jgA8TGxkodBhERkSikeFy5JtGIyoanpydmzJiB48ePw8fHB4aGhirHx40bJ1FkREREVF0akWysXLkSZmZmiIuLQ1xcnMoxmUzGZIOIiLSaFHejaBKNSDZSU1OlDoGIiEg0Op5raMaajX8SBKFSi1qIiIhIO2hMsrF27Vr4+PhAoVBAoVCgadOm+PHHH6UOi4iIqPrU9SY2LaUR0yjR0dGYMWMGxowZg3bt2kEQBBw5cgQjRozAvXv3MGHCBKlDJCIiemnafCeJOmhEsrFkyRIsX74c/fv3V7aFhISgcePGCA8PZ7JBRESkxTQi2UhLS0NAQECZ9oCAAKSlpUkQERERkfro+t0oGrFmw9PTExs3bizTvmHDBnh5eUkQERERkfro+JINzahsfP755+jduzcOHTqEdu3aQSaT4fDhw9i3b1+5SQgREZFW0eZMQQ00orLx3nvv4cSJE6hduza2bduGX375BTY2Njh58iR69OghdXhERERUDRpR2QCAli1b4qeffpI6DCIiIrXj3SgS0tPTg6yCVTMymQxFRUWvKCIiIiL10/UFopImG1u3bn3usaNHj2LJkiV8migREZGWkzTZCAkJKdN2+fJlhIWF4bfffkO/fv0we/ZsCSIjIiJSHx0vbGjGAlEA+PvvvzF06FA0bdoURUVFOHv2LGJiYlCvXj2pQyMiIqoeHb/3VfJk49GjR5gyZQo8PT2RmJiIffv24bfffkOTJk2kDo2IiIjUQNJplPnz5yMqKgoODg74+eefy51WISIi0na8G0VCU6dOhUKhgKenJ2JiYhATE1Nuv19++eUVR0ZERKQ+vBtFQv3796/w1lciIiLSbpImG2vWrJHy44mIiF4JXf+1WmOeIEpERFRj6Xi2wWSDiIhIZLq+QFTyW1+JiIioZmNlg4iISGS6fi8Ekw0iIiKR6XiuwWkUIiIiEhcrG0RERCLjNAoRERGJTLezDU6jEBERkahY2SAiIhIZp1GIiIhIVDqea3AahYiIiMTFygYREZHIOI1CREREotL1d6Mw2SAiIhKbbucaXLNBRERE4mJlg4iISGQ6XthgskFERCQ2XV8gymkUIiIiEhUrG0RERCLj3ShEREQkLt3ONTiNQkREROJiZYOIiEhkOl7YYLJBREQkNt6NQkRERCQiVjaIiIhExrtRiIiISFScRiEiIiISEZMNIiIiEhWnUYiIiESm69MoTDaIiIhEpusLRDmNQkRERKJiZYOIiEhknEYhIiIiUel4rsFpFCIiIhIXKxtERERi0/HSBpMNIiIikfFuFCIiIiIRsbJBREQkMt6NQkRERKLS8VyD0yhERESik6lpewnLli2Dm5sbjI2N0bJlS/z555/VupSXwWSDiIiohtqwYQNCQ0Mxbdo0nDlzBq+//jreeust3Lp165XGwWSDiIhIZDI1/VdV0dHRGDx4MIYMGYJGjRph0aJFcHZ2xvLly0W4yudjskFERCQymUw9W1UUFBQgPj4enTp1Umnv1KkTjh49qsarqxgXiBIREWmJ/Px85Ofnq7TJ5XLI5fIyfe/du4fi4mLY29urtNvb2yM9PV3UOP+txiYb7zerI3UIOi8/Px+RkZEICwsr9weBpHF8aqDUIeg8/mzoHmM1/WsbPicSn3/+uUrbrFmzEB4e/txzZP8qiQiCUKZNbDJBEIRX+omkM7Kzs2FpaYlHjx7BwsJC6nCINAZ/NuhlVaWyUVBQABMTE2zatAk9evRQto8fPx5nz55FXFyc6PE+wzUbREREWkIul8PCwkJle151zMjICC1btsTevXtV2vfu3YuAgIBXEa5SjZ1GISIi0nUTJ07Exx9/DD8/P/j7+2PlypW4desWRowY8UrjYLJBRERUQ/Xu3Rv379/HF198gbS0NDRp0gQ7d+6Ei4vLK42DyQaJRi6XY9asWVwAR/Qv/NmgV2nUqFEYNWqUpDFwgSgRERGJigtEiYiISFRMNoiIiEhUTDaIiIhIVEw2qMYICgpCaGio1GEQ1Siurq5YtGiR1GGQlmOyoWMyMjIwfPhw1KtXD3K5HA4ODujcuTOOHTsGoPSxttu2bZM2SKKXMGDAAMhkMsybN0+lfdu2ba/80cz/dOPGDchkMpw9e1ayGIikxmRDx7z33ns4d+4cYmJikJKSgu3btyMoKAgPHjyo9BiFhYUiRkj08oyNjREVFYWsrCypQ6mygoICqUMgEg2TDR3y8OFDHD58GFFRUXjzzTfh4uKC1q1bIywsDF27doWrqysAoEePHpDJZMr98PBwNG/eHD/88APc3d0hl8shCAIePXqEYcOGwc7ODhYWFmjfvj3OnTun/Lxz587hzTffhLm5OSwsLNCyZUucPn0aAHDz5k1069YNtWrVgqmpKRo3boydO3cqz7106RLefvttmJmZwd7eHh9//DHu3bunPJ6Tk4P+/fvDzMwMderUwVdffSX+F5A0XseOHeHg4IDIyMjn9tmyZQsaN24MuVwOV1fXMt87rq6uiIiIwKBBg2Bubo569eph5cqVL/zcrKws9OvXD7a2tlAoFPDy8sLq1asBAG5ubgAAX19fyGQyBAUFASitxHTv3h2RkZFwdHRE/fr1AQB//fUXevfujVq1aqF27doICQnBjRs3lJ918OBBtG7dGqamprCyskK7du1w8+ZNAC/+mQOAo0eP4o033oBCoYCzszPGjRuHnJwc5fGMjAx069YNCoUCbm5u+Omnnyr4ihNVDpMNHWJmZgYzMzNs27atzIt8AODUqVMAgNWrVyMtLU25DwBXr17Fxo0bsWXLFmU5uGvXrkhPT8fOnTsRHx+PFi1aoEOHDsoqSb9+/VC3bl2cOnUK8fHxmDp1KgwNDQEAo0ePRn5+Pg4dOoQLFy4gKioKZmZmAIC0tDQEBgaiefPmOH36NHbv3o27d++iV69eyngmTZqEAwcOYOvWrYiNjcXBgwcRHx8vyteNtIe+vj4iIiKwZMkS3Llzp8zx+Ph49OrVC3369MGFCxcQHh6OGTNmYM2aNSr9vvrqK/j5+eHMmTMYNWoURo4cicuXLz/3c2fMmIFLly5h165dSEpKwvLly2FjYwMAOHnyJADgjz/+QFpaGn755Rflefv27UNSUhL27t2LHTt2IDc3F2+++SbMzMxw6NAhHD58GGZmZujSpQsKCgpQVFSE7t27IzAwEOfPn8exY8cwbNgw5TTRi37mLly4gM6dO6Nnz544f/48NmzYgMOHD2PMmDHKeAYMGIAbN25g//792Lx5M5YtW4aMjIyX+8sg+ieBdMrmzZuFWrVqCcbGxkJAQIAQFhYmnDt3TnkcgLB161aVc2bNmiUYGhoKGRkZyrZ9+/YJFhYWwtOnT1X6enh4CCtWrBAEQRDMzc2FNWvWlBuHj4+PEB4eXu6xGTNmCJ06dVJpu337tgBASE5OFh4/fiwYGRkJ69evVx6/f/++oFAohPHjx1f4NaCa6ZNPPhFCQkIEQRCEtm3bCoMGDRIEQRC2bt0qPPtfXd++fYXg4GCV8yZNmiR4e3sr911cXISPPvpIuV9SUiLY2dkJy5cvf+5nd+vWTRg4cGC5x1JTUwUAwpkzZ8rEa29vL+Tn5yvbvv/+e6FBgwZCSUmJsi0/P19QKBTCnj17hPv37wsAhIMHD5b7WS/6mfv444+FYcOGqbT9+eefgp6enpCXlyckJycLAITjx48rjyclJQkAhIULFz732okqg5UNHfPee+/h77//xvbt29G5c2ccPHgQLVq0KPOb3b+5uLjA1tZWuR8fH48nT56gdu3ayoqJmZkZUlNTce3aNQClLwAaMmQIOnbsiHnz5inbAWDcuHGYM2cO2rVrh1mzZuH8+fMqYx84cEBl3IYNGwIArl27hmvXrqGgoAD+/v7Kc6ytrdGgQQN1fImoBoiKikJMTAwuXbqk0p6UlIR27dqptLVr1w5XrlxBcXGxsq1p06bKP8tkMjg4OCh/w3/rrbeU35eNGzcGAIwcORLr169H8+bNMXnyZBw9erRScfr4+MDIyEi5Hx8fj6tXr8Lc3Fz5GdbW1nj69CmuXbsGa2trDBgwAJ07d0a3bt3w9ddfIy0tTXn+i37m4uPjsWbNGpWfq86dO6OkpASpqalISkqCgYEB/Pz8lOc0bNgQVlZWlboWohdhsqGDjI2NERwcjJkzZ+Lo0aMYMGAAZs2a9cJzTE1NVfZLSkpQp04dnD17VmVLTk7GpEmTAJSu9UhMTETXrl2xf/9+eHt7Y+vWrQCAIUOG4Pr16/j4449x4cIF+Pn5YcmSJcqxu3XrVmbsK1eu4I033oDAJ+xTBd544w107twZ//nPf1TaBUEoc2dKed9Pz6YenpHJZCgpKQEAfPfdd8rvyWfrjN566y3cvHkToaGh+Pvvv9GhQwd89tlnFcZZ3s9Vy5Yty3zvp6SkoG/fvgBKpzmPHTuGgIAAbNiwAfXr18fx48cBvPhnrqSkBMOHD1cZ99y5c7hy5Qo8PDyUXwcp79yhmosvYiN4e3srb3c1NDRU+Q3veVq0aIH09HQYGBgoF5KWp379+qhfvz4mTJiADz/8EKtXr0aPHj0AAM7OzhgxYgRGjBiBsLAwrFq1CmPHjkWLFi2wZcsWuLq6wsCg7Leop6cnDA0Ncfz4cdSrVw9A6QK9lJQUBAYGVv0LQDXSvHnz0Lx5c+XCS6D0e/3w4cMq/Y4ePYr69etDX1+/UuM6OTmV225ra4sBAwZgwIABeP311zFp0iR8+eWXyspFZX+uNmzYoFx0/Ty+vr7w9fVFWFgY/P39sW7dOrRt2xbA83/mWrRogcTERHh6epY7ZqNGjVBUVITTp0+jdevWAIDk5GQ8fPiwwriJKsLKhg65f/8+2rdvj//+9784f/48UlNTsWnTJsyfPx8hISEASlfi79u3D+np6S+8fbBjx47w9/dH9+7dsWfPHty4cQNHjx7F9OnTcfr0aeTl5WHMmDE4ePAgbt68iSNHjuDUqVNo1KgRACA0NBR79uxBamoqEhISsH//fuWx0aNH48GDB/jwww9x8uRJXL9+HbGxsRg0aBCKi4thZmaGwYMHY9KkSdi3bx8uXryIAQMGQE+P3870Pz4+PujXr5+yYgYAn376Kfbt24fZs2cjJSUFMTExWLp0aaWqEC8yc+ZM/Prrr7h69SoSExOxY8cO5feznZ0dFAqFcqHzo0ePnjtOv379YGNjg5CQEPz5559ITU1FXFwcxo8fjzt37iA1NRVhYWE4duwYbt68idjYWKSkpKBRo0YV/sxNmTIFx44dw+jRo5WVwu3bt2Ps2LEAgAYNGqBLly4YOnQoTpw4gfj4eAwZMgQKhaJaXxsiAFwgqkuePn0qTJ06VWjRooVgaWkpmJiYCA0aNBCmT58u5ObmCoIgCNu3bxc8PT0FAwMDwcXFRRCE0gWizZo1KzNedna2MHbsWMHR0VEwNDQUnJ2dhX79+gm3bt0S8vPzhT59+gjOzs6CkZGR4OjoKIwZM0bIy8sTBEEQxowZI3h4eAhyuVywtbUVPv74Y+HevXvKsVNSUoQePXoIVlZWgkKhEBo2bCiEhoYqF849fvxY+OijjwQTExPB3t5emD9/vhAYGMgFojrsnwtEn7lx44Ygl8uFf/6vbvPmzYK3t7dgaGgo1KtXT1iwYIHKOS4uLmUWRDZr1kyYNWvWcz979uzZQqNGjQSFQiFYW1sLISEhwvXr15XHV61aJTg7Owt6enpCYGDgc+MVBEFIS0sT+vfvL9jY2AhyuVxwd3cXhg4dKjx69EhIT08XunfvLtSpU0cwMjISXFxchJkzZwrFxcUV/swJgiCcPHlSCA4OFszMzARTU1OhadOmwty5c1U+u2vXroJcLhfq1asnrF27ttyvB1FV8RXzREREJCrWnYmIiEhUTDaIiIhIVEw2iIiISFRMNoiIiEhUTDaIiIhIVEw2iIiISFRMNoiIiEhUTDaINEh4eDiaN2+u3B8wYAC6d+/+yuO4ceMGZDIZzp49+9w+rq6uWLRoUaXHXLNmjVpe6iWTyZSP1yci7cBkg6gCAwYMgEwmg0wmg6GhIdzd3fHZZ58hJydH9M/++uuvK3wj7zOVSRCIiKTAF7ERVUKXLl2wevVqFBYW4s8//8SQIUOQk5OD5cuXl+lbWFhY5q2hL8vS0lIt4xARSYmVDaJKkMvlcHBwgLOzM/r27Yt+/fopS/nPpj5++OEHuLu7Qy6XQxAEPHr0CMOGDVO+wbN9+/Y4d+6cyrjz5s2Dvb09zM3NMXjwYDx9+lTl+L+nUUpKShAVFQVPT0/I5XLUq1cPc+fOBQC4ubkBKH0jqEwmQ1BQkPK81atXo1GjRjA2NkbDhg2xbNkylc85efIkfH19YWxsDD8/P5w5c6bKX6Po6Gj4+PjA1NQUzs7OGDVqFJ48eVKm37Zt21C/fn0YGxsjODgYt2/fVjn+22+/oWXLljA2Noa7uzs+//xzFBUVVTkeItIcTDaIXoJCoUBhYaFy/+rVq9i4cSO2bNminMbo2rUr0tPTsXPnTsTHx6NFixbo0KEDHjx4AADYuHEjZs2ahblz5+L06dOoU6dOmSTg38LCwhAVFYUZM2bg0qVLWLduHezt7QGUJgwA8McffyAtLQ2//PILAGDVqlWYNm0a5s6di6SkJERERGDGjBmIiYkBAOTk5OCdd95BgwYNEB8fj/Dw8Jd6C6qenh4WL16MixcvIiYmBvv378fkyZNV+uTm5mLu3LmIiYnBkSNHkJ2djT59+iiP79mzBx999BHGjRuHS5cuYcWKFVizZo0yoSIiLSXxi+CINN6/38554sQJoXbt2kKvXr0EQSh9K66hoaGQkZGh7LNv3z7BwsJCePr0qcpYHh4ewooVKwRBEAR/f39hxIgRKsfbtGmj8obdf352dna2IJfLhVWrVpUbZ2pqqgBAOHPmjEq7s7OzsG7dOpW22bNnC/7+/oIgCMKKFSsEa2trIScnR3l8+fLl5Y71TxW9DXTjxo1C7dq1lfurV68WAAjHjx9XtiUlJQkAhBMnTgiCIAivv/66EBERoTLOjz/+KNSpU0e5D0DYunXrcz+XiDQP12wQVcKOHTtgZmaGoqIiFBYWIiQkBEuWLFEed3Fxga2trXI/Pj4eT548Qe3atVXGycvLw7Vr1wAASUlJGDFihMpxf39/HDhwoNwYkpKSkJ+fjw4dOlQ67szMTNy+fRuDBw/G0KFDle1FRUXK9SBJSUlo1qwZTExMVOKoqgMHDiAiIgKXLl1CdnY2ioqK8PTpU+Tk5MDU1BQAYGBgAD8/P+U5DRs2hJWVFZKSktC6dWvEx8fj1KlTKpWM4uJiPH36FLm5uSoxEpH2YLJBVAlvvvkmli9fDkNDQzg6OpZZAPrsH9NnSkpKUKdOHRw8eLDMWC97+6dCoajyOSUlJQBKp1LatGmjckxfXx8AIAjCS8XzTzdv3sTbb7+NESNGYPbs2bC2tsbhw4cxePBglekmoPTW1X971lZSUoLPP/8cPXv2LNPH2Ni42nESkTSYbBBVgqmpKTw9PSvdv0WLFkhPT4eBgQFcXV3L7dOoUSMcP34c/fv3V7YdP378uWN6eXlBoVBg3759GDJkSJnjRkZGAEorAc/Y29vDyckJ169fR79+/cod19vbGz/++CPy8vKUCc2L4ijP6dOnUVRUhK+++gp6eqVLwTZu3FimX1FREU6fPo3WrVsDAJKTk/Hw4UM0bNgQQOnXLTk5uUpfayLSfEw2iETQsWNH+Pv7o3v37oiKikKDBg3w999/Y+fOnejevTv8/Pwwfvx4fPLJJ/Dz88Nrr72Gn376CYmJiXB3dy93TGNjY0yZMgWTJ0+GkZER2rVrh8zMTCQmJmLw4MGws7ODQqHA7t27UbduXRgbG8PS0hLh4eEYN24cLCws8NZbbyE/Px+nT59GVlYWJk6ciL59+2LatGkYPHgwpk+fjhs3buDLL7+s0vV6eHigqKgIS5YsQbdu3XDkyBF8++23ZfoZGhpi7NixWLx4MQwNDTFmzBi0bdtWmXzMnDkT77zzDpydnfHBBx9AT08P58+fx4ULFzBnzpyq/0UQkUbg3ShEIpDJZNi5cyfeeOMNDBo0CPXr10efPn1w48YN5d0jvXv3xsyZMzFlyhS0bNkSN2/exMiRI1847owZM/Dpp59i5syZaNSoEXr37o2MjAwApeshFi9ejBUrVsDR0REhISEAgCFDhuC7777DmjVr4OPjg8DAQKxZs0Z5q6yZmRl+++03XLp0Cb6+vpg2bRqioqKqdL3NmzdHdHQ0oqKi0KRJE/z000+IjIws08/ExARTpkxB37594e/vD4VCgfXr1yuPd+7cGTt27MDevXvRqlUrtG3bFtHR0XBxcalSPESkWWSCOiZsiYiIiJ6DlQ0iIiISFZMNIiIiEhWTDSIiIhIVkw0iIiISFZMNIiIiEhWTDSIiIhIVkw0iIiISFZMNIiIiEhWTDSIiIhIVkw0iIiISFZMNIiIiEhWTDSIiIhLV/wFwoHZZnLpxTwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.95184946e-01 3.04815054e-01]\n",
      " [4.21892613e-01 5.78107417e-01]\n",
      " [7.89708734e-01 2.10291266e-01]\n",
      " [9.99535143e-01 4.64851793e-04]\n",
      " [1.59405246e-01 8.40594828e-01]\n",
      " [9.02964830e-01 9.70351920e-02]\n",
      " [1.18944913e-01 8.81055117e-01]\n",
      " [1.71721101e-01 8.28278899e-01]\n",
      " [9.67139781e-01 3.28601785e-02]\n",
      " [7.46518612e-01 2.53481418e-01]\n",
      " [2.98798531e-01 7.01201379e-01]\n",
      " [1.78000614e-10 9.99999940e-01]\n",
      " [8.16536665e-01 1.83463335e-01]\n",
      " [2.43162528e-01 7.56837428e-01]\n",
      " [1.04919985e-01 8.95080090e-01]\n",
      " [9.93469775e-01 6.53020665e-03]\n",
      " [9.06261265e-01 9.37386826e-02]\n",
      " [6.60291553e-01 3.39708477e-01]\n",
      " [7.47849047e-01 2.52150923e-01]]\n",
      "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[7 4]\n",
      " [4 4]]\n",
      "[57.89 63.64 50.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "C:\\Users\\annej\\AppData\\Local\\Temp\\ipykernel_14032\\2687471855.py:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  [2.9218609e-02, 9.7078133e-01]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14032\\2687471855.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m7.1874863e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.8125137e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m9.9979991e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0014797e-04\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                             \u001b[1;33m[\u001b[0m\u001b[1;36m2.9218609e-02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9.7078133e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m9.9454159e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.4584546e-03\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m5.5008806e-02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9.4499117e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# init data, 300 epochs, softmax activation\n",
    "probs_Deep_init_soft = np.array([[7.0045400e-01, 2.9954603e-01],\n",
    "                            [7.8413427e-01, 2.1586572e-01],\n",
    "                            [7.1874863e-01, 2.8125137e-01],\n",
    "                            [9.9979991e-01, 2.0014797e-04],\n",
    "                            [2.9218609e-02, 9.7078133e-01]\n",
    "                            [9.9454159e-01, 5.4584546e-03],\n",
    "                            [5.5008806e-02, 9.4499117e-01],\n",
    "                            [9.6587259e-01, 3.4127403e-02],\n",
    "                            [6.3271725e-01, 3.6728275e-01],\n",
    "                            [9.8455411e-01, 1.5445878e-02],\n",
    "                            [8.1000119e-01, 1.8999882e-01],\n",
    "                            [9.9752015e-01, 2.4798173e-03],\n",
    "                            [9.8586375e-01, 1.4136297e-02],\n",
    "                            [4.3618846e-01, 5.6381154e-01],\n",
    "                            [1.4959927e-01, 8.5040075e-01],\n",
    "                            [9.5763546e-01, 4.2364582e-02],\n",
    "                            [9.9997401e-01, 2.5972882e-05],\n",
    "                            [9.9117404e-01, 8.8259308e-03],\n",
    "                            [9.9719328e-01, 2.8067816e-03]])\n",
    "\n",
    "\n",
    "probs_Deep_init_sigm = np.array([[6.95184946e-01 3.04815054e-01]\n",
    " [4.21892613e-01 5.78107417e-01]\n",
    " [7.89708734e-01 2.10291266e-01]\n",
    " [9.99535143e-01 4.64851793e-04]\n",
    " [1.59405246e-01 8.40594828e-01]\n",
    " [9.02964830e-01 9.70351920e-02]\n",
    " [1.18944913e-01 8.81055117e-01]\n",
    " [1.71721101e-01 8.28278899e-01]\n",
    " [9.67139781e-01 3.28601785e-02]\n",
    " [7.46518612e-01 2.53481418e-01]\n",
    " [2.98798531e-01 7.01201379e-01]\n",
    " [1.78000614e-10 9.99999940e-01]\n",
    " [8.16536665e-01 1.83463335e-01]\n",
    " [2.43162528e-01 7.56837428e-01]\n",
    " [1.04919985e-01 8.95080090e-01]\n",
    " [9.93469775e-01 6.53020665e-03]\n",
    " [9.06261265e-01 9.37386826e-02]\n",
    " [6.60291553e-01 3.39708477e-01]\n",
    " [7.47849047e-01 2.52150923e-01]]\n",
    "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
    "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 87.15217, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 23.3107 - accuracy: 0.4773 - val_loss: 87.1522 - val_accuracy: 0.6250 - 11s/epoch - 6s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 87.15217 to 41.84463, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 170.6238 - accuracy: 0.6136 - val_loss: 41.8446 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 41.84463\n",
      "2/2 - 11s - loss: 58.7394 - accuracy: 0.5682 - val_loss: 65.2082 - val_accuracy: 0.4375 - 11s/epoch - 6s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 41.84463 to 36.31944, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 37.4983 - accuracy: 0.6364 - val_loss: 36.3194 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 36.31944 to 16.31143, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 23.7577 - accuracy: 0.6591 - val_loss: 16.3114 - val_accuracy: 0.4375 - 12s/epoch - 6s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 16.31143 to 8.91383, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 8.3401 - accuracy: 0.7500 - val_loss: 8.9138 - val_accuracy: 0.4375 - 12s/epoch - 6s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 8.91383\n",
      "2/2 - 11s - loss: 1.0100 - accuracy: 0.8636 - val_loss: 32.5006 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 8.91383 to 5.88339, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 12.9091 - accuracy: 0.6818 - val_loss: 5.8834 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.3545 - accuracy: 0.9545 - val_loss: 7.2523 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 5.88339\n",
      "2/2 - 11s - loss: 0.0928 - accuracy: 0.9773 - val_loss: 8.6581 - val_accuracy: 0.7500 - 11s/epoch - 5s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 4.2381e-04 - accuracy: 1.0000 - val_loss: 14.7726 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 1.5372 - accuracy: 0.8864 - val_loss: 27.2083 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 2.6863 - accuracy: 0.8182 - val_loss: 10.9076 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 5.88339\n",
      "2/2 - 12s - loss: 1.0654 - accuracy: 0.9545 - val_loss: 43.6488 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 14.0460 - accuracy: 0.6818 - val_loss: 18.8799 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 4.8166 - accuracy: 0.7727 - val_loss: 12.6407 - val_accuracy: 0.3125 - 10s/epoch - 5s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.1874 - accuracy: 0.9545 - val_loss: 23.6714 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 5.88339\n",
      "2/2 - 11s - loss: 1.5760 - accuracy: 0.8864 - val_loss: 16.9942 - val_accuracy: 0.7500 - 11s/epoch - 6s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 8.2615 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 6.4989e-06 - accuracy: 1.0000 - val_loss: 11.0372 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.0772 - accuracy: 0.9773 - val_loss: 13.3603 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.0337 - accuracy: 0.9773 - val_loss: 12.4977 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 10.0788 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.2746 - accuracy: 0.9773 - val_loss: 9.3666 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 2.7281e-06 - accuracy: 1.0000 - val_loss: 10.3604 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.0428 - accuracy: 0.9773 - val_loss: 10.1358 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.2656 - accuracy: 0.9773 - val_loss: 7.4948 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 1.5733e-05 - accuracy: 1.0000 - val_loss: 6.6773 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 0.1023 - accuracy: 0.9773 - val_loss: 7.0539 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 5.88339\n",
      "2/2 - 10s - loss: 2.1784e-05 - accuracy: 1.0000 - val_loss: 6.0790 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss improved from 5.88339 to 5.22845, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 5.1477e-08 - accuracy: 1.0000 - val_loss: 5.2284 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss improved from 5.22845 to 5.07236, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 6.6106e-07 - accuracy: 1.0000 - val_loss: 5.0724 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 5.07236\n",
      "2/2 - 10s - loss: 1.6169e-05 - accuracy: 1.0000 - val_loss: 5.1973 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 5.07236\n",
      "2/2 - 10s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.4038 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 5.07236\n",
      "2/2 - 10s - loss: 7.0216e-04 - accuracy: 1.0000 - val_loss: 5.6655 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 5.07236\n",
      "2/2 - 10s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 6.0004 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 5.07236\n",
      "2/2 - 10s - loss: 2.2743e-04 - accuracy: 1.0000 - val_loss: 6.4239 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 5.07236\n",
      "2/2 - 10s - loss: 0.0898 - accuracy: 0.9773 - val_loss: 8.0873 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss improved from 5.07236 to 4.92922, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.3557 - accuracy: 0.9318 - val_loss: 4.9292 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 0.0765 - accuracy: 0.9773 - val_loss: 14.7878 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 0.7922 - accuracy: 0.9318 - val_loss: 15.2469 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 2.8878 - accuracy: 0.8409 - val_loss: 36.5571 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 8.5037 - accuracy: 0.7045 - val_loss: 20.7118 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 5.7172 - accuracy: 0.8409 - val_loss: 54.3008 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 14.4151 - accuracy: 0.7500 - val_loss: 11.9437 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 2.0067 - accuracy: 0.8864 - val_loss: 23.3174 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 0.6257 - accuracy: 0.9318 - val_loss: 12.0140 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 0.7376 - accuracy: 0.9545 - val_loss: 10.2763 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 6.8894e-05 - accuracy: 1.0000 - val_loss: 7.3321 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 4.92922\n",
      "2/2 - 10s - loss: 0.1300 - accuracy: 0.9773 - val_loss: 22.0592 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 4.92922 to 4.77818, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.8346 - accuracy: 0.9318 - val_loss: 4.7782 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 4.77818\n",
      "2/2 - 10s - loss: 0.4669 - accuracy: 0.9318 - val_loss: 19.2712 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 4.77818\n",
      "2/2 - 10s - loss: 0.4378 - accuracy: 0.9545 - val_loss: 11.5978 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 4.77818\n",
      "2/2 - 10s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 7.0567 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss improved from 4.77818 to 4.63390, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 3.1858e-05 - accuracy: 1.0000 - val_loss: 4.6339 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 4.63390\n",
      "2/2 - 10s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 4.7577 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 4.63390\n",
      "2/2 - 10s - loss: 2.2398e-05 - accuracy: 1.0000 - val_loss: 5.3134 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 4.63390\n",
      "2/2 - 10s - loss: 1.5524e-06 - accuracy: 1.0000 - val_loss: 6.1494 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 4.63390\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 7.0547 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 4.63390\n",
      "2/2 - 10s - loss: 5.4998e-07 - accuracy: 1.0000 - val_loss: 7.8118 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 4.63390\n",
      "2/2 - 10s - loss: 3.4291e-05 - accuracy: 1.0000 - val_loss: 8.8196 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 4.63390\n",
      "2/2 - 10s - loss: 2.5467e-07 - accuracy: 1.0000 - val_loss: 9.4585 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 4.63390\n",
      "2/2 - 9s - loss: 1.8468e-05 - accuracy: 1.0000 - val_loss: 9.8074 - val_accuracy: 0.6250 - 9s/epoch - 5s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 4.63390\n",
      "2/2 - 9s - loss: 9.7141e-06 - accuracy: 1.0000 - val_loss: 10.1700 - val_accuracy: 0.6250 - 9s/epoch - 5s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 4.63390\n",
      "2/2 - 9s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 5.7822 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 4.63390\n",
      "2/2 - 9s - loss: 2.2811e-06 - accuracy: 1.0000 - val_loss: 4.7873 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss improved from 4.63390 to 4.13325, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 1.3031e-06 - accuracy: 1.0000 - val_loss: 4.1332 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss improved from 4.13325 to 3.75004, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 2.5872e-06 - accuracy: 1.0000 - val_loss: 3.7500 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss improved from 3.75004 to 3.72155, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 0.0304 - accuracy: 0.9773 - val_loss: 3.7215 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 3.7781 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.6878e-06 - accuracy: 1.0000 - val_loss: 3.8313 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.9940e-06 - accuracy: 1.0000 - val_loss: 3.8765 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 3.9117 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.0261e-05 - accuracy: 1.0000 - val_loss: 3.9403 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.1650e-07 - accuracy: 1.0000 - val_loss: 3.9641 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 2.1137e-05 - accuracy: 1.0000 - val_loss: 3.9838 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 3.9975 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.0081 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.0178 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.0256 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.3547e-08 - accuracy: 1.0000 - val_loss: 4.0328 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 3.6846e-07 - accuracy: 1.0000 - val_loss: 4.0413 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 9.6449e-07 - accuracy: 1.0000 - val_loss: 4.0409 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 4.0455 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 4.0485 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 5.5878e-05 - accuracy: 1.0000 - val_loss: 4.0485 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.6256e-08 - accuracy: 1.0000 - val_loss: 4.0506 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 3.72155\n",
      "2/2 - 10s - loss: 1.2463e-07 - accuracy: 1.0000 - val_loss: 4.0531 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 2.4384e-08 - accuracy: 1.0000 - val_loss: 4.0559 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 4.0558 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 5.1477e-08 - accuracy: 1.0000 - val_loss: 4.0551 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 2.9802e-08 - accuracy: 1.0000 - val_loss: 4.0547 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.9951 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.5985e-07 - accuracy: 1.0000 - val_loss: 3.8994 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 1.6445e-06 - accuracy: 1.0000 - val_loss: 3.8219 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 3.72155\n",
      "2/2 - 9s - loss: 3.4507e-05 - accuracy: 1.0000 - val_loss: 3.7594 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss improved from 3.72155 to 3.70752, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 3.2512e-08 - accuracy: 1.0000 - val_loss: 3.7075 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss improved from 3.70752 to 3.66650, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 9s - loss: 2.4384e-08 - accuracy: 1.0000 - val_loss: 3.6665 - val_accuracy: 0.7500 - 9s/epoch - 5s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss improved from 3.66650 to 3.64335, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 3.6433 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss improved from 3.64335 to 3.61866, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 1.0295e-07 - accuracy: 1.0000 - val_loss: 3.6187 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss improved from 3.61866 to 3.59198, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 9.1302e-07 - accuracy: 1.0000 - val_loss: 3.5920 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss improved from 3.59198 to 3.57715, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.5772 - val_accuracy: 0.7500 - 11s/epoch - 6s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 3.57715 to 3.56663, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 11s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.5666 - val_accuracy: 0.7500 - 11s/epoch - 5s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 3.56663 to 3.56518, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.5652 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.8294 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3440 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 3.56518\n",
      "2/2 - 12s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.8281 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.3035 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 5.7208 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 3.4243e-06 - accuracy: 1.0000 - val_loss: 5.9409 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 6.0341 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.6280e-07 - accuracy: 1.0000 - val_loss: 6.1224 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 4.0910e-07 - accuracy: 1.0000 - val_loss: 6.1266 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 4.7464e-06 - accuracy: 1.0000 - val_loss: 6.2027 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.0160e-06 - accuracy: 1.0000 - val_loss: 6.2758 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0252 - accuracy: 1.0000 - val_loss: 4.7059 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.8703 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 9.5099 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 12.9734 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0364 - accuracy: 0.9773 - val_loss: 5.9567 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.5234 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.8491 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 9.1844e-07 - accuracy: 1.0000 - val_loss: 15.4642 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0784 - accuracy: 0.9545 - val_loss: 6.6062 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 9.2116e-08 - accuracy: 1.0000 - val_loss: 5.7667 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 6.7732e-08 - accuracy: 1.0000 - val_loss: 10.1027 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 11.0032 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.1547e-05 - accuracy: 1.0000 - val_loss: 9.2008 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 8.0769e-05 - accuracy: 1.0000 - val_loss: 7.6981 - val_accuracy: 0.7500 - 10s/epoch - 5s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 7.5423e-04 - accuracy: 1.0000 - val_loss: 6.5403 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 6.0401 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 8.3052e-06 - accuracy: 1.0000 - val_loss: 6.6786 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 7.6787 - val_accuracy: 0.6250 - 9s/epoch - 5s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 9.2399e-06 - accuracy: 1.0000 - val_loss: 8.9926 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 2.8233e-05 - accuracy: 1.0000 - val_loss: 10.8656 - val_accuracy: 0.5000 - 9s/epoch - 5s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 10.2873 - val_accuracy: 0.5000 - 9s/epoch - 5s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 3.3485e-06 - accuracy: 1.0000 - val_loss: 9.8344 - val_accuracy: 0.5000 - 9s/epoch - 4s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 9.5702 - val_accuracy: 0.5000 - 9s/epoch - 4s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 7.3151e-08 - accuracy: 1.0000 - val_loss: 9.2853 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 2.7093e-08 - accuracy: 1.0000 - val_loss: 9.0740 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 7.9382e-07 - accuracy: 1.0000 - val_loss: 8.8093 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.8352 - val_accuracy: 0.5625 - 9s/epoch - 5s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7739 - val_accuracy: 0.5625 - 9s/epoch - 5s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 1.1715e-05 - accuracy: 1.0000 - val_loss: 8.7938 - val_accuracy: 0.5625 - 9s/epoch - 5s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.6256e-08 - accuracy: 1.0000 - val_loss: 8.8902 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.8164 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8730 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.9631 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.5947e-05 - accuracy: 1.0000 - val_loss: 8.9679 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.8138 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 8.5259 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.8784 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.4121 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.3492e-06 - accuracy: 1.0000 - val_loss: 7.1987 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.8627 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.6293 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.4312 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.3010 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 3.56518\n",
      "2/2 - 11s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.1799 - val_accuracy: 0.6875 - 11s/epoch - 6s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 3.56518\n",
      "2/2 - 14s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.0903 - val_accuracy: 0.6875 - 14s/epoch - 7s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 3.56518\n",
      "2/2 - 12s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.0280 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 3.56518\n",
      "2/2 - 12s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 5.9374 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 3.56518\n",
      "2/2 - 12s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.9000 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8909 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8616 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.8965e-08 - accuracy: 1.0000 - val_loss: 5.8009 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7889 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7944 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7793 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8201 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7986 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7876 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7664 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7686 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 4.8767e-08 - accuracy: 1.0000 - val_loss: 5.8305 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8117 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7821 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 5.8112 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 3.4408e-07 - accuracy: 1.0000 - val_loss: 5.8600 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8204 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 5.8577 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8399 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 3.56518\n",
      "2/2 - 11s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8281 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 5.8681 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8290 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8328 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8344 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8099 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 3.56518\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8200 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8066 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 5.8467 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 5.8681 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8566 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 3.56518\n",
      "2/2 - 11s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8586 - val_accuracy: 0.6875 - 11s/epoch - 5s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7962 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 5.7693 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 3.56518\n",
      "2/2 - 13s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7428 - val_accuracy: 0.6875 - 13s/epoch - 6s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.7902e-04 - accuracy: 1.0000 - val_loss: 5.8227 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8594 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.5985e-07 - accuracy: 1.0000 - val_loss: 5.9088 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 5.9096 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 5.8379 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 5.7976 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7765 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 6.9899e-07 - accuracy: 1.0000 - val_loss: 5.7269 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 3.6846e-07 - accuracy: 1.0000 - val_loss: 5.6762 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6646 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.1295e-05 - accuracy: 1.0000 - val_loss: 5.6343 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 3.5763e-07 - accuracy: 1.0000 - val_loss: 5.6982 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6904 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 8.6697e-08 - accuracy: 1.0000 - val_loss: 5.6752 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6695 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6581 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.6256e-08 - accuracy: 1.0000 - val_loss: 5.6976 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7236 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7250 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7388 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7369 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7000 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6762 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6649 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6470 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.3547e-08 - accuracy: 1.0000 - val_loss: 5.6981 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7195 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 3.7930e-08 - accuracy: 1.0000 - val_loss: 5.7748 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7513 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7746 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7653 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7709 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 6.4480e-07 - accuracy: 1.0000 - val_loss: 5.7287 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7136 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7600 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7364 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7405 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7346 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7045 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6975 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7361 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6806 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6756 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6804 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7147 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7191 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 4.0640e-08 - accuracy: 1.0000 - val_loss: 5.7714 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-08 - accuracy: 1.0000 - val_loss: 5.8141 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7937 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7862 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7803 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7784 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-07 - accuracy: 1.0000 - val_loss: 5.7362 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 5.7195 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6541 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6739 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6974 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 1.0566e-07 - accuracy: 1.0000 - val_loss: 5.6702 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6491 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.6149 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.6866 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.8906 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.0760 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.2335 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.2376e-05 - accuracy: 1.0000 - val_loss: 6.4595 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.5958 - val_accuracy: 0.6875 - 10s/epoch - 5s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 6.6427 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.7401 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 6.8991 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.0026 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.0452 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 3.56518\n",
      "2/2 - 11s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 6.9557 - val_accuracy: 0.6250 - 11s/epoch - 5s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.9448 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.9809 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.0618 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.0778 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1587 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1299 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1294 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1026 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1709 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1312 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.2387 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.2575 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 7.3709 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.2507 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1861 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 7.2875 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 7.3373 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 3.56518\n",
      "2/2 - 12s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.2919 - val_accuracy: 0.5625 - 12s/epoch - 6s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1640 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1615 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.0943 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.1667 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 8.2091e-07 - accuracy: 1.0000 - val_loss: 7.2957 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.2333 - val_accuracy: 0.6250 - 10s/epoch - 5s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.3030 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.2954 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.3846 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 7.5686 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.5604 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.4449 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 3.56518\n",
      "2/2 - 10s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 7.4144 - val_accuracy: 0.5625 - 10s/epoch - 5s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002781D42D5A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002781D42D5A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Classification accuracy: 0.529086 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9999946e-01 5.2360480e-07]\n",
      " [9.9920905e-01 7.9095934e-04]\n",
      " [3.1668501e-04 9.9968332e-01]\n",
      " [9.9958211e-01 4.1789180e-04]\n",
      " [1.0000000e+00 2.0106063e-12]\n",
      " [2.5430129e-15 1.0000000e+00]\n",
      " [4.5979065e-10 1.0000000e+00]\n",
      " [4.0611334e-04 9.9959385e-01]\n",
      " [2.0120059e-09 1.0000000e+00]\n",
      " [1.0000000e+00 1.1936473e-11]\n",
      " [9.9999958e-01 3.9360066e-07]\n",
      " [1.0000000e+00 4.5507523e-16]\n",
      " [1.0000000e+00 1.6772502e-13]\n",
      " [9.9999577e-01 4.2513248e-06]\n",
      " [9.9251729e-01 7.4824370e-03]\n",
      " [1.0000000e+00 1.4604005e-13]\n",
      " [2.5460558e-04 9.9974537e-01]\n",
      " [1.0000000e+00 2.3725112e-11]\n",
      " [9.6442640e-01 3.5573784e-02]]\n",
      "[0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[8 3]\n",
      " [5 3]]\n",
      "[57.89 61.54 50.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_Shallow_init_sigmoid = np.array([[6.95184946e-01 3.04815054e-01]\n",
    " [4.21892613e-01 5.78107417e-01]\n",
    " [7.89708734e-01 2.10291266e-01]\n",
    " [9.99535143e-01 4.64851793e-04]\n",
    " [1.59405246e-01 8.40594828e-01]\n",
    " [9.02964830e-01 9.70351920e-02]\n",
    " [1.18944913e-01 8.81055117e-01]\n",
    " [1.71721101e-01 8.28278899e-01]\n",
    " [9.67139781e-01 3.28601785e-02]\n",
    " [7.46518612e-01 2.53481418e-01]\n",
    " [2.98798531e-01 7.01201379e-01]\n",
    " [1.78000614e-10 9.99999940e-01]\n",
    " [8.16536665e-01 1.83463335e-01]\n",
    " [2.43162528e-01 7.56837428e-01]\n",
    " [1.04919985e-01 8.95080090e-01]\n",
    " [9.93469775e-01 6.53020665e-03]\n",
    " [9.06261265e-01 9.37386826e-02]\n",
    " [6.60291553e-01 3.39708477e-01]\n",
    " [7.47849047e-01 2.52150923e-01]]\n",
    "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
    "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
