{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.data import extract_eeg_data, multi_to_binary_classification, split_dataset, dict_to_arr\n",
    "from utils.labels import get_stai_labels\n",
    "from utils.valid_recs import get_valid_recs\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
<<<<<<< Updated upstream
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification\n",
    "import utils.variables as v\n"
=======
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification, EEGNet_TSGLEEGNet_classification\n",
    "import utils.variables as v\n",
    "\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n",
      "Data/ICA_data/sub-P010_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data/sub-P013_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data/sub-P013_ses-S001_run-002.mat not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/ICA_data/sub-P020_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data/sub-P023_ses-S002_run-002.mat not valid\n",
      "Returning valid recordings\n",
      "\n",
      "Valid recs ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n"
     ]
    }
   ],
   "source": [
    "valid_recs = get_valid_recs(data_type='ica', output_type = 'np')\n",
    "print(f'Valid recs {valid_recs}')\n",
    "\n",
    "x_dict_ = extract_eeg_data(valid_recs, data_type='ica', output_type='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n"
     ]
    }
   ],
   "source": [
    "y_dict_ = get_stai_labels(valid_recs) \n",
    "#y_dict = get_pss_labels(valid_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing invalid labels: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing invalid labels: {len(y_dict_)}\")\n",
    "\n",
    "print(y_dict_['P007_S001_002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n"
     ]
    }
   ],
   "source": [
    "x_dict, y_dict = multi_to_binary_classification(x_dict_, y_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing mildly stressed subjects: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing  mildly stressed subjects: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict, test_data_dict, val_data_dict, train_labels_dict, test_labels_dict, val_labels_dict = split_dataset(x_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train data set: {len(train_data_dict)}\")\n",
    "print(f\"Length of validation data set: {len(val_data_dict)}\")\n",
    "print(f\"Length of test data set: {len(test_data_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (44, 8, 75000)\n",
      "Shape of validation data set: (16, 8, 75000)\n",
      "Shape of test data set: (19, 8, 75000)\n",
      "Shape of train labels set: (44, 1)\n",
      "Shape of validation labels set: (16, 1)\n",
      "Shape of test labels set: (19, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = dict_to_arr(train_data_dict)\n",
    "test_data = dict_to_arr(test_data_dict)\n",
    "val_data = dict_to_arr(val_data_dict)\n",
    "\n",
    "train_labels = np.reshape(np.array(list(train_labels_dict.values())), (len(train_data),1))\n",
    "test_labels = np.reshape(np.array(list(test_labels_dict.values())), (len(test_data),1))\n",
    "val_labels = np.reshape(np.array(list(val_labels_dict.values())), (len(val_data),1))\n",
    "\n",
    "print(f\"Shape of train data set: {train_data.shape}\")\n",
    "print(f\"Shape of validation data set: {val_data.shape}\")\n",
    "print(f\"Shape of test data set: {test_data.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Shape of train labels set: {train_labels.shape}\")\n",
    "print(f\"Shape of validation labels set: {val_labels.shape}\")\n",
    "print(f\"Shape of test labels set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.71064, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 11s - loss: 0.9489 - accuracy: 0.4545 - val_loss: 0.7106 - val_accuracy: 0.3125 - 11s/epoch - 11s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.5629 - accuracy: 0.6818 - val_loss: 0.7657 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.3071 - accuracy: 0.8409 - val_loss: 0.7491 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1756 - accuracy: 0.9773 - val_loss: 0.7269 - val_accuracy: 0.3125 - 9s/epoch - 9s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1320 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.3750 - 9s/epoch - 9s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.7117 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.71064\n",
      "1/1 - 9s - loss: 0.0876 - accuracy: 1.0000 - val_loss: 0.7109 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 0.71064 to 0.70986, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.7099 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 0.70986 to 0.70700, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.7070 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 0.70700 to 0.70200, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.7020 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 0.70200 to 0.69611, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.6961 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 0.69611 to 0.68972, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 0.68972 to 0.68410, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.68410 to 0.67905, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0343 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.67905 to 0.67509, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.67509 to 0.67224, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.67224 to 0.67001, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.6700 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.67001 to 0.66768, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.6677 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 0.66768 to 0.66546, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.6655 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.66546 to 0.66335, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.66335 to 0.66125, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.66125 to 0.65953, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.6595 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.65953 to 0.65809, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss improved from 0.65809 to 0.65643, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.5000 - 9s/epoch - 9s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss improved from 0.65643 to 0.65470, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.6547 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss improved from 0.65470 to 0.65297, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.6530 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss improved from 0.65297 to 0.65119, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.6512 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 0.65119 to 0.64929, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.5625 - 9s/epoch - 9s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.64929 to 0.64748, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 10s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 0.6250 - 10s/epoch - 10s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss improved from 0.64748 to 0.64566, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 11s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.6457 - val_accuracy: 0.6250 - 11s/epoch - 11s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss improved from 0.64566 to 0.64393, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 12s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.6439 - val_accuracy: 0.6875 - 12s/epoch - 12s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss improved from 0.64393 to 0.64230, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.6423 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss improved from 0.64230 to 0.64093, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss improved from 0.64093 to 0.63976, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.6398 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss improved from 0.63976 to 0.63852, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.6385 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss improved from 0.63852 to 0.63742, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss improved from 0.63742 to 0.63661, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 8s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.6875 - 8s/epoch - 8s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss improved from 0.63661 to 0.63567, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6357 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss improved from 0.63567 to 0.63472, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss improved from 0.63472 to 0.63351, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.6335 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss improved from 0.63351 to 0.63230, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss improved from 0.63230 to 0.63113, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.6311 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss improved from 0.63113 to 0.63003, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6300 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss improved from 0.63003 to 0.62889, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss improved from 0.62889 to 0.62770, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.6277 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss improved from 0.62770 to 0.62656, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss improved from 0.62656 to 0.62544, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.6254 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss improved from 0.62544 to 0.62431, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.62431 to 0.62322, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss improved from 0.62322 to 0.62220, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.62220 to 0.62119, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.6212 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss improved from 0.62119 to 0.62010, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss improved from 0.62010 to 0.61895, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6189 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss improved from 0.61895 to 0.61802, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.6180 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss improved from 0.61802 to 0.61704, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss improved from 0.61704 to 0.61608, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss improved from 0.61608 to 0.61516, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss improved from 0.61516 to 0.61417, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss improved from 0.61417 to 0.61316, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss improved from 0.61316 to 0.61204, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6120 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss improved from 0.61204 to 0.61090, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6109 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss improved from 0.61090 to 0.60971, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss improved from 0.60971 to 0.60867, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6087 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss improved from 0.60867 to 0.60787, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.6079 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss improved from 0.60787 to 0.60701, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss improved from 0.60701 to 0.60634, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss improved from 0.60634 to 0.60557, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss improved from 0.60557 to 0.60476, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6048 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss improved from 0.60476 to 0.60401, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6040 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss improved from 0.60401 to 0.60309, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6031 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss improved from 0.60309 to 0.60220, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss improved from 0.60220 to 0.60128, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss improved from 0.60128 to 0.60033, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss improved from 0.60033 to 0.59933, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5993 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss improved from 0.59933 to 0.59831, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss improved from 0.59831 to 0.59742, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5974 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss improved from 0.59742 to 0.59663, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss improved from 0.59663 to 0.59588, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.5959 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss improved from 0.59588 to 0.59515, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5951 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss improved from 0.59515 to 0.59454, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5945 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss improved from 0.59454 to 0.59406, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss improved from 0.59406 to 0.59354, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5935 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss improved from 0.59354 to 0.59277, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5928 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss improved from 0.59277 to 0.59194, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss improved from 0.59194 to 0.59118, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.5912 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss improved from 0.59118 to 0.59035, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5903 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss improved from 0.59035 to 0.58935, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5893 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss improved from 0.58935 to 0.58838, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.5884 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss improved from 0.58838 to 0.58752, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss improved from 0.58752 to 0.58672, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5867 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss improved from 0.58672 to 0.58592, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss improved from 0.58592 to 0.58500, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss improved from 0.58500 to 0.58396, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.5840 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss improved from 0.58396 to 0.58298, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5830 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss improved from 0.58298 to 0.58201, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.5820 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss improved from 0.58201 to 0.58121, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.5812 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss improved from 0.58121 to 0.58055, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5805 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss improved from 0.58055 to 0.57995, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5799 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss improved from 0.57995 to 0.57959, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5796 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss improved from 0.57959 to 0.57911, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.5791 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss improved from 0.57911 to 0.57875, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.5787 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss improved from 0.57875 to 0.57842, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5784 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 0.57842 to 0.57811, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5781 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss improved from 0.57811 to 0.57753, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss improved from 0.57753 to 0.57697, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5770 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss improved from 0.57697 to 0.57632, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5763 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss improved from 0.57632 to 0.57562, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5756 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss improved from 0.57562 to 0.57481, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5748 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss improved from 0.57481 to 0.57412, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss improved from 0.57412 to 0.57358, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss improved from 0.57358 to 0.57302, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss improved from 0.57302 to 0.57257, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss improved from 0.57257 to 0.57224, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5722 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss improved from 0.57224 to 0.57193, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5719 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss improved from 0.57193 to 0.57155, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5715 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss improved from 0.57155 to 0.57117, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss improved from 0.57117 to 0.57093, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 545s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.5709 - val_accuracy: 0.7500 - 545s/epoch - 545s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss improved from 0.57093 to 0.57081, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss improved from 0.57081 to 0.57056, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 10s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.5706 - val_accuracy: 0.7500 - 10s/epoch - 10s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss improved from 0.57056 to 0.57030, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5703 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss improved from 0.57030 to 0.57006, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.5701 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss improved from 0.57006 to 0.56977, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5698 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss improved from 0.56977 to 0.56953, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.56953\n",
      "1/1 - 9s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss improved from 0.56953 to 0.56948, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.5697 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5699 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5698 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.56948\n",
      "1/1 - 9s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss improved from 0.56948 to 0.56925, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.5693 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss improved from 0.56925 to 0.56909, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss improved from 0.56909 to 0.56870, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss improved from 0.56870 to 0.56844, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss improved from 0.56844 to 0.56825, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5683 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss improved from 0.56825 to 0.56816, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.5682 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss improved from 0.56816 to 0.56805, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss improved from 0.56805 to 0.56801, saving model to /tmp/checkpoint.h5\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5680 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5681 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5687 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5695 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5702 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5709 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5727 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5735 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5749 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.56801\n",
      "1/1 - 10s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5756 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.56801\n",
      "1/1 - 10s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5761 - val_accuracy: 0.6875 - 10s/epoch - 10s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5769 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5781 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5788 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5792 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5797 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5804 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5813 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5825 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5834 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5842 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5846 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5851 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5856 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5863 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5891 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5902 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5911 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5921 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5934 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5947 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5957 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5980 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6000 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6006 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6047 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6111 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6126 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6143 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6208 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6244 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6261 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6273 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6259 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6256 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6246 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6250 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6260 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6279 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6307 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6332 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6374 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6405 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6402 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6401 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6417 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6448 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6455 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6491 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6505 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6602 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6670 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6693 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6757 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6795 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6805 - val_accuracy: 0.6875 - 9s/epoch - 9s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6798 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6790 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6738 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6730 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6736 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6756 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6776 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6865 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6903 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6960 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6997 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7001 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6994 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6984 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6970 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6962 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6965 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6983 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7019 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7041 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7071 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7096 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7130 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7161 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7184 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7212 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7259 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7260 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7267 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7272 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7280 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7291 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7308 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7326 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.56801\n",
      "1/1 - 9s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7345 - val_accuracy: 0.7500 - 9s/epoch - 9s/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x137038a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x137038a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 414ms/step\n",
      "Classification accuracy: 0.562327 \n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[9 2]\n",
      " [8 0]]\n",
      "[47.37 52.94  0.  ]\n"
     ]
    }
   ],
   "source": [
    "# with init_filter data, 300 epochs, sigmoid\n",
    "probs_EEGNet_init_sigmoid = np.array([\n",
    "                  [0.7138277, 0.28617287],\n",
    "                  [0.5637057, 0.43629473],\n",
    "                  [0.6218500, 0.37815124],\n",
    "                  [0.7166857, 0.28331438],\n",
    "                  [0.6754987, 0.32450426],\n",
    "                  [0.8090515, 0.19095036],\n",
    "                  [0.5339635, 0.46603800],\n",
    "                  [0.2460488, 0.75395140],\n",
    "                  [0.4467932, 0.55320940],\n",
    "                  [0.6290466, 0.37095330],\n",
    "                  [0.5762418, 0.42375806],\n",
    "                  [0.1861840, 0.81381420],\n",
    "                  [0.6687245, 0.33127743],\n",
    "                  [0.7082854, 0.29171503],\n",
    "                  [0.5183024, 0.48169836],\n",
    "                  [0.7217397, 0.27826140],\n",
    "                  [0.6201925, 0.37980822],\n",
    "                  [0.6769989, 0.32300153],\n",
    "                  [0.4880893, 0.51191190]])\n",
    "\n",
    "\n",
    "preds_EEGNet = probs_EEGNet.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6622658  0.33773378]\n",
      " [0.6305761  0.3694238 ]\n",
      " [0.59458554 0.40541294]\n",
      " [0.58825123 0.41174808]\n",
      " [0.61485636 0.38514262]\n",
      " [0.6281727  0.37182656]\n",
      " [0.6707296  0.32926738]\n",
      " [0.7152163  0.28478536]\n",
      " [0.3621873  0.63781214]\n",
      " [0.56972075 0.43027884]\n",
      " [0.629088   0.37091017]\n",
      " [0.4353447  0.5646552 ]\n",
      " [0.56697416 0.43302345]\n",
      " [0.6457108  0.35428876]\n",
      " [0.6471834  0.35281634]\n",
      " [0.5027475  0.497253  ]\n",
      " [0.62844676 0.37155262]\n",
      " [0.57276446 0.42723358]\n",
      " [0.5813717  0.41862816]]\n"
     ]
    }
   ],
   "source": [
    "print(probs_EEGNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
=======
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_SSVEP = EEGNet_SSVEP_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''print(probs_SSVEP)\n",
    "preds_SSVEP = probs_SSVEP.argmax(axis = -1)  \n",
    "print(preds_SSVEP)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_SSVEP = compute_metrics(test_labels, preds_SSVEP)\n",
    "print(performance_SSVEP)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.21357, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 4.7710 - accuracy: 0.4318 - val_loss: 2.2136 - val_accuracy: 0.7500 - 17s/epoch - 9s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 2.21357 to 1.97821, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 3.3222 - accuracy: 0.9318 - val_loss: 1.9782 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 1.97821 to 1.87410, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 3.1586 - accuracy: 1.0000 - val_loss: 1.8741 - val_accuracy: 0.3750 - 16s/epoch - 8s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 1.87410 to 1.78921, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 3.0149 - accuracy: 1.0000 - val_loss: 1.7892 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.78921\n",
      "2/2 - 21s - loss: 2.9520 - accuracy: 1.0000 - val_loss: 1.8271 - val_accuracy: 0.3125 - 21s/epoch - 11s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.78921\n",
      "2/2 - 25s - loss: 2.7504 - accuracy: 1.0000 - val_loss: 1.8053 - val_accuracy: 0.2500 - 25s/epoch - 12s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 1.78921 to 1.76140, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 2.6675 - accuracy: 1.0000 - val_loss: 1.7614 - val_accuracy: 0.2500 - 17s/epoch - 8s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 1.76140 to 1.63825, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 2.5956 - accuracy: 1.0000 - val_loss: 1.6382 - val_accuracy: 0.3125 - 15s/epoch - 8s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 1.63825 to 1.55995, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 2.4107 - accuracy: 1.0000 - val_loss: 1.5600 - val_accuracy: 0.3125 - 23s/epoch - 11s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 1.55995 to 1.48849, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 24s - loss: 2.3109 - accuracy: 1.0000 - val_loss: 1.4885 - val_accuracy: 0.3750 - 24s/epoch - 12s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 1.48849 to 1.41044, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 2.1621 - accuracy: 1.0000 - val_loss: 1.4104 - val_accuracy: 0.3125 - 23s/epoch - 11s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 1.41044 to 1.32575, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 25s - loss: 2.1183 - accuracy: 1.0000 - val_loss: 1.3258 - val_accuracy: 0.3750 - 25s/epoch - 12s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 1.32575 to 1.25974, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 2.0282 - accuracy: 1.0000 - val_loss: 1.2597 - val_accuracy: 0.4375 - 23s/epoch - 12s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 1.25974 to 1.20691, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 25s - loss: 1.9105 - accuracy: 1.0000 - val_loss: 1.2069 - val_accuracy: 0.3750 - 25s/epoch - 12s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 1.20691 to 1.15915, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 1.8160 - accuracy: 1.0000 - val_loss: 1.1592 - val_accuracy: 0.3750 - 21s/epoch - 11s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 1.15915 to 1.11778, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 24s - loss: 1.7316 - accuracy: 1.0000 - val_loss: 1.1178 - val_accuracy: 0.4375 - 24s/epoch - 12s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 1.11778 to 1.06750, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 1.6263 - accuracy: 1.0000 - val_loss: 1.0675 - val_accuracy: 0.3750 - 21s/epoch - 11s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 1.06750 to 1.01803, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 22s - loss: 1.5360 - accuracy: 1.0000 - val_loss: 1.0180 - val_accuracy: 0.4375 - 22s/epoch - 11s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 1.01803 to 0.98154, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 22s - loss: 1.4684 - accuracy: 1.0000 - val_loss: 0.9815 - val_accuracy: 0.5625 - 22s/epoch - 11s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss improved from 0.98154 to 0.94450, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 1.3773 - accuracy: 1.0000 - val_loss: 0.9445 - val_accuracy: 0.6250 - 21s/epoch - 11s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 0.94450 to 0.91465, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 22s - loss: 1.2831 - accuracy: 1.0000 - val_loss: 0.9147 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 0.91465 to 0.89414, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 1.2758 - accuracy: 1.0000 - val_loss: 0.8941 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 0.89414 to 0.88101, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 1.1214 - accuracy: 1.0000 - val_loss: 0.8810 - val_accuracy: 0.7500 - 21s/epoch - 11s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss improved from 0.88101 to 0.87372, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 22s - loss: 1.0637 - accuracy: 1.0000 - val_loss: 0.8737 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss improved from 0.87372 to 0.86029, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 1.0017 - accuracy: 1.0000 - val_loss: 0.8603 - val_accuracy: 0.7500 - 23s/epoch - 12s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss improved from 0.86029 to 0.85356, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.9414 - accuracy: 1.0000 - val_loss: 0.8536 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.85356\n",
      "2/2 - 21s - loss: 0.9275 - accuracy: 1.0000 - val_loss: 0.8541 - val_accuracy: 0.7500 - 21s/epoch - 11s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 0.85356 to 0.85059, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 0.8440 - accuracy: 1.0000 - val_loss: 0.8506 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss improved from 0.85059 to 0.84240, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 0.7970 - accuracy: 1.0000 - val_loss: 0.8424 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.84240\n",
      "2/2 - 22s - loss: 0.8304 - accuracy: 1.0000 - val_loss: 0.8492 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.84240\n",
      "2/2 - 22s - loss: 0.7270 - accuracy: 1.0000 - val_loss: 0.8465 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.84240\n",
      "2/2 - 21s - loss: 0.7361 - accuracy: 1.0000 - val_loss: 0.8464 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.84240\n",
      "2/2 - 20s - loss: 0.7531 - accuracy: 1.0000 - val_loss: 2.3376 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss improved from 0.84240 to 0.78793, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 3.1789 - accuracy: 0.5909 - val_loss: 0.7879 - val_accuracy: 0.7500 - 21s/epoch - 10s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 1.0914 - accuracy: 0.8636 - val_loss: 0.8610 - val_accuracy: 0.5000 - 21s/epoch - 10s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.9264 - accuracy: 0.9091 - val_loss: 0.9456 - val_accuracy: 0.5625 - 21s/epoch - 11s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.8773 - accuracy: 0.9318 - val_loss: 0.9316 - val_accuracy: 0.6250 - 22s/epoch - 11s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 1.0174 - accuracy: 0.8636 - val_loss: 1.6390 - val_accuracy: 0.3125 - 22s/epoch - 11s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 2.6499 - accuracy: 0.7500 - val_loss: 2.1133 - val_accuracy: 0.5625 - 22s/epoch - 11s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.78793\n",
      "2/2 - 26s - loss: 3.7951 - accuracy: 0.7045 - val_loss: 1.7158 - val_accuracy: 0.3750 - 26s/epoch - 13s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.78793\n",
      "2/2 - 24s - loss: 0.8483 - accuracy: 0.9773 - val_loss: 1.7557 - val_accuracy: 0.3750 - 24s/epoch - 12s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.78793\n",
      "2/2 - 24s - loss: 0.8298 - accuracy: 1.0000 - val_loss: 1.8127 - val_accuracy: 0.3750 - 24s/epoch - 12s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.78793\n",
      "2/2 - 24s - loss: 0.7939 - accuracy: 1.0000 - val_loss: 1.8946 - val_accuracy: 0.3750 - 24s/epoch - 12s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.8348 - accuracy: 1.0000 - val_loss: 1.9588 - val_accuracy: 0.3750 - 22s/epoch - 11s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.78793\n",
      "2/2 - 24s - loss: 0.8552 - accuracy: 1.0000 - val_loss: 2.0228 - val_accuracy: 0.3750 - 24s/epoch - 12s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.78793\n",
      "2/2 - 31s - loss: 0.8480 - accuracy: 1.0000 - val_loss: 2.0680 - val_accuracy: 0.3750 - 31s/epoch - 16s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.78793\n",
      "2/2 - 26s - loss: 0.7922 - accuracy: 1.0000 - val_loss: 2.0940 - val_accuracy: 0.3750 - 26s/epoch - 13s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.78793\n",
      "2/2 - 23s - loss: 0.7948 - accuracy: 1.0000 - val_loss: 2.1058 - val_accuracy: 0.3750 - 23s/epoch - 12s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.7783 - accuracy: 1.0000 - val_loss: 2.1052 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.8396 - accuracy: 1.0000 - val_loss: 2.1012 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.7729 - accuracy: 1.0000 - val_loss: 2.0791 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.7701 - accuracy: 1.0000 - val_loss: 2.0554 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.7737 - accuracy: 1.0000 - val_loss: 2.0316 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.7502 - accuracy: 1.0000 - val_loss: 2.0006 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.78793\n",
      "2/2 - 24s - loss: 0.7853 - accuracy: 1.0000 - val_loss: 1.9819 - val_accuracy: 0.3125 - 24s/epoch - 12s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.78793\n",
      "2/2 - 23s - loss: 0.7460 - accuracy: 1.0000 - val_loss: 1.9488 - val_accuracy: 0.3125 - 23s/epoch - 12s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.7328 - accuracy: 1.0000 - val_loss: 1.9285 - val_accuracy: 0.3125 - 21s/epoch - 11s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.7052 - accuracy: 1.0000 - val_loss: 1.8989 - val_accuracy: 0.3125 - 22s/epoch - 11s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.78793\n",
      "2/2 - 23s - loss: 0.6915 - accuracy: 1.0000 - val_loss: 1.8693 - val_accuracy: 0.3125 - 23s/epoch - 12s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.7140 - accuracy: 1.0000 - val_loss: 1.8345 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6654 - accuracy: 1.0000 - val_loss: 1.8107 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6552 - accuracy: 1.0000 - val_loss: 1.7856 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6973 - accuracy: 1.0000 - val_loss: 1.7651 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6328 - accuracy: 1.0000 - val_loss: 1.7389 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6372 - accuracy: 1.0000 - val_loss: 1.7175 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6296 - accuracy: 1.0000 - val_loss: 1.6939 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5991 - accuracy: 1.0000 - val_loss: 1.6707 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6223 - accuracy: 1.0000 - val_loss: 1.6499 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5780 - accuracy: 1.0000 - val_loss: 1.6260 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5770 - accuracy: 1.0000 - val_loss: 1.6063 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5557 - accuracy: 1.0000 - val_loss: 1.5875 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.6156 - accuracy: 1.0000 - val_loss: 1.5707 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5675 - accuracy: 1.0000 - val_loss: 1.5516 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5947 - accuracy: 1.0000 - val_loss: 1.5348 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5511 - accuracy: 1.0000 - val_loss: 1.5164 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.5397 - accuracy: 1.0000 - val_loss: 1.5019 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5147 - accuracy: 1.0000 - val_loss: 1.4870 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4890 - accuracy: 1.0000 - val_loss: 1.4693 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5404 - accuracy: 1.0000 - val_loss: 1.4528 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4930 - accuracy: 1.0000 - val_loss: 1.4375 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.4702 - accuracy: 1.0000 - val_loss: 1.4217 - val_accuracy: 0.3750 - 21s/epoch - 11s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4824 - accuracy: 1.0000 - val_loss: 1.4076 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4535 - accuracy: 1.0000 - val_loss: 1.3960 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4992 - accuracy: 1.0000 - val_loss: 1.3849 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4385 - accuracy: 1.0000 - val_loss: 1.3730 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4497 - accuracy: 1.0000 - val_loss: 1.3373 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4456 - accuracy: 1.0000 - val_loss: 1.3087 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.4352 - accuracy: 1.0000 - val_loss: 1.3015 - val_accuracy: 0.5000 - 21s/epoch - 10s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4139 - accuracy: 1.0000 - val_loss: 1.2922 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.4253 - accuracy: 1.0000 - val_loss: 1.2690 - val_accuracy: 0.5000 - 21s/epoch - 11s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4082 - accuracy: 1.0000 - val_loss: 1.2474 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4043 - accuracy: 1.0000 - val_loss: 1.2470 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4049 - accuracy: 1.0000 - val_loss: 1.2625 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4328 - accuracy: 1.0000 - val_loss: 1.2830 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.3874 - accuracy: 1.0000 - val_loss: 1.2890 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4121 - accuracy: 1.0000 - val_loss: 1.2697 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.3904 - accuracy: 1.0000 - val_loss: 1.1976 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4032 - accuracy: 1.0000 - val_loss: 1.1946 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4144 - accuracy: 1.0000 - val_loss: 1.2889 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4408 - accuracy: 1.0000 - val_loss: 1.1725 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4026 - accuracy: 1.0000 - val_loss: 1.5770 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.3635 - accuracy: 1.0000 - val_loss: 1.7949 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.3607 - accuracy: 1.0000 - val_loss: 1.4057 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.3565 - accuracy: 1.0000 - val_loss: 1.1859 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.3709 - accuracy: 1.0000 - val_loss: 1.2680 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.3453 - accuracy: 1.0000 - val_loss: 1.4061 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.3435 - accuracy: 1.0000 - val_loss: 1.3116 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.3393 - accuracy: 1.0000 - val_loss: 1.1487 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.3425 - accuracy: 1.0000 - val_loss: 1.1552 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3627 - accuracy: 1.0000 - val_loss: 1.2400 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.3402 - accuracy: 1.0000 - val_loss: 1.2936 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.3720 - accuracy: 1.0000 - val_loss: 1.2004 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3231 - accuracy: 1.0000 - val_loss: 1.0800 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3502 - accuracy: 1.0000 - val_loss: 1.7798 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3793 - accuracy: 1.0000 - val_loss: 3.4734 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 6.1342 - accuracy: 0.7273 - val_loss: 8.1800 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 1.6677 - accuracy: 0.7273 - val_loss: 6.8642 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 1.6905 - accuracy: 0.7045 - val_loss: 4.5044 - val_accuracy: 0.3125 - 21s/epoch - 10s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.5677 - accuracy: 0.9318 - val_loss: 3.9038 - val_accuracy: 0.3125 - 22s/epoch - 11s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 1.1996 - accuracy: 0.8182 - val_loss: 3.1071 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 1.1740 - accuracy: 0.7955 - val_loss: 2.4553 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5210 - accuracy: 0.9773 - val_loss: 2.7944 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5456 - accuracy: 1.0000 - val_loss: 2.9681 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5403 - accuracy: 1.0000 - val_loss: 3.0281 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5755 - accuracy: 1.0000 - val_loss: 3.0739 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5256 - accuracy: 1.0000 - val_loss: 3.1454 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5338 - accuracy: 1.0000 - val_loss: 3.1896 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5491 - accuracy: 1.0000 - val_loss: 3.1784 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.5487 - accuracy: 1.0000 - val_loss: 3.1433 - val_accuracy: 0.6250 - 21s/epoch - 10s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5575 - accuracy: 1.0000 - val_loss: 3.1146 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.5254 - accuracy: 1.0000 - val_loss: 3.0726 - val_accuracy: 0.6250 - 21s/epoch - 11s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5333 - accuracy: 1.0000 - val_loss: 3.0087 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5199 - accuracy: 1.0000 - val_loss: 2.9480 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5371 - accuracy: 1.0000 - val_loss: 2.8577 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5313 - accuracy: 1.0000 - val_loss: 2.7922 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.5186 - accuracy: 1.0000 - val_loss: 2.7129 - val_accuracy: 0.6250 - 22s/epoch - 11s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5084 - accuracy: 1.0000 - val_loss: 2.6377 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5091 - accuracy: 1.0000 - val_loss: 2.5817 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.5218 - accuracy: 1.0000 - val_loss: 2.5026 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4951 - accuracy: 1.0000 - val_loss: 2.4254 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.5095 - accuracy: 1.0000 - val_loss: 2.3724 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4790 - accuracy: 1.0000 - val_loss: 2.3196 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.4849 - accuracy: 1.0000 - val_loss: 2.2575 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4743 - accuracy: 1.0000 - val_loss: 2.1917 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4606 - accuracy: 1.0000 - val_loss: 2.1382 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4689 - accuracy: 1.0000 - val_loss: 2.0746 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4595 - accuracy: 1.0000 - val_loss: 2.0204 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4829 - accuracy: 1.0000 - val_loss: 1.9700 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.4456 - accuracy: 1.0000 - val_loss: 1.9225 - val_accuracy: 0.6250 - 21s/epoch - 10s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4396 - accuracy: 1.0000 - val_loss: 1.8750 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4425 - accuracy: 1.0000 - val_loss: 1.8293 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4303 - accuracy: 1.0000 - val_loss: 1.7947 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4650 - accuracy: 1.0000 - val_loss: 1.7617 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4261 - accuracy: 1.0000 - val_loss: 1.7210 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4240 - accuracy: 1.0000 - val_loss: 1.6900 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4152 - accuracy: 1.0000 - val_loss: 1.6568 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4104 - accuracy: 1.0000 - val_loss: 1.6278 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4307 - accuracy: 1.0000 - val_loss: 1.6040 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.4100 - accuracy: 1.0000 - val_loss: 1.5763 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.3936 - accuracy: 1.0000 - val_loss: 1.5499 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.3981 - accuracy: 1.0000 - val_loss: 1.5250 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.4111 - accuracy: 1.0000 - val_loss: 1.5079 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.3884 - accuracy: 1.0000 - val_loss: 1.4850 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3785 - accuracy: 1.0000 - val_loss: 1.4640 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3891 - accuracy: 1.0000 - val_loss: 1.4466 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3728 - accuracy: 1.0000 - val_loss: 1.4249 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3620 - accuracy: 1.0000 - val_loss: 1.4075 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3581 - accuracy: 1.0000 - val_loss: 1.3905 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3760 - accuracy: 1.0000 - val_loss: 1.3724 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3510 - accuracy: 1.0000 - val_loss: 1.3546 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3580 - accuracy: 1.0000 - val_loss: 1.3326 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.3419 - accuracy: 1.0000 - val_loss: 1.3155 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3426 - accuracy: 1.0000 - val_loss: 1.2964 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3345 - accuracy: 1.0000 - val_loss: 1.2803 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3352 - accuracy: 1.0000 - val_loss: 1.2627 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3435 - accuracy: 1.0000 - val_loss: 1.2460 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3383 - accuracy: 1.0000 - val_loss: 1.2310 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3240 - accuracy: 1.0000 - val_loss: 1.2206 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3266 - accuracy: 1.0000 - val_loss: 1.2064 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.3185 - accuracy: 1.0000 - val_loss: 1.1969 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.3108 - accuracy: 1.0000 - val_loss: 1.1860 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3091 - accuracy: 1.0000 - val_loss: 1.1733 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3122 - accuracy: 1.0000 - val_loss: 1.1621 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3363 - accuracy: 1.0000 - val_loss: 1.1469 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.3153 - accuracy: 1.0000 - val_loss: 1.1371 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3377 - accuracy: 1.0000 - val_loss: 1.1210 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2973 - accuracy: 1.0000 - val_loss: 1.1089 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3044 - accuracy: 1.0000 - val_loss: 1.1052 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.3061 - accuracy: 1.0000 - val_loss: 1.1120 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2944 - accuracy: 1.0000 - val_loss: 1.1066 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2902 - accuracy: 1.0000 - val_loss: 1.0976 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2903 - accuracy: 1.0000 - val_loss: 1.0884 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2926 - accuracy: 1.0000 - val_loss: 1.0853 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2755 - accuracy: 1.0000 - val_loss: 1.0788 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2852 - accuracy: 1.0000 - val_loss: 1.0663 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2840 - accuracy: 1.0000 - val_loss: 1.0559 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2667 - accuracy: 1.0000 - val_loss: 1.0491 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2679 - accuracy: 1.0000 - val_loss: 1.0381 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2909 - accuracy: 1.0000 - val_loss: 1.0279 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2591 - accuracy: 1.0000 - val_loss: 1.0138 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2918 - accuracy: 1.0000 - val_loss: 0.9868 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2635 - accuracy: 1.0000 - val_loss: 0.9553 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.78793\n",
      "2/2 - 23s - loss: 0.2542 - accuracy: 1.0000 - val_loss: 0.9321 - val_accuracy: 0.6875 - 23s/epoch - 11s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.78793\n",
      "2/2 - 23s - loss: 0.2537 - accuracy: 1.0000 - val_loss: 0.9106 - val_accuracy: 0.6875 - 23s/epoch - 11s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.2744 - accuracy: 1.0000 - val_loss: 0.8963 - val_accuracy: 0.7500 - 21s/epoch - 10s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.78793\n",
      "2/2 - 22s - loss: 0.2631 - accuracy: 1.0000 - val_loss: 0.8985 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2425 - accuracy: 1.0000 - val_loss: 0.9108 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.2396 - accuracy: 1.0000 - val_loss: 0.9256 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2344 - accuracy: 1.0000 - val_loss: 0.9349 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2347 - accuracy: 1.0000 - val_loss: 0.9368 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.2472 - accuracy: 1.0000 - val_loss: 0.9314 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2378 - accuracy: 1.0000 - val_loss: 0.9211 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2420 - accuracy: 1.0000 - val_loss: 0.9050 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.2305 - accuracy: 1.0000 - val_loss: 0.8975 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2249 - accuracy: 1.0000 - val_loss: 0.8957 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2222 - accuracy: 1.0000 - val_loss: 0.8865 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2288 - accuracy: 1.0000 - val_loss: 0.8741 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.2460 - accuracy: 1.0000 - val_loss: 0.8714 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.2309 - accuracy: 1.0000 - val_loss: 0.8635 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.2284 - accuracy: 1.0000 - val_loss: 0.8607 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2175 - accuracy: 1.0000 - val_loss: 0.8538 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2253 - accuracy: 1.0000 - val_loss: 0.8439 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2341 - accuracy: 1.0000 - val_loss: 0.8378 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2119 - accuracy: 1.0000 - val_loss: 0.8277 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.78793\n",
      "2/2 - 21s - loss: 0.2055 - accuracy: 1.0000 - val_loss: 0.8196 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2107 - accuracy: 1.0000 - val_loss: 0.8202 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2066 - accuracy: 1.0000 - val_loss: 0.8189 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.1991 - accuracy: 1.0000 - val_loss: 0.8176 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.2226 - accuracy: 1.0000 - val_loss: 0.8179 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1943 - accuracy: 1.0000 - val_loss: 0.8186 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2119 - accuracy: 1.0000 - val_loss: 0.8161 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.2037 - accuracy: 1.0000 - val_loss: 0.8075 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.1998 - accuracy: 1.0000 - val_loss: 0.8284 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1961 - accuracy: 1.0000 - val_loss: 0.8770 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.1839 - accuracy: 1.0000 - val_loss: 0.9073 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1843 - accuracy: 1.0000 - val_loss: 0.9008 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.1912 - accuracy: 1.0000 - val_loss: 0.8708 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.2001 - accuracy: 1.0000 - val_loss: 0.8432 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.1920 - accuracy: 1.0000 - val_loss: 0.8144 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.1770 - accuracy: 1.0000 - val_loss: 0.7971 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1748 - accuracy: 1.0000 - val_loss: 0.7889 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1812 - accuracy: 1.0000 - val_loss: 0.7888 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1844 - accuracy: 1.0000 - val_loss: 0.8027 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1989 - accuracy: 1.0000 - val_loss: 0.8121 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1786 - accuracy: 1.0000 - val_loss: 0.8192 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.1744 - accuracy: 1.0000 - val_loss: 0.8237 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1764 - accuracy: 1.0000 - val_loss: 0.9091 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.1656 - accuracy: 1.0000 - val_loss: 1.0796 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1812 - accuracy: 1.0000 - val_loss: 1.1317 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1634 - accuracy: 1.0000 - val_loss: 1.0533 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1847 - accuracy: 1.0000 - val_loss: 0.8562 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1717 - accuracy: 1.0000 - val_loss: 0.7905 - val_accuracy: 0.7500 - 19s/epoch - 9s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1694 - accuracy: 1.0000 - val_loss: 0.8188 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.78793\n",
      "2/2 - 20s - loss: 0.1578 - accuracy: 1.0000 - val_loss: 0.8395 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1627 - accuracy: 1.0000 - val_loss: 0.8084 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1585 - accuracy: 1.0000 - val_loss: 0.7896 - val_accuracy: 0.7500 - 19s/epoch - 9s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.1693 - accuracy: 1.0000 - val_loss: 0.8080 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.1584 - accuracy: 1.0000 - val_loss: 0.8637 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.1636 - accuracy: 1.0000 - val_loss: 0.9133 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1506 - accuracy: 1.0000 - val_loss: 0.9375 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.78793\n",
      "2/2 - 19s - loss: 0.1592 - accuracy: 1.0000 - val_loss: 0.9439 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.78793\n",
      "2/2 - 17s - loss: 0.1473 - accuracy: 1.0000 - val_loss: 0.9075 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.78793\n",
      "2/2 - 17s - loss: 0.1540 - accuracy: 1.0000 - val_loss: 0.8662 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.78793\n",
      "2/2 - 18s - loss: 0.1498 - accuracy: 1.0000 - val_loss: 0.8127 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss improved from 0.78793 to 0.76813, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.1433 - accuracy: 1.0000 - val_loss: 0.7681 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss improved from 0.76813 to 0.74753, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.1526 - accuracy: 1.0000 - val_loss: 0.7475 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss improved from 0.74753 to 0.73527, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.1404 - accuracy: 1.0000 - val_loss: 0.7353 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.73527\n",
      "2/2 - 18s - loss: 0.1407 - accuracy: 1.0000 - val_loss: 0.7380 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.73527\n",
      "2/2 - 18s - loss: 0.1388 - accuracy: 1.0000 - val_loss: 0.7503 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.73527\n",
      "2/2 - 18s - loss: 0.1499 - accuracy: 1.0000 - val_loss: 0.7733 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.73527\n",
      "2/2 - 18s - loss: 0.1364 - accuracy: 1.0000 - val_loss: 0.7855 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.73527\n",
      "2/2 - 18s - loss: 0.1376 - accuracy: 1.0000 - val_loss: 0.7546 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss improved from 0.73527 to 0.71055, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 24s - loss: 0.1426 - accuracy: 1.0000 - val_loss: 0.7105 - val_accuracy: 0.6875 - 24s/epoch - 12s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss improved from 0.71055 to 0.69864, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 0.1417 - accuracy: 1.0000 - val_loss: 0.6986 - val_accuracy: 0.7500 - 23s/epoch - 12s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss improved from 0.69864 to 0.69358, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 27s - loss: 0.1313 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.7500 - 27s/epoch - 14s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.69358\n",
      "2/2 - 24s - loss: 0.1380 - accuracy: 1.0000 - val_loss: 0.6968 - val_accuracy: 0.7500 - 24s/epoch - 12s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.69358\n",
      "2/2 - 23s - loss: 0.1301 - accuracy: 1.0000 - val_loss: 0.7035 - val_accuracy: 0.6875 - 23s/epoch - 12s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss improved from 0.69358 to 0.68973, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 0.1630 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.7500 - 23s/epoch - 12s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss improved from 0.68973 to 0.68309, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 0.1436 - accuracy: 1.0000 - val_loss: 0.6831 - val_accuracy: 0.6875 - 23s/epoch - 11s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.68309\n",
      "2/2 - 22s - loss: 0.1270 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.68309\n",
      "2/2 - 22s - loss: 0.1371 - accuracy: 1.0000 - val_loss: 0.7044 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.68309\n",
      "2/2 - 21s - loss: 0.1453 - accuracy: 1.0000 - val_loss: 0.7607 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.68309\n",
      "2/2 - 20s - loss: 0.1306 - accuracy: 1.0000 - val_loss: 0.8031 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.68309\n",
      "2/2 - 19s - loss: 0.1283 - accuracy: 1.0000 - val_loss: 0.8114 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.68309\n",
      "2/2 - 20s - loss: 0.1215 - accuracy: 1.0000 - val_loss: 0.7784 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.68309\n",
      "2/2 - 19s - loss: 0.1203 - accuracy: 1.0000 - val_loss: 0.7386 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.68309\n",
      "2/2 - 19s - loss: 0.1257 - accuracy: 1.0000 - val_loss: 0.7095 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.68309\n",
      "2/2 - 19s - loss: 0.1323 - accuracy: 1.0000 - val_loss: 0.6957 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.68309\n",
      "2/2 - 19s - loss: 0.1320 - accuracy: 1.0000 - val_loss: 0.6920 - val_accuracy: 0.7500 - 19s/epoch - 9s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.68309\n",
      "2/2 - 19s - loss: 0.1175 - accuracy: 1.0000 - val_loss: 0.7085 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.68309\n",
      "2/2 - 18s - loss: 0.1222 - accuracy: 1.0000 - val_loss: 0.6855 - val_accuracy: 0.7500 - 18s/epoch - 9s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.68309\n",
      "2/2 - 18s - loss: 0.1407 - accuracy: 1.0000 - val_loss: 0.7574 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.68309\n",
      "2/2 - 18s - loss: 0.1158 - accuracy: 1.0000 - val_loss: 0.8367 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.68309\n",
      "2/2 - 17s - loss: 0.1403 - accuracy: 1.0000 - val_loss: 0.8827 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.68309\n",
      "2/2 - 17s - loss: 0.5816 - accuracy: 0.8864 - val_loss: 12.4886 - val_accuracy: 0.6875 - 17s/epoch - 8s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.68309\n",
      "2/2 - 18s - loss: 32.6824 - accuracy: 0.5227 - val_loss: 3.5800 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.68309\n",
      "2/2 - 18s - loss: 7.2961 - accuracy: 0.7500 - val_loss: 1.3330 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.68309\n",
      "2/2 - 17s - loss: 0.2867 - accuracy: 0.9318 - val_loss: 2.3390 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.68309\n",
      "2/2 - 18s - loss: 0.3818 - accuracy: 0.8864 - val_loss: 3.3036 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.68309\n",
      "2/2 - 17s - loss: 0.6642 - accuracy: 0.8182 - val_loss: 8.5186 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Classification accuracy: 0.554017 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49535114 0.5046542 ]\n",
      " [0.57664955 0.42335668]\n",
      " [0.5051649  0.4948404 ]\n",
      " [0.6698863  0.3301189 ]\n",
      " [0.8654175  0.13457999]\n",
      " [0.70322603 0.29677698]\n",
      " [0.40584943 0.59415764]\n",
      " [0.30876002 0.691245  ]\n",
      " [0.62726325 0.3727362 ]\n",
      " [0.5356905  0.46431568]\n",
      " [0.5011029  0.49890018]\n",
      " [0.6419778  0.35802346]\n",
      " [0.50045997 0.49954244]\n",
      " [0.6443047  0.35569978]\n",
      " [0.61499846 0.38500226]\n",
      " [0.8026085  0.19739002]\n",
      " [0.5537232  0.44628215]\n",
      " [0.5887728  0.41123027]\n",
      " [0.6090183  0.3909854 ]]\n",
      "[1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[11  0]\n",
      " [ 5  3]]\n",
      "[ 73.68  68.75 100.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with init data, 300 epochs, softmax\n",
<<<<<<< Updated upstream
    "probs_TSGL_init = np.array([[0.34468523, 0.65531474],\n",
=======
    "probs_TSGL_init_soft = np.array([[0.34468523, 0.65531474],\n",
>>>>>>> Stashed changes
    "                            [0.4223048,  0.5776952 ],\n",
    "                            [0.66058546, 0.33941454],\n",
    "                            [0.82226074, 0.1777392 ],\n",
    "                            [0.85768410, 0.142316  ],\n",
    "                            [0.79356056, 0.2064394 ],\n",
    "                            [0.43697017, 0.5630298 ],\n",
    "                            [0.23311326, 0.7668868 ],\n",
    "                            [0.06507578, 0.93492424],\n",
    "                            [0.48482734, 0.5151727 ],\n",
    "                            [0.50462850, 0.49537155],\n",
    "                            [0.68665516, 0.3133448 ],\n",
    "                            [0.46980935, 0.5301907 ],\n",
    "                            [0.55152243, 0.4484776 ],\n",
    "                            [0.11421819, 0.8857818 ],\n",
    "                            [0.25776443, 0.74223554],\n",
    "                            [0.45055872, 0.5494413 ],\n",
    "                            [0.75034060, 0.24965943],\n",
    "                            [0.59446220, 0.40553778]])\n",
    "\n",
    "# with init data, 300 epochs, softmax\n",
    "probs_TSGL_init_sigm = np.array([[0.49535114 0.5046542 ]\n",
    " [0.57664955 0.42335668]\n",
    " [0.5051649  0.4948404 ]\n",
    " [0.6698863  0.3301189 ]\n",
    " [0.8654175  0.13457999]\n",
    " [0.70322603 0.29677698]\n",
    " [0.40584943 0.59415764]\n",
    " [0.30876002 0.691245  ]\n",
    " [0.62726325 0.3727362 ]\n",
    " [0.5356905  0.46431568]\n",
    " [0.5011029  0.49890018]\n",
    " [0.6419778  0.35802346]\n",
    " [0.50045997 0.49954244]\n",
    " [0.6443047  0.35569978]\n",
    " [0.61499846 0.38500226]\n",
    " [0.8026085  0.19739002]\n",
    " [0.5537232  0.44628215]\n",
    " [0.5887728  0.41123027]\n",
    " [0.6090183  0.3909854 ]])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 8.09153, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 14s - loss: 7.8203 - accuracy: 0.4773 - val_loss: 8.0915 - val_accuracy: 0.6875 - 14s/epoch - 7s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 8.09153 to 1.37230, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 11s - loss: 12.2146 - accuracy: 0.5000 - val_loss: 1.3723 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 0.3350 - accuracy: 0.8636 - val_loss: 2.6437 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 11.8041 - accuracy: 0.6591 - val_loss: 10.2536 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 3.7748 - accuracy: 0.7727 - val_loss: 9.0993 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.37230\n",
      "2/2 - 11s - loss: 1.9966 - accuracy: 0.7500 - val_loss: 8.0614 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 1.37230 to 1.26682, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 11s - loss: 0.6301 - accuracy: 0.8636 - val_loss: 1.2668 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5868 - accuracy: 0.9318 - val_loss: 11.3714 - val_accuracy: 0.6250 - 11s/epoch - 5s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.1444 - accuracy: 0.7045 - val_loss: 20.2057 - val_accuracy: 0.6250 - 11s/epoch - 5s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4914 - accuracy: 0.7955 - val_loss: 13.1583 - val_accuracy: 0.6250 - 11s/epoch - 5s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4072 - accuracy: 0.8409 - val_loss: 3.6578 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.8117 - accuracy: 0.8636 - val_loss: 5.8793 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.1059e-04 - accuracy: 1.0000 - val_loss: 7.0600 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.2872e-05 - accuracy: 1.0000 - val_loss: 7.7414 - val_accuracy: 0.5625 - 11s/epoch - 5s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0161 - accuracy: 1.0000 - val_loss: 8.5172 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 8.8442 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 8.6818 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 9.0005 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8203e-05 - accuracy: 1.0000 - val_loss: 8.9204 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3175e-04 - accuracy: 1.0000 - val_loss: 8.7869 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0755 - accuracy: 0.9545 - val_loss: 6.8538 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0280e-04 - accuracy: 1.0000 - val_loss: 4.1668 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2317e-05 - accuracy: 1.0000 - val_loss: 2.3026 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.0997 - val_accuracy: 0.3125 - 11s/epoch - 5s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.7641 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.3052 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7746e-05 - accuracy: 1.0000 - val_loss: 3.6790 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.8392e-05 - accuracy: 1.0000 - val_loss: 3.9137 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.7343e-06 - accuracy: 1.0000 - val_loss: 4.0930 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7437e-04 - accuracy: 1.0000 - val_loss: 4.1246 - val_accuracy: 0.3750 - 11s/epoch - 5s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.4624e-05 - accuracy: 1.0000 - val_loss: 4.2337 - val_accuracy: 0.3750 - 10s/epoch - 5s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.8992e-05 - accuracy: 1.0000 - val_loss: 4.3333 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.9389e-05 - accuracy: 1.0000 - val_loss: 4.4658 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.6551e-05 - accuracy: 1.0000 - val_loss: 4.4993 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.2754e-05 - accuracy: 1.0000 - val_loss: 4.5091 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.6058e-04 - accuracy: 1.0000 - val_loss: 4.5108 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4780e-04 - accuracy: 1.0000 - val_loss: 4.4206 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.0822e-04 - accuracy: 1.0000 - val_loss: 4.4161 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6139e-05 - accuracy: 1.0000 - val_loss: 4.2555 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.5661e-06 - accuracy: 1.0000 - val_loss: 4.1531 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.5603e-04 - accuracy: 1.0000 - val_loss: 4.0003 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.4001e-06 - accuracy: 1.0000 - val_loss: 3.8715 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.6380e-05 - accuracy: 1.0000 - val_loss: 3.7605 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0580e-05 - accuracy: 1.0000 - val_loss: 3.7036 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.1831e-06 - accuracy: 1.0000 - val_loss: 3.5900 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0991e-05 - accuracy: 1.0000 - val_loss: 3.5034 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7391e-06 - accuracy: 1.0000 - val_loss: 3.3993 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4021e-05 - accuracy: 1.0000 - val_loss: 3.3614 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.2700e-06 - accuracy: 1.0000 - val_loss: 3.2842 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.0929e-05 - accuracy: 1.0000 - val_loss: 3.3054 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.5620e-06 - accuracy: 1.0000 - val_loss: 3.2620 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.1162e-06 - accuracy: 1.0000 - val_loss: 3.2385 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3844e-06 - accuracy: 1.0000 - val_loss: 3.1537 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1142e-05 - accuracy: 1.0000 - val_loss: 3.1087 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.2121e-06 - accuracy: 1.0000 - val_loss: 3.0678 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8342e-06 - accuracy: 1.0000 - val_loss: 3.0777 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0019e-06 - accuracy: 1.0000 - val_loss: 3.0956 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.5517e-06 - accuracy: 1.0000 - val_loss: 3.0525 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4384e-06 - accuracy: 1.0000 - val_loss: 3.0695 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.1972e-06 - accuracy: 1.0000 - val_loss: 3.0689 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5633e-06 - accuracy: 1.0000 - val_loss: 3.0752 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4695e-05 - accuracy: 1.0000 - val_loss: 3.1076 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7583e-06 - accuracy: 1.0000 - val_loss: 3.1152 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.2832e-06 - accuracy: 1.0000 - val_loss: 3.0205 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9642e-06 - accuracy: 1.0000 - val_loss: 3.0173 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0072e-06 - accuracy: 1.0000 - val_loss: 3.0043 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.1631e-06 - accuracy: 1.0000 - val_loss: 3.0301 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.7702e-06 - accuracy: 1.0000 - val_loss: 2.9666 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8948e-05 - accuracy: 1.0000 - val_loss: 2.9496 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9615e-06 - accuracy: 1.0000 - val_loss: 2.9547 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0301e-05 - accuracy: 1.0000 - val_loss: 2.9727 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.0636e-06 - accuracy: 1.0000 - val_loss: 2.9959 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.5018e-05 - accuracy: 1.0000 - val_loss: 2.9783 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.9010e-06 - accuracy: 1.0000 - val_loss: 2.9858 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.5788e-06 - accuracy: 1.0000 - val_loss: 3.0194 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2896e-06 - accuracy: 1.0000 - val_loss: 3.0285 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2869e-06 - accuracy: 1.0000 - val_loss: 3.0017 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.9595e-06 - accuracy: 1.0000 - val_loss: 2.9777 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.0393e-07 - accuracy: 1.0000 - val_loss: 3.0007 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4413e-06 - accuracy: 1.0000 - val_loss: 2.9549 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8694e-06 - accuracy: 1.0000 - val_loss: 2.9336 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.9764e-06 - accuracy: 1.0000 - val_loss: 2.9357 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.6135e-06 - accuracy: 1.0000 - val_loss: 2.9319 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.9423e-06 - accuracy: 1.0000 - val_loss: 2.9340 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4681e-06 - accuracy: 1.0000 - val_loss: 2.9443 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4735e-06 - accuracy: 1.0000 - val_loss: 2.9449 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.8054e-06 - accuracy: 1.0000 - val_loss: 2.9065 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4847e-06 - accuracy: 1.0000 - val_loss: 2.8372 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.3441e-06 - accuracy: 1.0000 - val_loss: 2.8268 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7851e-06 - accuracy: 1.0000 - val_loss: 2.8344 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0295e-06 - accuracy: 1.0000 - val_loss: 2.8560 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7583e-06 - accuracy: 1.0000 - val_loss: 2.8612 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.3973e-06 - accuracy: 1.0000 - val_loss: 2.7927 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.9653e-07 - accuracy: 1.0000 - val_loss: 2.8077 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3574e-06 - accuracy: 1.0000 - val_loss: 2.8145 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.1779e-06 - accuracy: 1.0000 - val_loss: 2.8140 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.5199e-06 - accuracy: 1.0000 - val_loss: 2.8430 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6987e-06 - accuracy: 1.0000 - val_loss: 2.8388 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3462e-06 - accuracy: 1.0000 - val_loss: 2.9189 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4221e-06 - accuracy: 1.0000 - val_loss: 2.9250 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.5343e-07 - accuracy: 1.0000 - val_loss: 2.8637 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9019e-06 - accuracy: 1.0000 - val_loss: 2.8715 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.6131e-07 - accuracy: 1.0000 - val_loss: 2.8610 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7502e-06 - accuracy: 1.0000 - val_loss: 2.8744 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.8878e-06 - accuracy: 1.0000 - val_loss: 2.8805 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5928e-06 - accuracy: 1.0000 - val_loss: 2.9053 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.7979e-07 - accuracy: 1.0000 - val_loss: 2.9275 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1268e-06 - accuracy: 1.0000 - val_loss: 2.9529 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0268e-06 - accuracy: 1.0000 - val_loss: 2.9282 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0130e-06 - accuracy: 1.0000 - val_loss: 2.8526 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.8938e-06 - accuracy: 1.0000 - val_loss: 2.8321 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7986e-06 - accuracy: 1.0000 - val_loss: 2.9094 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5115e-06 - accuracy: 1.0000 - val_loss: 2.8095 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.4976e-06 - accuracy: 1.0000 - val_loss: 2.7421 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.8288e-06 - accuracy: 1.0000 - val_loss: 2.7854 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3492e-06 - accuracy: 1.0000 - val_loss: 2.8464 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0405e-05 - accuracy: 1.0000 - val_loss: 2.8325 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7990e-06 - accuracy: 1.0000 - val_loss: 2.8115 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.4825e-07 - accuracy: 1.0000 - val_loss: 2.7810 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.2460e-06 - accuracy: 1.0000 - val_loss: 2.7953 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.6637e-05 - accuracy: 1.0000 - val_loss: 2.8530 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.1845e-07 - accuracy: 1.0000 - val_loss: 2.8149 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8369e-06 - accuracy: 1.0000 - val_loss: 2.8291 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3983e-05 - accuracy: 1.0000 - val_loss: 2.7948 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.7805e-07 - accuracy: 1.0000 - val_loss: 2.7932 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.9124e-06 - accuracy: 1.0000 - val_loss: 2.8082 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7445e-06 - accuracy: 1.0000 - val_loss: 2.8040 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 8.1279e-07 - accuracy: 1.0000 - val_loss: 2.8065 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5034e-06 - accuracy: 1.0000 - val_loss: 2.7610 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3438e-06 - accuracy: 1.0000 - val_loss: 2.8000 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.1723e-07 - accuracy: 1.0000 - val_loss: 2.7975 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.7853e-05 - accuracy: 1.0000 - val_loss: 2.7866 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.7604e-06 - accuracy: 1.0000 - val_loss: 2.7340 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.8552e-06 - accuracy: 1.0000 - val_loss: 2.7551 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2761e-06 - accuracy: 1.0000 - val_loss: 2.7162 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0157e-06 - accuracy: 1.0000 - val_loss: 2.7487 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6391e-06 - accuracy: 1.0000 - val_loss: 2.6918 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.1586e-06 - accuracy: 1.0000 - val_loss: 2.6823 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7366e-06 - accuracy: 1.0000 - val_loss: 2.6690 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.4028e-06 - accuracy: 1.0000 - val_loss: 2.6410 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.5294e-06 - accuracy: 1.0000 - val_loss: 2.6049 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.8743e-07 - accuracy: 1.0000 - val_loss: 2.5412 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8315e-06 - accuracy: 1.0000 - val_loss: 2.5509 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.5072e-07 - accuracy: 1.0000 - val_loss: 2.5805 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.4489e-06 - accuracy: 1.0000 - val_loss: 2.6074 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3438e-06 - accuracy: 1.0000 - val_loss: 2.5511 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.7054e-06 - accuracy: 1.0000 - val_loss: 2.5274 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.9669e-06 - accuracy: 1.0000 - val_loss: 2.5448 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.7420e-05 - accuracy: 1.0000 - val_loss: 2.5524 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.3964e-07 - accuracy: 1.0000 - val_loss: 2.5333 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1142e-05 - accuracy: 1.0000 - val_loss: 2.5364 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7475e-06 - accuracy: 1.0000 - val_loss: 2.5498 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.0164e-06 - accuracy: 1.0000 - val_loss: 2.5437 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7281e-06 - accuracy: 1.0000 - val_loss: 2.5065 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4302e-06 - accuracy: 1.0000 - val_loss: 2.4829 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.3939e-07 - accuracy: 1.0000 - val_loss: 2.5209 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.4582e-06 - accuracy: 1.0000 - val_loss: 2.5431 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7556e-06 - accuracy: 1.0000 - val_loss: 2.5459 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.4711e-06 - accuracy: 1.0000 - val_loss: 2.5503 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.4801e-05 - accuracy: 1.0000 - val_loss: 2.4662 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.1725e-06 - accuracy: 1.0000 - val_loss: 2.4383 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.7166e-07 - accuracy: 1.0000 - val_loss: 2.4525 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3543e-06 - accuracy: 1.0000 - val_loss: 2.4080 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.5982e-06 - accuracy: 1.0000 - val_loss: 2.4045 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.5362e-06 - accuracy: 1.0000 - val_loss: 2.3716 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2977e-06 - accuracy: 1.0000 - val_loss: 2.4183 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.6697e-07 - accuracy: 1.0000 - val_loss: 2.4424 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.9313e-06 - accuracy: 1.0000 - val_loss: 2.4172 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.7143e-06 - accuracy: 1.0000 - val_loss: 2.4609 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.8086e-06 - accuracy: 1.0000 - val_loss: 2.4693 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.4728e-07 - accuracy: 1.0000 - val_loss: 2.4778 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.1108e-06 - accuracy: 1.0000 - val_loss: 2.5137 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0079e-06 - accuracy: 1.0000 - val_loss: 2.5433 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0595e-06 - accuracy: 1.0000 - val_loss: 2.4751 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.4505e-07 - accuracy: 1.0000 - val_loss: 2.4982 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.8889e-07 - accuracy: 1.0000 - val_loss: 2.4958 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4251e-06 - accuracy: 1.0000 - val_loss: 2.4690 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.4210e-07 - accuracy: 1.0000 - val_loss: 2.4695 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.6378e-07 - accuracy: 1.0000 - val_loss: 2.4889 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0810e-06 - accuracy: 1.0000 - val_loss: 2.5148 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.4299e-06 - accuracy: 1.0000 - val_loss: 2.5099 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3384e-06 - accuracy: 1.0000 - val_loss: 2.5232 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0783e-06 - accuracy: 1.0000 - val_loss: 2.5373 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.0915e-06 - accuracy: 1.0000 - val_loss: 2.5478 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0620e-06 - accuracy: 1.0000 - val_loss: 2.5379 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.5023e-07 - accuracy: 1.0000 - val_loss: 2.4962 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9588e-06 - accuracy: 1.0000 - val_loss: 2.5238 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.8231e-06 - accuracy: 1.0000 - val_loss: 2.5571 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.7659e-07 - accuracy: 1.0000 - val_loss: 2.4878 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.8170e-06 - accuracy: 1.0000 - val_loss: 2.4759 - val_accuracy: 0.5000 - 11s/epoch - 5s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4007e-06 - accuracy: 1.0000 - val_loss: 2.4861 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.6673e-07 - accuracy: 1.0000 - val_loss: 2.5001 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2706e-06 - accuracy: 1.0000 - val_loss: 2.5452 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5931e-06 - accuracy: 1.0000 - val_loss: 2.5262 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0171e-07 - accuracy: 1.0000 - val_loss: 2.4887 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0024e-06 - accuracy: 1.0000 - val_loss: 2.5202 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9398e-06 - accuracy: 1.0000 - val_loss: 2.5301 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.7215e-07 - accuracy: 1.0000 - val_loss: 2.5223 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6770e-06 - accuracy: 1.0000 - val_loss: 2.5624 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.6223e-06 - accuracy: 1.0000 - val_loss: 2.5581 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.7510e-07 - accuracy: 1.0000 - val_loss: 2.5497 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.7383e-06 - accuracy: 1.0000 - val_loss: 2.4915 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0756e-06 - accuracy: 1.0000 - val_loss: 2.5142 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.0417e-07 - accuracy: 1.0000 - val_loss: 2.4813 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7908e-06 - accuracy: 1.0000 - val_loss: 2.4688 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3492e-06 - accuracy: 1.0000 - val_loss: 2.4452 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.8396e-06 - accuracy: 1.0000 - val_loss: 2.4473 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0712e-07 - accuracy: 1.0000 - val_loss: 2.4167 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.5540e-07 - accuracy: 1.0000 - val_loss: 2.3790 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1945e-07 - accuracy: 1.0000 - val_loss: 2.4283 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.4878e-05 - accuracy: 1.0000 - val_loss: 2.4337 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5114e-06 - accuracy: 1.0000 - val_loss: 2.4819 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.8341e-06 - accuracy: 1.0000 - val_loss: 2.4089 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.0688e-07 - accuracy: 1.0000 - val_loss: 2.4462 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.6920e-07 - accuracy: 1.0000 - val_loss: 2.4595 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.8201e-07 - accuracy: 1.0000 - val_loss: 2.4703 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.5849e-06 - accuracy: 1.0000 - val_loss: 2.5231 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7989e-06 - accuracy: 1.0000 - val_loss: 2.5421 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3001e-06 - accuracy: 1.0000 - val_loss: 2.5338 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.7757e-07 - accuracy: 1.0000 - val_loss: 2.5339 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3571e-06 - accuracy: 1.0000 - val_loss: 2.5528 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.3002e-06 - accuracy: 1.0000 - val_loss: 2.5627 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.6180e-07 - accuracy: 1.0000 - val_loss: 2.5801 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2625e-06 - accuracy: 1.0000 - val_loss: 2.5536 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0984e-07 - accuracy: 1.0000 - val_loss: 2.5614 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.5954e-06 - accuracy: 1.0000 - val_loss: 2.5766 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.5270e-07 - accuracy: 1.0000 - val_loss: 2.5511 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.0994e-06 - accuracy: 1.0000 - val_loss: 2.5630 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7285e-06 - accuracy: 1.0000 - val_loss: 2.5476 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.0441e-07 - accuracy: 1.0000 - val_loss: 2.5446 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.6635e-06 - accuracy: 1.0000 - val_loss: 2.5230 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.6527e-07 - accuracy: 1.0000 - val_loss: 2.5579 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4332e-06 - accuracy: 1.0000 - val_loss: 2.5166 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1108e-06 - accuracy: 1.0000 - val_loss: 2.5319 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0566e-07 - accuracy: 1.0000 - val_loss: 2.4943 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0458e-06 - accuracy: 1.0000 - val_loss: 2.4897 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 7.3964e-07 - accuracy: 1.0000 - val_loss: 2.5115 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.8911e-06 - accuracy: 1.0000 - val_loss: 2.4826 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2598e-06 - accuracy: 1.0000 - val_loss: 2.4870 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.2314e-07 - accuracy: 1.0000 - val_loss: 2.5259 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.2831e-07 - accuracy: 1.0000 - val_loss: 2.4931 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2083e-06 - accuracy: 1.0000 - val_loss: 2.4960 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7364e-07 - accuracy: 1.0000 - val_loss: 2.5309 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2517e-06 - accuracy: 1.0000 - val_loss: 2.5312 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.4004e-06 - accuracy: 1.0000 - val_loss: 2.5564 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5253e-06 - accuracy: 1.0000 - val_loss: 2.4613 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.8690e-06 - accuracy: 1.0000 - val_loss: 2.4397 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.7393e-06 - accuracy: 1.0000 - val_loss: 2.4093 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.8760e-06 - accuracy: 1.0000 - val_loss: 2.3975 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.0160e-06 - accuracy: 1.0000 - val_loss: 2.4433 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1189e-06 - accuracy: 1.0000 - val_loss: 2.4828 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5307e-06 - accuracy: 1.0000 - val_loss: 2.4540 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0861e-06 - accuracy: 1.0000 - val_loss: 2.4463 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.5551e-06 - accuracy: 1.0000 - val_loss: 2.4510 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.6721e-07 - accuracy: 1.0000 - val_loss: 2.4419 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.2043e-07 - accuracy: 1.0000 - val_loss: 2.4728 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.2028e-06 - accuracy: 1.0000 - val_loss: 2.5026 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.8864e-07 - accuracy: 1.0000 - val_loss: 2.5152 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.9507e-07 - accuracy: 1.0000 - val_loss: 2.5658 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 7.9924e-07 - accuracy: 1.0000 - val_loss: 2.5236 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1105e-06 - accuracy: 1.0000 - val_loss: 2.5205 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.9063e-07 - accuracy: 1.0000 - val_loss: 2.5600 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.2560e-07 - accuracy: 1.0000 - val_loss: 2.5443 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 6.1501e-07 - accuracy: 1.0000 - val_loss: 2.5249 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.2595e-06 - accuracy: 1.0000 - val_loss: 2.4819 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4166e-06 - accuracy: 1.0000 - val_loss: 2.4922 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.2351e-06 - accuracy: 1.0000 - val_loss: 2.5342 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.8719e-07 - accuracy: 1.0000 - val_loss: 2.4388 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3763e-06 - accuracy: 1.0000 - val_loss: 2.4650 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 5.5812e-07 - accuracy: 1.0000 - val_loss: 2.4663 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1620e-06 - accuracy: 1.0000 - val_loss: 2.4957 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7635e-07 - accuracy: 1.0000 - val_loss: 2.5308 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 5.6241e-06 - accuracy: 1.0000 - val_loss: 2.5005 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2219e-06 - accuracy: 1.0000 - val_loss: 2.5324 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.6364e-06 - accuracy: 1.0000 - val_loss: 2.5005 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.5638e-07 - accuracy: 1.0000 - val_loss: 2.4458 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 6.4210e-07 - accuracy: 1.0000 - val_loss: 2.4564 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 9.7535e-07 - accuracy: 1.0000 - val_loss: 2.4989 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.9558e-06 - accuracy: 1.0000 - val_loss: 2.5319 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.4221e-06 - accuracy: 1.0000 - val_loss: 2.4773 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.1921e-06 - accuracy: 1.0000 - val_loss: 2.5112 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.7688e-06 - accuracy: 1.0000 - val_loss: 2.5023 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 4.9851e-07 - accuracy: 1.0000 - val_loss: 2.4902 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.3303e-06 - accuracy: 1.0000 - val_loss: 2.4974 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3384e-06 - accuracy: 1.0000 - val_loss: 2.4948 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 3.3107e-06 - accuracy: 1.0000 - val_loss: 2.4917 - val_accuracy: 0.5000 - 10s/epoch - 5s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 8.3717e-07 - accuracy: 1.0000 - val_loss: 2.5020 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.1671e-06 - accuracy: 1.0000 - val_loss: 2.4930 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.4793e-06 - accuracy: 1.0000 - val_loss: 2.4342 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 2.1891e-06 - accuracy: 1.0000 - val_loss: 2.4242 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.0563e-06 - accuracy: 1.0000 - val_loss: 2.4340 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 1.2029e-06 - accuracy: 1.0000 - val_loss: 2.4360 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0051e-06 - accuracy: 1.0000 - val_loss: 2.4509 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 9.1574e-07 - accuracy: 1.0000 - val_loss: 2.4543 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 4.0910e-07 - accuracy: 1.0000 - val_loss: 2.4365 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 3.9285e-07 - accuracy: 1.0000 - val_loss: 2.4189 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.0837e-06 - accuracy: 1.0000 - val_loss: 2.3982 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.3113e-06 - accuracy: 1.0000 - val_loss: 2.4308 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 1.26682\n",
      "2/2 - 10s - loss: 1.2977e-06 - accuracy: 1.0000 - val_loss: 2.4475 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 1.26682\n",
      "2/2 - 11s - loss: 2.7712e-05 - accuracy: 1.0000 - val_loss: 2.4801 - val_accuracy: 0.4375 - 11s/epoch - 5s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Classification accuracy: 0.512465 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.95184946e-01 3.04815054e-01]\n",
      " [4.21892613e-01 5.78107417e-01]\n",
      " [7.89708734e-01 2.10291266e-01]\n",
      " [9.99535143e-01 4.64851793e-04]\n",
      " [1.59405246e-01 8.40594828e-01]\n",
      " [9.02964830e-01 9.70351920e-02]\n",
      " [1.18944913e-01 8.81055117e-01]\n",
      " [1.71721101e-01 8.28278899e-01]\n",
      " [9.67139781e-01 3.28601785e-02]\n",
      " [7.46518612e-01 2.53481418e-01]\n",
      " [2.98798531e-01 7.01201379e-01]\n",
      " [1.78000614e-10 9.99999940e-01]\n",
      " [8.16536665e-01 1.83463335e-01]\n",
      " [2.43162528e-01 7.56837428e-01]\n",
      " [1.04919985e-01 8.95080090e-01]\n",
      " [9.93469775e-01 6.53020665e-03]\n",
      " [9.06261265e-01 9.37386826e-02]\n",
      " [6.60291553e-01 3.39708477e-01]\n",
      " [7.47849047e-01 2.52150923e-01]]\n",
      "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[7 4]\n",
      " [4 4]]\n",
      "[57.89 63.64 50.  ]\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data, 300 epochs, softmax activation\n",
<<<<<<< Updated upstream
    "probs_Deep_init_soft = np.array([[7.0045400e-01, 2.9954603e-01],\n",
=======
    "probs_Deep_init = np.array([[7.0045400e-01, 2.9954603e-01],\n",
>>>>>>> Stashed changes
    "                            [7.8413427e-01, 2.1586572e-01],\n",
    "                            [7.1874863e-01, 2.8125137e-01],\n",
    "                            [9.9979991e-01, 2.0014797e-04],\n",
    "                            [2.9218609e-02, 9.7078133e-01]\n",
    "                            [9.9454159e-01, 5.4584546e-03],\n",
    "                            [5.5008806e-02, 9.4499117e-01],\n",
    "                            [9.6587259e-01, 3.4127403e-02],\n",
    "                            [6.3271725e-01, 3.6728275e-01],\n",
    "                            [9.8455411e-01, 1.5445878e-02],\n",
    "                            [8.1000119e-01, 1.8999882e-01],\n",
    "                            [9.9752015e-01, 2.4798173e-03],\n",
    "                            [9.8586375e-01, 1.4136297e-02],\n",
    "                            [4.3618846e-01, 5.6381154e-01],\n",
    "                            [1.4959927e-01, 8.5040075e-01],\n",
    "                            [9.5763546e-01, 4.2364582e-02],\n",
    "                            [9.9997401e-01, 2.5972882e-05],\n",
    "                            [9.9117404e-01, 8.8259308e-03],\n",
    "                            [9.9719328e-01, 2.8067816e-03]])\n",
    "\n",
    "\n",
    "probs_Deep_init_sigm = np.array([[6.95184946e-01 3.04815054e-01]\n",
    " [4.21892613e-01 5.78107417e-01]\n",
    " [7.89708734e-01 2.10291266e-01]\n",
    " [9.99535143e-01 4.64851793e-04]\n",
    " [1.59405246e-01 8.40594828e-01]\n",
    " [9.02964830e-01 9.70351920e-02]\n",
    " [1.18944913e-01 8.81055117e-01]\n",
    " [1.71721101e-01 8.28278899e-01]\n",
    " [9.67139781e-01 3.28601785e-02]\n",
    " [7.46518612e-01 2.53481418e-01]\n",
    " [2.98798531e-01 7.01201379e-01]\n",
    " [1.78000614e-10 9.99999940e-01]\n",
    " [8.16536665e-01 1.83463335e-01]\n",
    " [2.43162528e-01 7.56837428e-01]\n",
    " [1.04919985e-01 8.95080090e-01]\n",
    " [9.93469775e-01 6.53020665e-03]\n",
    " [9.06261265e-01 9.37386826e-02]\n",
    " [6.60291553e-01 3.39708477e-01]\n",
    " [7.47849047e-01 2.52150923e-01]]\n",
    "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
    "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 43.95264, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 10s - loss: 78.8688 - accuracy: 0.4318 - val_loss: 43.9526 - val_accuracy: 0.4375 - 10s/epoch - 5s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 86.1343 - accuracy: 0.5455 - val_loss: 149.1919 - val_accuracy: 0.4375 - 9s/epoch - 4s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 202.2070 - accuracy: 0.5000 - val_loss: 94.7610 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 131.3749 - accuracy: 0.5227 - val_loss: 96.1171 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 80.6944 - accuracy: 0.5227 - val_loss: 112.3041 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 123.3803 - accuracy: 0.5227 - val_loss: 92.6022 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 43.95264\n",
      "2/2 - 9s - loss: 78.4258 - accuracy: 0.6136 - val_loss: 60.1742 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 43.95264 to 32.75097, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 37.4668 - accuracy: 0.5909 - val_loss: 32.7510 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 32.75097\n",
      "2/2 - 8s - loss: 10.1157 - accuracy: 0.7273 - val_loss: 39.9827 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 32.75097 to 18.14144, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 14.9758 - accuracy: 0.7955 - val_loss: 18.1414 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 18.14144\n",
      "2/2 - 9s - loss: 0.2918 - accuracy: 0.9318 - val_loss: 26.9031 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 18.14144 to 11.75048, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 0.6369 - accuracy: 0.9318 - val_loss: 11.7505 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 11.75048 to 5.23638, saving model to /tmp/checkpoint.h5\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.2364 - val_accuracy: 0.5625 - 9s/epoch - 4s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 13.6759 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0091 - accuracy: 1.0000 - val_loss: 17.3170 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0734 - accuracy: 0.9773 - val_loss: 5.6160 - val_accuracy: 0.6250 - 9s/epoch - 4s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2106 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 6.6937e-06 - accuracy: 1.0000 - val_loss: 14.5230 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.1760 - accuracy: 0.9773 - val_loss: 14.6603 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.0862e-04 - accuracy: 1.0000 - val_loss: 14.4515 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 4.8248e-06 - accuracy: 1.0000 - val_loss: 14.5977 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.1406e-06 - accuracy: 1.0000 - val_loss: 14.6366 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 5.6895e-08 - accuracy: 1.0000 - val_loss: 14.5806 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.4129e-06 - accuracy: 1.0000 - val_loss: 14.3573 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.8667e-06 - accuracy: 1.0000 - val_loss: 14.3160 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 7.5860e-08 - accuracy: 1.0000 - val_loss: 14.3574 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.8887e-07 - accuracy: 1.0000 - val_loss: 14.2316 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.3777e-04 - accuracy: 1.0000 - val_loss: 14.2435 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.4928e-05 - accuracy: 1.0000 - val_loss: 13.5583 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.2007e-04 - accuracy: 1.0000 - val_loss: 13.1742 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.8075e-07 - accuracy: 1.0000 - val_loss: 12.2424 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.3644e-07 - accuracy: 1.0000 - val_loss: 11.7089 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.4088e-07 - accuracy: 1.0000 - val_loss: 11.1886 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.5876e-06 - accuracy: 1.0000 - val_loss: 10.8942 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.9967e-06 - accuracy: 1.0000 - val_loss: 10.4596 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.1540e-05 - accuracy: 1.0000 - val_loss: 10.0898 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 9.4825e-08 - accuracy: 1.0000 - val_loss: 10.0081 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.8989e-07 - accuracy: 1.0000 - val_loss: 9.8631 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.9802e-08 - accuracy: 1.0000 - val_loss: 9.8093 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 3.1970e-07 - accuracy: 1.0000 - val_loss: 9.7251 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 3.7388e-07 - accuracy: 1.0000 - val_loss: 9.5806 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 6.7461e-07 - accuracy: 1.0000 - val_loss: 9.4852 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0890e-04 - accuracy: 1.0000 - val_loss: 9.6460 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.1081e-06 - accuracy: 1.0000 - val_loss: 9.4020 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.5182e-04 - accuracy: 1.0000 - val_loss: 9.4752 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.4384e-08 - accuracy: 1.0000 - val_loss: 9.4618 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 9.5348 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.5908 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.7490e-04 - accuracy: 1.0000 - val_loss: 9.7065 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.5074 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2909 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2726 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7931e-06 - accuracy: 1.0000 - val_loss: 9.3026 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1273 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1026 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0024e-07 - accuracy: 1.0000 - val_loss: 9.1139 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1670 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.0145 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9424 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 8.9655 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9433 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9245 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9671 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9791 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8804 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7777 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6653 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8902 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0153 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9295 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.8300 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7876 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.6144 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 3.4731e-06 - accuracy: 1.0000 - val_loss: 8.8174 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8093 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7647 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.6120 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 4.5787e-07 - accuracy: 1.0000 - val_loss: 8.8001 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8321 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8810 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7359 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8110 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7339 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8234 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8674 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7895 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6460 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.8965e-08 - accuracy: 1.0000 - val_loss: 8.5151 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7676 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.0530e-05 - accuracy: 1.0000 - val_loss: 8.9580 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8350 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9693 - val_accuracy: 0.6875 - 9s/epoch - 5s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.3547e-08 - accuracy: 1.0000 - val_loss: 8.8560 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7519 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7127 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7903 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8287 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.8558e-06 - accuracy: 1.0000 - val_loss: 9.0365 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0554 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9630 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9653 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8779 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7542 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8986 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9681 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8959 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8613 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8340 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 3.2512e-08 - accuracy: 1.0000 - val_loss: 8.9947 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8548 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9198 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1150 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.1674e-08 - accuracy: 1.0000 - val_loss: 9.1509 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.0877 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0857 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9535 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0496 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1655 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0184 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0595 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0199 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.8965e-08 - accuracy: 1.0000 - val_loss: 8.7150 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.5471 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5566 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5877 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.5136 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4329 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5572 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5789 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6248 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7148 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5713 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.8243e-05 - accuracy: 1.0000 - val_loss: 8.7888 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6289 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5697 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4816 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.1674e-08 - accuracy: 1.0000 - val_loss: 8.3858 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.2311 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.6256e-08 - accuracy: 1.0000 - val_loss: 8.0355 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 7.9159 - val_accuracy: 0.7500 - 8s/epoch - 4s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.0702 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.1583 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.1835 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3623 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4772 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5796 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6740 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7145 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7010 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.5664 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6989 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6751 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.8993 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9435 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8638 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7466 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6676 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7362 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6957 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7325 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7529 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6407 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6157 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5053 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7343 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.5932 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5362 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5821 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.4958 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 9.4555e-05 - accuracy: 1.0000 - val_loss: 8.7158 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7682 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6320 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-08 - accuracy: 1.0000 - val_loss: 8.8310 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8947 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1853 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9329 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9411 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8251 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.6959 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7030 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5961 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.4484 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4800 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6906 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7517 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7304 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7526 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 3.7930e-08 - accuracy: 1.0000 - val_loss: 8.5095 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4647 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4847 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6004 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.7193 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6185 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5995 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5489 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5584 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7258 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7826 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.5461 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6360 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7663 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7166 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.4976 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.9231 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8732 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8057 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9107 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7900 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7256 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4846 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4224 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4187 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4363 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6469 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5497 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.4536 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5227 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7794 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8171 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8289 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7928 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.5438 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7125 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7728 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0039 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0306 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 9.2177 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.3714 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2687 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.1853 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0514 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8101 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7849 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8168 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9166 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 8.1279e-09 - accuracy: 1.0000 - val_loss: 9.0517 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0985 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2701 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.2126 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9590 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6692 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5467 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3182 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.1280 - val_accuracy: 0.7500 - 9s/epoch - 4s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3570 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3575 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4283 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5639 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7421 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7618 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7808 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7804 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8035 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7550 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6544 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5220 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6227 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5433 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5511 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3593 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.0837e-08 - accuracy: 1.0000 - val_loss: 8.5080 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4296 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4512 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5369 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4706 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6046 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4727 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4619 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.4738 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.5712 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6644 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 6.2314e-08 - accuracy: 1.0000 - val_loss: 8.5080 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6817 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6193 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6880 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7285 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8661 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8302 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7311 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 1.8965e-08 - accuracy: 1.0000 - val_loss: 8.9242 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6679 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6933 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7067 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.6168 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7254 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8799 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8450 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8845 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9049 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 5.4186e-09 - accuracy: 1.0000 - val_loss: 8.6136 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.7037 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7865 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7707 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8302 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0004 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9952 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9706 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8857 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 5.23638\n",
      "2/2 - 9s - loss: 2.7093e-09 - accuracy: 1.0000 - val_loss: 8.5555 - val_accuracy: 0.6875 - 9s/epoch - 4s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 5.23638\n",
      "2/2 - 8s - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.3591 - val_accuracy: 0.6875 - 8s/epoch - 4s/step\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "Classification accuracy: 0.520776 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.7226576e-03 9.9227726e-01]\n",
      " [8.9589642e-13 9.9999994e-01]\n",
      " [9.9999994e-01 9.9106887e-14]\n",
      " [9.9999803e-01 1.9031139e-06]\n",
      " [9.9999994e-01 2.5240918e-13]\n",
      " [9.9999994e-01 4.0680857e-13]\n",
      " [1.8764605e-14 9.9999994e-01]\n",
      " [9.9999994e-01 9.1274961e-09]\n",
      " [1.2797143e-12 9.9999994e-01]\n",
      " [9.9999875e-01 1.2368912e-06]\n",
      " [6.0726139e-11 9.9999994e-01]\n",
      " [9.9989623e-01 1.0368826e-04]\n",
      " [9.8513472e-01 1.4865178e-02]\n",
      " [1.5122936e-02 9.8487711e-01]\n",
      " [9.9999994e-01 3.5599435e-15]\n",
      " [9.9999994e-01 2.3553793e-13]\n",
      " [1.0905969e-05 9.9998915e-01]\n",
      " [1.0000000e+00 3.4641903e-12]\n",
      " [1.0000000e+00 1.7420365e-09]]\n",
      "[1 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[9 2]\n",
      " [3 5]]\n",
      "[73.68 75.   71.43]\n"
     ]
    }
   ],
   "source": [
    "'''print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 5.23783, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 4.2665 - accuracy: 0.4091 - val_loss: 5.2378 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 5.23783 to 2.79517, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 3.2022 - accuracy: 0.9545 - val_loss: 2.7952 - val_accuracy: 0.6875 - 14s/epoch - 7s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 2.79517 to 1.80878, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 3.1416 - accuracy: 0.9545 - val_loss: 1.8088 - val_accuracy: 0.6875 - 15s/epoch - 8s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 1.80878 to 1.60418, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 2.8276 - accuracy: 1.0000 - val_loss: 1.6042 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 1.60418 to 1.51392, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 2.6849 - accuracy: 1.0000 - val_loss: 1.5139 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 1.51392 to 1.40123, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 2.7378 - accuracy: 1.0000 - val_loss: 1.4012 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 1.40123 to 1.32824, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 2.4172 - accuracy: 1.0000 - val_loss: 1.3282 - val_accuracy: 0.5000 - 17s/epoch - 9s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 1.32824 to 1.23708, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 2.4498 - accuracy: 1.0000 - val_loss: 1.2371 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 1.23708 to 1.16963, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 2.1601 - accuracy: 1.0000 - val_loss: 1.1696 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 1.16963 to 1.12032, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 2.1551 - accuracy: 1.0000 - val_loss: 1.1203 - val_accuracy: 0.6250 - 21s/epoch - 10s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 1.12032 to 1.07013, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 22s - loss: 1.9177 - accuracy: 1.0000 - val_loss: 1.0701 - val_accuracy: 0.6250 - 22s/epoch - 11s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 1.07013 to 1.03034, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.8447 - accuracy: 1.0000 - val_loss: 1.0303 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 1.03034 to 0.99797, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.7450 - accuracy: 1.0000 - val_loss: 0.9980 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 0.99797 to 0.97314, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.6708 - accuracy: 1.0000 - val_loss: 0.9731 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.97314 to 0.94297, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.5188 - accuracy: 1.0000 - val_loss: 0.9430 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.94297 to 0.93098, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.4531 - accuracy: 1.0000 - val_loss: 0.9310 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 1.4062 - accuracy: 1.0000 - val_loss: 0.9881 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 2.4640 - accuracy: 0.9091 - val_loss: 1.2568 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 4.4231 - accuracy: 0.9318 - val_loss: 1.2585 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 1.9348 - accuracy: 0.8636 - val_loss: 1.0090 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 2.6962 - accuracy: 0.5909 - val_loss: 0.9437 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 1.9068 - accuracy: 0.6591 - val_loss: 0.9411 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 1.5004 - accuracy: 0.9318 - val_loss: 0.9513 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.93098\n",
      "2/2 - 20s - loss: 1.2945 - accuracy: 1.0000 - val_loss: 0.9788 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.93098\n",
      "2/2 - 19s - loss: 1.3231 - accuracy: 0.9773 - val_loss: 0.9495 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.93098\n",
      "2/2 - 18s - loss: 1.4119 - accuracy: 1.0000 - val_loss: 0.9627 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss improved from 0.93098 to 0.91997, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 1.3384 - accuracy: 0.9773 - val_loss: 0.9200 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss improved from 0.91997 to 0.91884, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 1.2699 - accuracy: 0.9773 - val_loss: 0.9188 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.91884\n",
      "2/2 - 19s - loss: 1.5799 - accuracy: 0.9091 - val_loss: 0.9453 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss improved from 0.91884 to 0.90979, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 1.3609 - accuracy: 1.0000 - val_loss: 0.9098 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.90979\n",
      "2/2 - 19s - loss: 1.2034 - accuracy: 1.0000 - val_loss: 0.9353 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.90979\n",
      "2/2 - 19s - loss: 1.2040 - accuracy: 1.0000 - val_loss: 0.9880 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.90979\n",
      "2/2 - 18s - loss: 1.1846 - accuracy: 1.0000 - val_loss: 1.0691 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.90979\n",
      "2/2 - 17s - loss: 1.2767 - accuracy: 1.0000 - val_loss: 1.1240 - val_accuracy: 0.3750 - 17s/epoch - 9s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.90979\n",
      "2/2 - 17s - loss: 1.3385 - accuracy: 0.9773 - val_loss: 1.1212 - val_accuracy: 0.3125 - 17s/epoch - 8s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.90979\n",
      "2/2 - 18s - loss: 1.0870 - accuracy: 1.0000 - val_loss: 1.0518 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.90979\n",
      "2/2 - 17s - loss: 1.0620 - accuracy: 1.0000 - val_loss: 0.9949 - val_accuracy: 0.4375 - 17s/epoch - 9s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.90979\n",
      "2/2 - 18s - loss: 1.1977 - accuracy: 1.0000 - val_loss: 0.9511 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.90979\n",
      "2/2 - 17s - loss: 1.0798 - accuracy: 1.0000 - val_loss: 0.9221 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss improved from 0.90979 to 0.90900, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 1.0355 - accuracy: 1.0000 - val_loss: 0.9090 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss improved from 0.90900 to 0.89722, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 17s - loss: 1.0295 - accuracy: 1.0000 - val_loss: 0.8972 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss improved from 0.89722 to 0.88687, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.9705 - accuracy: 1.0000 - val_loss: 0.8869 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss improved from 0.88687 to 0.87438, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.9670 - accuracy: 1.0000 - val_loss: 0.8744 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss improved from 0.87438 to 0.85930, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.0926 - accuracy: 1.0000 - val_loss: 0.8593 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss improved from 0.85930 to 0.83537, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.9177 - accuracy: 1.0000 - val_loss: 0.8354 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss improved from 0.83537 to 0.82025, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.9563 - accuracy: 1.0000 - val_loss: 0.8202 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.82025\n",
      "2/2 - 18s - loss: 0.8979 - accuracy: 1.0000 - val_loss: 0.8447 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.82025\n",
      "2/2 - 17s - loss: 0.9356 - accuracy: 1.0000 - val_loss: 0.8292 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.82025\n",
      "2/2 - 17s - loss: 0.8738 - accuracy: 1.0000 - val_loss: 0.8424 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.82025\n",
      "2/2 - 17s - loss: 0.9633 - accuracy: 1.0000 - val_loss: 0.8488 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.82025\n",
      "2/2 - 17s - loss: 0.9167 - accuracy: 1.0000 - val_loss: 0.8390 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.82025\n",
      "2/2 - 17s - loss: 0.8571 - accuracy: 1.0000 - val_loss: 0.8291 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.82025\n",
      "2/2 - 18s - loss: 0.8792 - accuracy: 1.0000 - val_loss: 0.8203 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss improved from 0.82025 to 0.80817, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.8336 - accuracy: 1.0000 - val_loss: 0.8082 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss improved from 0.80817 to 0.79078, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.8229 - accuracy: 1.0000 - val_loss: 0.7908 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss improved from 0.79078 to 0.77298, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 0.8451 - accuracy: 1.0000 - val_loss: 0.7730 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss improved from 0.77298 to 0.75701, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.8046 - accuracy: 1.0000 - val_loss: 0.7570 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss improved from 0.75701 to 0.74545, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 21s - loss: 0.8030 - accuracy: 1.0000 - val_loss: 0.7455 - val_accuracy: 0.6250 - 21s/epoch - 10s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.74545\n",
      "2/2 - 19s - loss: 0.8207 - accuracy: 1.0000 - val_loss: 0.7513 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss improved from 0.74545 to 0.73642, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.7856 - accuracy: 1.0000 - val_loss: 0.7364 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss improved from 0.73642 to 0.73061, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.9055 - accuracy: 1.0000 - val_loss: 0.7306 - val_accuracy: 0.6875 - 19s/epoch - 9s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.73061\n",
      "2/2 - 18s - loss: 0.8757 - accuracy: 1.0000 - val_loss: 0.7369 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss improved from 0.73061 to 0.72452, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 0.9086 - accuracy: 1.0000 - val_loss: 0.7245 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7641 - accuracy: 1.0000 - val_loss: 0.8002 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8352 - accuracy: 1.0000 - val_loss: 0.7581 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7555 - accuracy: 1.0000 - val_loss: 0.7920 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8713 - accuracy: 1.0000 - val_loss: 0.7517 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.9293 - accuracy: 1.0000 - val_loss: 1.1519 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 2.2702 - accuracy: 0.7273 - val_loss: 1.2767 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 1.9798 - accuracy: 0.8182 - val_loss: 0.8357 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.9375 - accuracy: 0.9091 - val_loss: 0.7366 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8452 - accuracy: 1.0000 - val_loss: 0.7634 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8536 - accuracy: 0.9773 - val_loss: 0.8383 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8548 - accuracy: 0.9773 - val_loss: 0.8218 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8374 - accuracy: 0.9773 - val_loss: 0.8984 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.9768 - accuracy: 0.9545 - val_loss: 2.3133 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 2.4675 - accuracy: 0.8182 - val_loss: 1.6573 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 3.0931 - accuracy: 0.5909 - val_loss: 1.9124 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 1.5684 - accuracy: 0.7955 - val_loss: 1.3632 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.9945 - accuracy: 0.9545 - val_loss: 1.7021 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.9032 - accuracy: 1.0000 - val_loss: 1.6977 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8982 - accuracy: 1.0000 - val_loss: 1.5929 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8662 - accuracy: 1.0000 - val_loss: 1.5948 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.9557 - accuracy: 1.0000 - val_loss: 1.6043 - val_accuracy: 0.2500 - 19s/epoch - 9s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.8239 - accuracy: 1.0000 - val_loss: 1.7064 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.8544 - accuracy: 1.0000 - val_loss: 1.6552 - val_accuracy: 0.3125 - 18s/epoch - 9s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8310 - accuracy: 1.0000 - val_loss: 1.4572 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8014 - accuracy: 1.0000 - val_loss: 1.4944 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8007 - accuracy: 1.0000 - val_loss: 1.7585 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.9947 - accuracy: 0.9773 - val_loss: 11.8753 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 16.1391 - accuracy: 0.6136 - val_loss: 1.1745 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 1.2848 - accuracy: 0.9318 - val_loss: 1.2453 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 1.0940 - accuracy: 0.9318 - val_loss: 1.4538 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8967 - accuracy: 0.9318 - val_loss: 1.4400 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.9034 - accuracy: 1.0000 - val_loss: 1.5629 - val_accuracy: 0.1875 - 19s/epoch - 10s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8053 - accuracy: 1.0000 - val_loss: 1.6537 - val_accuracy: 0.1875 - 19s/epoch - 10s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8893 - accuracy: 1.0000 - val_loss: 1.6829 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7972 - accuracy: 1.0000 - val_loss: 1.7724 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8360 - accuracy: 1.0000 - val_loss: 1.6306 - val_accuracy: 0.1875 - 19s/epoch - 9s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8118 - accuracy: 1.0000 - val_loss: 1.5800 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7672 - accuracy: 1.0000 - val_loss: 1.5427 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.72452\n",
      "2/2 - 17s - loss: 0.7905 - accuracy: 1.0000 - val_loss: 1.4681 - val_accuracy: 0.2500 - 17s/epoch - 9s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.72452\n",
      "2/2 - 17s - loss: 0.7771 - accuracy: 1.0000 - val_loss: 1.4273 - val_accuracy: 0.2500 - 17s/epoch - 9s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.8247 - accuracy: 1.0000 - val_loss: 1.3770 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7660 - accuracy: 1.0000 - val_loss: 1.4237 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7369 - accuracy: 1.0000 - val_loss: 1.4514 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7814 - accuracy: 1.0000 - val_loss: 1.3727 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.9303 - accuracy: 0.9545 - val_loss: 4.2705 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 4.1806 - accuracy: 0.6818 - val_loss: 2.3110 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 4.3322 - accuracy: 0.6591 - val_loss: 2.7623 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 1.9481 - accuracy: 0.7500 - val_loss: 3.9595 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 2.8917 - accuracy: 0.7727 - val_loss: 2.3651 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 1.4205 - accuracy: 0.7273 - val_loss: 2.1745 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7970 - accuracy: 0.9773 - val_loss: 2.9652 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.9369 - accuracy: 0.9545 - val_loss: 2.3237 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 1.5783 - accuracy: 0.7727 - val_loss: 1.7135 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8291 - accuracy: 1.0000 - val_loss: 2.4473 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7736 - accuracy: 1.0000 - val_loss: 2.5775 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8078 - accuracy: 1.0000 - val_loss: 2.4343 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7884 - accuracy: 1.0000 - val_loss: 2.2927 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8394 - accuracy: 1.0000 - val_loss: 2.2064 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7440 - accuracy: 1.0000 - val_loss: 2.1400 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7422 - accuracy: 1.0000 - val_loss: 2.0854 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7333 - accuracy: 1.0000 - val_loss: 2.0434 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7289 - accuracy: 1.0000 - val_loss: 1.9944 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7777 - accuracy: 1.0000 - val_loss: 1.9558 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7029 - accuracy: 1.0000 - val_loss: 1.9274 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6960 - accuracy: 1.0000 - val_loss: 1.8946 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8518 - accuracy: 0.9773 - val_loss: 5.0433 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 4.4114 - accuracy: 0.7045 - val_loss: 3.0435 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 2.4099 - accuracy: 0.7273 - val_loss: 2.9515 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 3.0660 - accuracy: 0.7273 - val_loss: 2.2939 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 1.5269 - accuracy: 0.8864 - val_loss: 2.7347 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.9547 - accuracy: 0.9545 - val_loss: 1.7109 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8814 - accuracy: 0.9773 - val_loss: 1.7797 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.8351 - accuracy: 1.0000 - val_loss: 1.8767 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7881 - accuracy: 1.0000 - val_loss: 1.8972 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8556 - accuracy: 1.0000 - val_loss: 1.9135 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8598 - accuracy: 1.0000 - val_loss: 1.8999 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7812 - accuracy: 1.0000 - val_loss: 1.8885 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.8243 - accuracy: 1.0000 - val_loss: 1.8679 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7846 - accuracy: 1.0000 - val_loss: 1.8462 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8067 - accuracy: 1.0000 - val_loss: 1.8233 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7640 - accuracy: 1.0000 - val_loss: 1.7935 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7474 - accuracy: 1.0000 - val_loss: 1.7641 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.8151 - accuracy: 1.0000 - val_loss: 1.7386 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.8176 - accuracy: 1.0000 - val_loss: 1.7218 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7129 - accuracy: 1.0000 - val_loss: 1.7006 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7219 - accuracy: 1.0000 - val_loss: 1.6772 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7670 - accuracy: 1.0000 - val_loss: 1.6602 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7986 - accuracy: 1.0000 - val_loss: 1.6408 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.72452\n",
      "2/2 - 23s - loss: 0.7286 - accuracy: 1.0000 - val_loss: 1.6218 - val_accuracy: 0.4375 - 23s/epoch - 11s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7509 - accuracy: 1.0000 - val_loss: 1.6059 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7964 - accuracy: 1.0000 - val_loss: 1.5987 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6706 - accuracy: 1.0000 - val_loss: 1.5837 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6565 - accuracy: 1.0000 - val_loss: 1.5700 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.6609 - accuracy: 1.0000 - val_loss: 1.5495 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.6787 - accuracy: 1.0000 - val_loss: 1.5311 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.6464 - accuracy: 1.0000 - val_loss: 1.5124 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6405 - accuracy: 1.0000 - val_loss: 1.4928 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6256 - accuracy: 1.0000 - val_loss: 1.4742 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6240 - accuracy: 1.0000 - val_loss: 1.4560 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7583 - accuracy: 1.0000 - val_loss: 1.4198 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6441 - accuracy: 1.0000 - val_loss: 1.3843 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6448 - accuracy: 1.0000 - val_loss: 1.3524 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6508 - accuracy: 1.0000 - val_loss: 1.3584 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5953 - accuracy: 1.0000 - val_loss: 1.3946 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5940 - accuracy: 1.0000 - val_loss: 1.4276 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6653 - accuracy: 1.0000 - val_loss: 1.4546 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5879 - accuracy: 1.0000 - val_loss: 1.4727 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5846 - accuracy: 1.0000 - val_loss: 1.4856 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5768 - accuracy: 1.0000 - val_loss: 1.4902 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6113 - accuracy: 1.0000 - val_loss: 1.4732 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6077 - accuracy: 1.0000 - val_loss: 1.4416 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.6319 - accuracy: 1.0000 - val_loss: 1.4084 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.72452\n",
      "2/2 - 17s - loss: 0.5659 - accuracy: 1.0000 - val_loss: 1.3900 - val_accuracy: 0.4375 - 17s/epoch - 9s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5623 - accuracy: 1.0000 - val_loss: 1.4245 - val_accuracy: 0.4375 - 18s/epoch - 9s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5551 - accuracy: 1.0000 - val_loss: 1.5053 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5849 - accuracy: 1.0000 - val_loss: 1.5585 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5391 - accuracy: 1.0000 - val_loss: 1.5346 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7199 - accuracy: 0.9545 - val_loss: 4.5768 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 7.4137 - accuracy: 0.6591 - val_loss: 3.5401 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 3.3766 - accuracy: 0.6136 - val_loss: 3.5406 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7859 - accuracy: 0.9318 - val_loss: 3.4330 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7016 - accuracy: 0.9773 - val_loss: 3.2753 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6569 - accuracy: 0.9773 - val_loss: 3.2013 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5855 - accuracy: 1.0000 - val_loss: 3.2039 - val_accuracy: 0.2500 - 19s/epoch - 10s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6219 - accuracy: 1.0000 - val_loss: 3.1486 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5801 - accuracy: 1.0000 - val_loss: 3.0930 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5765 - accuracy: 1.0000 - val_loss: 2.9474 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5887 - accuracy: 1.0000 - val_loss: 2.7829 - val_accuracy: 0.3750 - 19s/epoch - 9s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6223 - accuracy: 1.0000 - val_loss: 2.6850 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6496 - accuracy: 1.0000 - val_loss: 2.6498 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6092 - accuracy: 1.0000 - val_loss: 2.6568 - val_accuracy: 0.2500 - 20s/epoch - 10s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5936 - accuracy: 1.0000 - val_loss: 2.6905 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5570 - accuracy: 1.0000 - val_loss: 2.6356 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.72452\n",
      "2/2 - 21s - loss: 0.5552 - accuracy: 1.0000 - val_loss: 2.5160 - val_accuracy: 0.3125 - 21s/epoch - 11s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6164 - accuracy: 1.0000 - val_loss: 2.3901 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.72452\n",
      "2/2 - 21s - loss: 0.5575 - accuracy: 1.0000 - val_loss: 2.2845 - val_accuracy: 0.5000 - 21s/epoch - 11s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6354 - accuracy: 1.0000 - val_loss: 2.2167 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5444 - accuracy: 1.0000 - val_loss: 2.1873 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5358 - accuracy: 1.0000 - val_loss: 2.1364 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5994 - accuracy: 1.0000 - val_loss: 2.0577 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5354 - accuracy: 1.0000 - val_loss: 1.9752 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5306 - accuracy: 1.0000 - val_loss: 1.9076 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5287 - accuracy: 1.0000 - val_loss: 1.8591 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5518 - accuracy: 1.0000 - val_loss: 1.8237 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5795 - accuracy: 1.0000 - val_loss: 1.7964 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5469 - accuracy: 1.0000 - val_loss: 1.7664 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6014 - accuracy: 1.0000 - val_loss: 1.6749 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5429 - accuracy: 1.0000 - val_loss: 1.6081 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.6050 - accuracy: 1.0000 - val_loss: 1.6013 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5083 - accuracy: 1.0000 - val_loss: 1.5848 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5050 - accuracy: 1.0000 - val_loss: 1.5289 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5419 - accuracy: 1.0000 - val_loss: 1.4685 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5212 - accuracy: 1.0000 - val_loss: 1.4355 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.72452\n",
      "2/2 - 17s - loss: 0.5275 - accuracy: 1.0000 - val_loss: 1.4351 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5786 - accuracy: 1.0000 - val_loss: 1.4538 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7649 - accuracy: 0.9773 - val_loss: 6.8634 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 12.2518 - accuracy: 0.6818 - val_loss: 2.6435 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 3.2569 - accuracy: 0.5909 - val_loss: 1.5772 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7030 - accuracy: 0.9318 - val_loss: 1.2680 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5712 - accuracy: 1.0000 - val_loss: 1.3808 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5193 - accuracy: 1.0000 - val_loss: 1.5006 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6489 - accuracy: 1.0000 - val_loss: 1.4009 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5847 - accuracy: 1.0000 - val_loss: 1.4480 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5208 - accuracy: 1.0000 - val_loss: 1.4445 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5237 - accuracy: 1.0000 - val_loss: 1.4362 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5466 - accuracy: 1.0000 - val_loss: 1.4402 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5350 - accuracy: 1.0000 - val_loss: 1.4479 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5513 - accuracy: 1.0000 - val_loss: 1.4449 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6653 - accuracy: 0.9318 - val_loss: 3.5018 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 3.1673 - accuracy: 0.7500 - val_loss: 2.1884 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 1.0986 - accuracy: 0.8409 - val_loss: 2.2230 - val_accuracy: 0.6250 - 18s/epoch - 9s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 1.0018 - accuracy: 0.9318 - val_loss: 1.7646 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 1.3804 - accuracy: 0.8409 - val_loss: 1.1661 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.7197 - accuracy: 0.9545 - val_loss: 1.8975 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.8471 - accuracy: 0.8636 - val_loss: 1.3905 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5994 - accuracy: 1.0000 - val_loss: 1.9248 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6079 - accuracy: 1.0000 - val_loss: 1.9171 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6051 - accuracy: 1.0000 - val_loss: 1.8185 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5325 - accuracy: 1.0000 - val_loss: 1.7868 - val_accuracy: 0.3750 - 19s/epoch - 9s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5648 - accuracy: 1.0000 - val_loss: 1.7721 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5292 - accuracy: 1.0000 - val_loss: 1.7582 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5298 - accuracy: 1.0000 - val_loss: 1.7457 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5228 - accuracy: 1.0000 - val_loss: 1.7345 - val_accuracy: 0.3750 - 18s/epoch - 9s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5191 - accuracy: 1.0000 - val_loss: 1.7211 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5857 - accuracy: 1.0000 - val_loss: 1.6969 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5509 - accuracy: 1.0000 - val_loss: 1.6746 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5595 - accuracy: 1.0000 - val_loss: 1.6556 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5899 - accuracy: 1.0000 - val_loss: 1.6413 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4896 - accuracy: 1.0000 - val_loss: 1.6240 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4987 - accuracy: 1.0000 - val_loss: 1.6108 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4918 - accuracy: 1.0000 - val_loss: 1.5979 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5065 - accuracy: 1.0000 - val_loss: 1.5791 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4768 - accuracy: 1.0000 - val_loss: 1.5675 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5061 - accuracy: 1.0000 - val_loss: 1.5538 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5246 - accuracy: 1.0000 - val_loss: 1.5389 - val_accuracy: 0.5000 - 19s/epoch - 9s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5636 - accuracy: 1.0000 - val_loss: 1.5277 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.5146 - accuracy: 0.9773 - val_loss: 1.7573 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.72452\n",
      "2/2 - 25s - loss: 0.4965 - accuracy: 1.0000 - val_loss: 2.1204 - val_accuracy: 0.6250 - 25s/epoch - 13s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.72452\n",
      "2/2 - 26s - loss: 0.7748 - accuracy: 0.8864 - val_loss: 1.1624 - val_accuracy: 0.5625 - 26s/epoch - 13s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.72452\n",
      "2/2 - 24s - loss: 0.5288 - accuracy: 1.0000 - val_loss: 2.1222 - val_accuracy: 0.5625 - 24s/epoch - 12s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.5034 - accuracy: 1.0000 - val_loss: 2.3746 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.7626 - accuracy: 0.8636 - val_loss: 1.4065 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6417 - accuracy: 0.9318 - val_loss: 1.2641 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.4696 - accuracy: 1.0000 - val_loss: 0.8896 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4964 - accuracy: 1.0000 - val_loss: 0.9081 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4679 - accuracy: 1.0000 - val_loss: 0.9867 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4848 - accuracy: 1.0000 - val_loss: 1.0249 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4543 - accuracy: 1.0000 - val_loss: 1.0267 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5345 - accuracy: 0.9773 - val_loss: 1.0329 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4651 - accuracy: 1.0000 - val_loss: 1.1697 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5155 - accuracy: 0.9773 - val_loss: 1.3222 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.6394 - accuracy: 0.9773 - val_loss: 1.8292 - val_accuracy: 0.4375 - 19s/epoch - 9s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.9351 - accuracy: 0.9318 - val_loss: 1.4557 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.7105 - accuracy: 0.9545 - val_loss: 1.3165 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5696 - accuracy: 1.0000 - val_loss: 1.4585 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4760 - accuracy: 1.0000 - val_loss: 1.4556 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4633 - accuracy: 1.0000 - val_loss: 1.4501 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4365 - accuracy: 1.0000 - val_loss: 1.4391 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.4298 - accuracy: 1.0000 - val_loss: 1.4252 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4575 - accuracy: 1.0000 - val_loss: 1.4120 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.5083 - accuracy: 1.0000 - val_loss: 1.4006 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4233 - accuracy: 1.0000 - val_loss: 1.3842 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4438 - accuracy: 1.0000 - val_loss: 1.3707 - val_accuracy: 0.5000 - 18s/epoch - 9s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4818 - accuracy: 1.0000 - val_loss: 1.3531 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4465 - accuracy: 1.0000 - val_loss: 1.3279 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.72452\n",
      "2/2 - 17s - loss: 0.4204 - accuracy: 1.0000 - val_loss: 1.3143 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.72452\n",
      "2/2 - 17s - loss: 0.4386 - accuracy: 1.0000 - val_loss: 1.2995 - val_accuracy: 0.5625 - 17s/epoch - 9s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4505 - accuracy: 1.0000 - val_loss: 1.2909 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.72452\n",
      "2/2 - 20s - loss: 0.4535 - accuracy: 1.0000 - val_loss: 1.2822 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4453 - accuracy: 1.0000 - val_loss: 1.2714 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4513 - accuracy: 1.0000 - val_loss: 1.2626 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.4221 - accuracy: 1.0000 - val_loss: 1.2538 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.3982 - accuracy: 1.0000 - val_loss: 1.2442 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.72452\n",
      "2/2 - 18s - loss: 0.3919 - accuracy: 1.0000 - val_loss: 1.2341 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4268 - accuracy: 1.0000 - val_loss: 1.2237 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.3963 - accuracy: 1.0000 - val_loss: 1.2140 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.72452\n",
      "2/2 - 19s - loss: 0.4307 - accuracy: 1.0000 - val_loss: 1.2002 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Classification accuracy: 0.537396 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGLEEGNet = EEGNet_TSGLEEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4615923  0.5384077 ]\n",
      " [0.50386655 0.49613342]\n",
      " [0.5275223  0.47247747]\n",
      " [0.5807     0.41929984]\n",
      " [0.7991415  0.2008584 ]\n",
      " [0.7148916  0.28510842]\n",
      " [0.50698924 0.49301076]\n",
      " [0.4584877  0.54151237]\n",
      " [0.12931395 0.8706862 ]\n",
      " [0.5184885  0.48151147]\n",
      " [0.5175963  0.48240378]\n",
      " [0.52264977 0.4773499 ]\n",
      " [0.506836   0.49316385]\n",
      " [0.5407657  0.45923415]\n",
      " [0.4541824  0.54581803]\n",
      " [0.6069303  0.39306998]\n",
      " [0.4972432  0.5027569 ]\n",
      " [0.5781056  0.42189437]\n",
      " [0.59960777 0.40039232]]\n",
      "[1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[9 2]\n",
      " [5 3]]\n",
      "[63.16 64.29 60.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGLEEGNet)\n",
    "preds_Shallow = probs_TSGLEEGNet.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "probs_Shallow_init_sigmoid = np.array([[6.95184946e-01 3.04815054e-01]\n",
    " [4.21892613e-01 5.78107417e-01]\n",
    " [7.89708734e-01 2.10291266e-01]\n",
    " [9.99535143e-01 4.64851793e-04]\n",
    " [1.59405246e-01 8.40594828e-01]\n",
    " [9.02964830e-01 9.70351920e-02]\n",
    " [1.18944913e-01 8.81055117e-01]\n",
    " [1.71721101e-01 8.28278899e-01]\n",
    " [9.67139781e-01 3.28601785e-02]\n",
    " [7.46518612e-01 2.53481418e-01]\n",
    " [2.98798531e-01 7.01201379e-01]\n",
    " [1.78000614e-10 9.99999940e-01]\n",
    " [8.16536665e-01 1.83463335e-01]\n",
    " [2.43162528e-01 7.56837428e-01]\n",
    " [1.04919985e-01 8.95080090e-01]\n",
    " [9.93469775e-01 6.53020665e-03]\n",
    " [9.06261265e-01 9.37386826e-02]\n",
    " [6.60291553e-01 3.39708477e-01]\n",
    " [7.47849047e-01 2.52150923e-01]]\n",
    "[0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0]\n",
    "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]])"
=======
    "probs_TSGLEEGNet_init_sigmoid = [[0.4615923  0.5384077 ]\n",
    " [0.50386655 0.49613342]\n",
    " [0.5275223  0.47247747]\n",
    " [0.5807     0.41929984]\n",
    " [0.7991415  0.2008584 ]\n",
    " [0.7148916  0.28510842]\n",
    " [0.50698924 0.49301076]\n",
    " [0.4584877  0.54151237]\n",
    " [0.12931395 0.8706862 ]\n",
    " [0.5184885  0.48151147]\n",
    " [0.5175963  0.48240378]\n",
    " [0.52264977 0.4773499 ]\n",
    " [0.506836   0.49316385]\n",
    " [0.5407657  0.45923415]\n",
    " [0.4541824  0.54581803]\n",
    " [0.6069303  0.39306998]\n",
    " [0.4972432  0.5027569 ]\n",
    " [0.5781056  0.42189437]\n",
    " [0.59960777 0.40039232]]"
>>>>>>> Stashed changes
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
