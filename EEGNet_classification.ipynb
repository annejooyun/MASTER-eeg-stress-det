{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.data import extract_eeg_data, multi_to_binary_classification, split_dataset, dict_to_arr\n",
    "from utils.labels import get_stai_labels\n",
    "from utils.valid_recs import get_valid_recs\n",
    "from utils.metrics import compute_metrics\n",
    "\n",
    "from classifiers import EEGNet_classification, EEGNet_SSVEP_classification, EEGNet_TSGL_classification, EEGNet_DeepConvNet_classification, EEGNet_ShallowConvNet_classification\n",
    "import utils.variables as v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P010_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P013_S001_002\n",
      "ERROR:root:1) Failed to read data for recording P020_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P023_S002_002\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n"
     ]
    }
   ],
   "source": [
    "valid_recs = get_valid_recs(data_type='init', output_type = 'np')\n",
    "print(f'Valid recs {valid_recs}')\n",
    "\n",
    "x_dict_ = extract_eeg_data(valid_recs, data_type='init', output_type='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "P006_S001_002 has invalid value for label\n",
      "P006_S001_002 has invalid value for label\n",
      "P010_S001_001 has invalid record length\n",
      "P013_S001_001 has invalid record length\n",
      "P013_S001_002 has invalid record length\n",
      "P020_S001_001 has invalid record length\n",
      "P023_S002_002 has invalid record length\n",
      "P027_S002_002 has invalid value for label\n",
      "P027_S002_002 has invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n"
     ]
    }
   ],
   "source": [
    "y_dict_ = get_stai_labels(valid_recs) \n",
    "#y_dict = get_pss_labels(valid_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing invalid labels: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing invalid labels: {len(y_dict_)}\")\n",
    "\n",
    "print(y_dict_['P007_S001_002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n"
     ]
    }
   ],
   "source": [
    "x_dict, y_dict = multi_to_binary_classification(x_dict_, y_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing mildly stressed subjects: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing  mildly stressed subjects: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict, test_data_dict, val_data_dict, train_labels_dict, test_labels_dict, val_labels_dict = split_dataset(x_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data set: 44\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train data set: {len(train_data_dict)}\")\n",
    "print(f\"Length of validation data set: {len(val_data_dict)}\")\n",
    "print(f\"Length of test data set: {len(test_data_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (44, 8, 75000)\n",
      "Shape of validation data set: (16, 8, 75000)\n",
      "Shape of test data set: (19, 8, 75000)\n",
      "Shape of train labels set: (44, 1)\n",
      "Shape of validation labels set: (16, 1)\n",
      "Shape of test labels set: (19, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = dict_to_arr(train_data_dict)\n",
    "test_data = dict_to_arr(test_data_dict)\n",
    "val_data = dict_to_arr(val_data_dict)\n",
    "\n",
    "train_labels = np.reshape(np.array(list(train_labels_dict.values())), (len(train_data),1))\n",
    "test_labels = np.reshape(np.array(list(test_labels_dict.values())), (len(test_data),1))\n",
    "val_labels = np.reshape(np.array(list(val_labels_dict.values())), (len(val_data),1))\n",
    "\n",
    "print(f\"Shape of train data set: {train_data.shape}\")\n",
    "print(f\"Shape of validation data set: {val_data.shape}\")\n",
    "print(f\"Shape of test data set: {test_data.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Shape of train labels set: {train_labels.shape}\")\n",
    "print(f\"Shape of validation labels set: {val_labels.shape}\")\n",
    "print(f\"Shape of test labels set: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_EEGNet = EEGNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[8 3]\n",
      " [7 1]]\n",
      "[47.37 53.33 25.  ]\n"
     ]
    }
   ],
   "source": [
    "# with init_filter data, 300 epochs\n",
    "probs_EEGNet_init = np.array([\n",
    "                  [0.7138277, 0.28617287],\n",
    "                  [0.5637057, 0.43629473],\n",
    "                  [0.6218500, 0.37815124],\n",
    "                  [0.7166857, 0.28331438],\n",
    "                  [0.6754987, 0.32450426],\n",
    "                  [0.8090515, 0.19095036],\n",
    "                  [0.5339635, 0.46603800],\n",
    "                  [0.2460488, 0.75395140],\n",
    "                  [0.4467932, 0.55320940],\n",
    "                  [0.6290466, 0.37095330],\n",
    "                  [0.5762418, 0.42375806],\n",
    "                  [0.1861840, 0.81381420],\n",
    "                  [0.6687245, 0.33127743],\n",
    "                  [0.7082854, 0.29171503],\n",
    "                  [0.5183024, 0.48169836],\n",
    "                  [0.7217397, 0.27826140],\n",
    "                  [0.6201925, 0.37980822],\n",
    "                  [0.6769989, 0.32300153],\n",
    "                  [0.4880893, 0.51191190]])\n",
    "\n",
    "\n",
    "preds_EEGNet = probs_EEGNet_init.argmax(axis = -1)  \n",
    "print(preds_EEGNet)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_EEGNet = compute_metrics(test_labels, preds_EEGNet)\n",
    "print(performance_EEGNet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_SSVEP = EEGNet_SSVEP_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(probs_SSVEP)\\npreds_SSVEP = probs_SSVEP.argmax(axis = -1)  \\nprint(preds_SSVEP)\\nprint(test_labels.T)\\n\\nperformance_SSVEP = compute_metrics(test_labels, preds_SSVEP)\\nprint(performance_SSVEP)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(probs_SSVEP)\n",
    "preds_SSVEP = probs_SSVEP.argmax(axis = -1)  \n",
    "print(preds_SSVEP)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_SSVEP = compute_metrics(test_labels, preds_SSVEP)\n",
    "print(performance_SSVEP)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 6.27307, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 23s - loss: 4.0190 - accuracy: 0.4773 - val_loss: 6.2731 - val_accuracy: 0.6875 - 23s/epoch - 12s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 6.27307 to 2.36650, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 3.7352 - accuracy: 0.7955 - val_loss: 2.3665 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 2.36650 to 1.98935, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.9717 - accuracy: 1.0000 - val_loss: 1.9894 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 1.98935 to 1.86849, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.8628 - accuracy: 1.0000 - val_loss: 1.8685 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 1.86849 to 1.71422, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.6239 - accuracy: 1.0000 - val_loss: 1.7142 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 1.71422 to 1.58675, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.5761 - accuracy: 1.0000 - val_loss: 1.5867 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss improved from 1.58675 to 1.42664, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 2.4057 - accuracy: 1.0000 - val_loss: 1.4266 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss improved from 1.42664 to 1.30816, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 2.3062 - accuracy: 1.0000 - val_loss: 1.3082 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 1.30816 to 1.21788, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.2305 - accuracy: 1.0000 - val_loss: 1.2179 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 1.21788 to 1.14220, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 2.0308 - accuracy: 1.0000 - val_loss: 1.1422 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss improved from 1.14220 to 1.08146, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.9791 - accuracy: 1.0000 - val_loss: 1.0815 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss improved from 1.08146 to 1.05300, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.9983 - accuracy: 1.0000 - val_loss: 1.0530 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss improved from 1.05300 to 1.02180, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.8117 - accuracy: 1.0000 - val_loss: 1.0218 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss improved from 1.02180 to 0.98353, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.7074 - accuracy: 1.0000 - val_loss: 0.9835 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss improved from 0.98353 to 0.95233, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.5503 - accuracy: 1.0000 - val_loss: 0.9523 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss improved from 0.95233 to 0.92529, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.7369 - accuracy: 1.0000 - val_loss: 0.9253 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss improved from 0.92529 to 0.90449, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 18s - loss: 1.3877 - accuracy: 1.0000 - val_loss: 0.9045 - val_accuracy: 0.6875 - 18s/epoch - 9s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss improved from 0.90449 to 0.88787, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.3181 - accuracy: 1.0000 - val_loss: 0.8879 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.2535 - accuracy: 1.0000 - val_loss: 1.0483 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.9340 - accuracy: 0.7955 - val_loss: 1.0947 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 2.2689 - accuracy: 0.7273 - val_loss: 1.3368 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.8528 - accuracy: 0.8636 - val_loss: 0.8907 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 2.4674 - accuracy: 0.7955 - val_loss: 2.4543 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 3.8694 - accuracy: 0.7273 - val_loss: 3.1393 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 5.3905 - accuracy: 0.5455 - val_loss: 1.6290 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4653 - accuracy: 0.9318 - val_loss: 1.2138 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.8584 - accuracy: 0.8182 - val_loss: 1.3382 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4604 - accuracy: 1.0000 - val_loss: 1.5068 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.5651 - accuracy: 0.9545 - val_loss: 1.2809 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4737 - accuracy: 1.0000 - val_loss: 1.3788 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.88787\n",
      "2/2 - 21s - loss: 1.4356 - accuracy: 1.0000 - val_loss: 1.4939 - val_accuracy: 0.4375 - 21s/epoch - 10s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.4822 - accuracy: 1.0000 - val_loss: 1.3995 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.5076 - accuracy: 1.0000 - val_loss: 1.3228 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.5063 - accuracy: 1.0000 - val_loss: 1.2688 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3816 - accuracy: 1.0000 - val_loss: 1.2280 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.88787\n",
      "2/2 - 19s - loss: 1.4742 - accuracy: 1.0000 - val_loss: 1.1939 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3909 - accuracy: 1.0000 - val_loss: 1.1628 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3037 - accuracy: 1.0000 - val_loss: 1.1349 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2763 - accuracy: 1.0000 - val_loss: 1.1073 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.3576 - accuracy: 1.0000 - val_loss: 1.0829 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2239 - accuracy: 1.0000 - val_loss: 1.0578 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.1845 - accuracy: 1.0000 - val_loss: 1.0315 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2561 - accuracy: 1.0000 - val_loss: 1.0041 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2256 - accuracy: 1.0000 - val_loss: 0.9873 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.1168 - accuracy: 1.0000 - val_loss: 0.9688 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.2218 - accuracy: 1.0000 - val_loss: 0.9454 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.1900 - accuracy: 1.0000 - val_loss: 0.9130 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.88787\n",
      "2/2 - 20s - loss: 1.0534 - accuracy: 1.0000 - val_loss: 0.8922 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss improved from 0.88787 to 0.88268, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.1142 - accuracy: 1.0000 - val_loss: 0.8827 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss improved from 0.88268 to 0.86842, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.0866 - accuracy: 1.0000 - val_loss: 0.8684 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss improved from 0.86842 to 0.84958, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 1.0811 - accuracy: 1.0000 - val_loss: 0.8496 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss improved from 0.84958 to 0.83640, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.9852 - accuracy: 1.0000 - val_loss: 0.8364 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss improved from 0.83640 to 0.82864, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 19s - loss: 0.9599 - accuracy: 1.0000 - val_loss: 0.8286 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0331 - accuracy: 1.0000 - val_loss: 1.0365 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0016 - accuracy: 1.0000 - val_loss: 1.3143 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 3.1522 - accuracy: 0.8409 - val_loss: 1.0322 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 2.1564 - accuracy: 0.7727 - val_loss: 2.0707 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 2.5873 - accuracy: 0.6818 - val_loss: 0.9352 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1899 - accuracy: 0.9545 - val_loss: 1.0048 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1092 - accuracy: 0.9773 - val_loss: 0.9831 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0651 - accuracy: 1.0000 - val_loss: 1.1015 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 2.2930 - accuracy: 0.8182 - val_loss: 2.4593 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 3.6294 - accuracy: 0.7500 - val_loss: 1.3839 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.2361 - accuracy: 0.9545 - val_loss: 1.6808 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.8863 - accuracy: 0.8636 - val_loss: 1.7575 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1535 - accuracy: 0.9545 - val_loss: 1.9198 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0773 - accuracy: 1.0000 - val_loss: 1.9910 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1413 - accuracy: 0.9773 - val_loss: 1.9298 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0724 - accuracy: 1.0000 - val_loss: 1.8270 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1709 - accuracy: 1.0000 - val_loss: 1.7699 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1427 - accuracy: 1.0000 - val_loss: 1.6574 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0471 - accuracy: 1.0000 - val_loss: 1.5369 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.1450 - accuracy: 1.0000 - val_loss: 1.4823 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0628 - accuracy: 1.0000 - val_loss: 1.4428 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0106 - accuracy: 1.0000 - val_loss: 1.4095 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0384 - accuracy: 1.0000 - val_loss: 1.3762 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9741 - accuracy: 1.0000 - val_loss: 1.3473 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9697 - accuracy: 1.0000 - val_loss: 1.3192 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 1.0131 - accuracy: 1.0000 - val_loss: 1.2920 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 1.0093 - accuracy: 1.0000 - val_loss: 1.2716 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9805 - accuracy: 1.0000 - val_loss: 1.2463 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9536 - accuracy: 1.0000 - val_loss: 1.2223 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9038 - accuracy: 1.0000 - val_loss: 1.2020 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8922 - accuracy: 1.0000 - val_loss: 1.1807 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8723 - accuracy: 1.0000 - val_loss: 1.1667 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9936 - accuracy: 1.0000 - val_loss: 1.1575 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.9077 - accuracy: 1.0000 - val_loss: 1.1391 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8402 - accuracy: 1.0000 - val_loss: 1.1148 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8226 - accuracy: 1.0000 - val_loss: 1.0878 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8075 - accuracy: 1.0000 - val_loss: 1.0632 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8791 - accuracy: 1.0000 - val_loss: 1.0434 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8540 - accuracy: 1.0000 - val_loss: 1.0278 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7821 - accuracy: 1.0000 - val_loss: 1.0174 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.9284 - accuracy: 1.0000 - val_loss: 1.0116 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7574 - accuracy: 1.0000 - val_loss: 1.0479 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8464 - accuracy: 1.0000 - val_loss: 1.0869 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.8876 - accuracy: 1.0000 - val_loss: 1.0938 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7313 - accuracy: 1.0000 - val_loss: 1.0387 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7259 - accuracy: 1.0000 - val_loss: 0.9936 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7193 - accuracy: 1.0000 - val_loss: 0.9701 - val_accuracy: 0.5625 - 19s/epoch - 9s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7079 - accuracy: 1.0000 - val_loss: 0.9650 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7749 - accuracy: 1.0000 - val_loss: 0.9666 - val_accuracy: 0.5000 - 19s/epoch - 10s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7417 - accuracy: 1.0000 - val_loss: 0.9674 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6793 - accuracy: 1.0000 - val_loss: 0.9622 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7452 - accuracy: 1.0000 - val_loss: 0.9526 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7803 - accuracy: 1.0000 - val_loss: 0.9420 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7051 - accuracy: 1.0000 - val_loss: 0.9269 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.7380 - accuracy: 1.0000 - val_loss: 0.9149 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7375 - accuracy: 1.0000 - val_loss: 0.9078 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.6659 - accuracy: 1.0000 - val_loss: 0.9037 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6917 - accuracy: 1.0000 - val_loss: 0.8993 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6782 - accuracy: 1.0000 - val_loss: 0.8919 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.7630 - accuracy: 1.0000 - val_loss: 0.8900 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6120 - accuracy: 1.0000 - val_loss: 0.8745 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6875 - accuracy: 1.0000 - val_loss: 0.8740 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.82864\n",
      "2/2 - 20s - loss: 0.6350 - accuracy: 1.0000 - val_loss: 0.9020 - val_accuracy: 0.3125 - 20s/epoch - 10s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.6339 - accuracy: 1.0000 - val_loss: 0.8853 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.82864\n",
      "2/2 - 19s - loss: 0.6264 - accuracy: 1.0000 - val_loss: 0.8511 - val_accuracy: 0.3125 - 19s/epoch - 10s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss improved from 0.82864 to 0.82616, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.6650 - accuracy: 1.0000 - val_loss: 0.8262 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.82616\n",
      "2/2 - 20s - loss: 0.5740 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss improved from 0.82616 to 0.81672, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 20s - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.8167 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5720 - accuracy: 1.0000 - val_loss: 1.2585 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6166 - accuracy: 0.9773 - val_loss: 1.2214 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6112 - accuracy: 0.9773 - val_loss: 1.7733 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.3359 - accuracy: 0.8636 - val_loss: 2.2089 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.2774 - accuracy: 0.7727 - val_loss: 1.3418 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.9889 - accuracy: 0.7955 - val_loss: 1.3810 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.1751 - accuracy: 0.8409 - val_loss: 1.7032 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 1.5100 - accuracy: 0.8182 - val_loss: 1.5636 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7769 - accuracy: 0.9773 - val_loss: 1.3042 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7883 - accuracy: 1.0000 - val_loss: 1.2864 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7236 - accuracy: 1.0000 - val_loss: 1.3165 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7570 - accuracy: 1.0000 - val_loss: 1.3463 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7279 - accuracy: 1.0000 - val_loss: 1.3728 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7318 - accuracy: 1.0000 - val_loss: 1.3846 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7657 - accuracy: 1.0000 - val_loss: 1.3922 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.7992 - accuracy: 1.0000 - val_loss: 1.3968 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7519 - accuracy: 1.0000 - val_loss: 1.3938 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6983 - accuracy: 1.0000 - val_loss: 1.3849 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7421 - accuracy: 1.0000 - val_loss: 1.3691 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7473 - accuracy: 1.0000 - val_loss: 1.3577 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.8124 - accuracy: 1.0000 - val_loss: 1.3419 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6910 - accuracy: 1.0000 - val_loss: 1.3360 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6996 - accuracy: 1.0000 - val_loss: 1.3263 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6897 - accuracy: 1.0000 - val_loss: 1.3098 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7044 - accuracy: 1.0000 - val_loss: 1.2972 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6286 - accuracy: 1.0000 - val_loss: 1.2813 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6211 - accuracy: 1.0000 - val_loss: 1.2622 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6120 - accuracy: 1.0000 - val_loss: 1.2443 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6666 - accuracy: 1.0000 - val_loss: 1.2257 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5957 - accuracy: 1.0000 - val_loss: 1.2112 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6249 - accuracy: 1.0000 - val_loss: 1.1962 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6909 - accuracy: 1.0000 - val_loss: 1.1839 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.7134 - accuracy: 1.0000 - val_loss: 1.1725 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5820 - accuracy: 1.0000 - val_loss: 1.1622 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6803 - accuracy: 1.0000 - val_loss: 1.1629 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6031 - accuracy: 1.0000 - val_loss: 1.1700 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6803 - accuracy: 1.0000 - val_loss: 1.1600 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.5840 - accuracy: 1.0000 - val_loss: 1.1387 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5437 - accuracy: 1.0000 - val_loss: 1.1252 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5717 - accuracy: 1.0000 - val_loss: 1.1111 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5364 - accuracy: 1.0000 - val_loss: 1.1035 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.5258 - accuracy: 1.0000 - val_loss: 1.0966 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5608 - accuracy: 1.0000 - val_loss: 1.0885 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5676 - accuracy: 1.0000 - val_loss: 1.0817 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5233 - accuracy: 1.0000 - val_loss: 1.0762 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5121 - accuracy: 1.0000 - val_loss: 1.0806 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5040 - accuracy: 1.0000 - val_loss: 1.0849 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5289 - accuracy: 1.0000 - val_loss: 1.0829 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.5306 - accuracy: 1.0000 - val_loss: 1.0787 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5467 - accuracy: 1.0000 - val_loss: 1.0709 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5448 - accuracy: 1.0000 - val_loss: 1.0631 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4916 - accuracy: 1.0000 - val_loss: 1.0525 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5509 - accuracy: 1.0000 - val_loss: 1.0414 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4842 - accuracy: 1.0000 - val_loss: 1.0310 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.6044 - accuracy: 1.0000 - val_loss: 1.0225 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5225 - accuracy: 1.0000 - val_loss: 1.0146 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4739 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5132 - accuracy: 1.0000 - val_loss: 1.0035 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5114 - accuracy: 1.0000 - val_loss: 0.9993 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4610 - accuracy: 1.0000 - val_loss: 0.9958 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4979 - accuracy: 1.0000 - val_loss: 0.9937 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4943 - accuracy: 1.0000 - val_loss: 0.9918 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4510 - accuracy: 1.0000 - val_loss: 0.9906 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4914 - accuracy: 1.0000 - val_loss: 0.9892 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4892 - accuracy: 1.0000 - val_loss: 0.9855 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4749 - accuracy: 1.0000 - val_loss: 0.9794 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4385 - accuracy: 1.0000 - val_loss: 0.9736 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4364 - accuracy: 1.0000 - val_loss: 0.9622 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4292 - accuracy: 1.0000 - val_loss: 0.9497 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4698 - accuracy: 1.0000 - val_loss: 0.9396 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4288 - accuracy: 1.0000 - val_loss: 0.9350 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.5168 - accuracy: 1.0000 - val_loss: 0.9343 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4195 - accuracy: 1.0000 - val_loss: 0.9319 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4166 - accuracy: 1.0000 - val_loss: 0.9280 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4456 - accuracy: 1.0000 - val_loss: 0.9244 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4554 - accuracy: 1.0000 - val_loss: 0.9219 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4044 - accuracy: 1.0000 - val_loss: 0.9189 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4060 - accuracy: 1.0000 - val_loss: 0.9118 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4017 - accuracy: 1.0000 - val_loss: 0.9043 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4382 - accuracy: 1.0000 - val_loss: 0.8973 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4358 - accuracy: 1.0000 - val_loss: 0.8955 - val_accuracy: 0.6875 - 21s/epoch - 10s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.3948 - accuracy: 1.0000 - val_loss: 0.9016 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.81672\n",
      "2/2 - 22s - loss: 0.4316 - accuracy: 1.0000 - val_loss: 0.9126 - val_accuracy: 0.6875 - 22s/epoch - 11s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4404 - accuracy: 1.0000 - val_loss: 0.9250 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4217 - accuracy: 1.0000 - val_loss: 0.9313 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4286 - accuracy: 1.0000 - val_loss: 0.9301 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3781 - accuracy: 1.0000 - val_loss: 0.9191 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4036 - accuracy: 1.0000 - val_loss: 0.8982 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3966 - accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3721 - accuracy: 1.0000 - val_loss: 0.8676 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4573 - accuracy: 1.0000 - val_loss: 0.8598 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3656 - accuracy: 1.0000 - val_loss: 0.8558 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3654 - accuracy: 1.0000 - val_loss: 0.8540 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3605 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3580 - accuracy: 1.0000 - val_loss: 0.8847 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3849 - accuracy: 1.0000 - val_loss: 0.8989 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3565 - accuracy: 1.0000 - val_loss: 0.9004 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3863 - accuracy: 1.0000 - val_loss: 0.8834 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4434 - accuracy: 1.0000 - val_loss: 5.4565 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 2.4289 - accuracy: 0.7500 - val_loss: 2.3055 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 1.5117 - accuracy: 0.7500 - val_loss: 1.0511 - val_accuracy: 0.3750 - 19s/epoch - 10s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4389 - accuracy: 0.9318 - val_loss: 1.5448 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.6890 - accuracy: 0.9091 - val_loss: 2.6497 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 2.2237 - accuracy: 0.7727 - val_loss: 2.0479 - val_accuracy: 0.4375 - 20s/epoch - 10s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4655 - accuracy: 1.0000 - val_loss: 1.9882 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4712 - accuracy: 0.9773 - val_loss: 1.5476 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4398 - accuracy: 1.0000 - val_loss: 1.1877 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4305 - accuracy: 1.0000 - val_loss: 1.1122 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4149 - accuracy: 1.0000 - val_loss: 1.1378 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4063 - accuracy: 1.0000 - val_loss: 1.1841 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4395 - accuracy: 1.0000 - val_loss: 1.4746 - val_accuracy: 0.4375 - 19s/epoch - 10s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4805 - accuracy: 1.0000 - val_loss: 1.3403 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4093 - accuracy: 1.0000 - val_loss: 1.3806 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4042 - accuracy: 1.0000 - val_loss: 1.4454 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4806 - accuracy: 1.0000 - val_loss: 1.4847 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4361 - accuracy: 1.0000 - val_loss: 1.5081 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3850 - accuracy: 1.0000 - val_loss: 1.5062 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4204 - accuracy: 1.0000 - val_loss: 1.4936 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4135 - accuracy: 1.0000 - val_loss: 1.4758 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4045 - accuracy: 1.0000 - val_loss: 1.4577 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4008 - accuracy: 1.0000 - val_loss: 1.4308 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3854 - accuracy: 1.0000 - val_loss: 1.4042 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3622 - accuracy: 1.0000 - val_loss: 1.3753 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4079 - accuracy: 1.0000 - val_loss: 1.3525 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4273 - accuracy: 1.0000 - val_loss: 1.3265 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3491 - accuracy: 1.0000 - val_loss: 1.3020 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3795 - accuracy: 1.0000 - val_loss: 1.2804 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3770 - accuracy: 1.0000 - val_loss: 1.2505 - val_accuracy: 0.5625 - 20s/epoch - 10s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3695 - accuracy: 0.9773 - val_loss: 1.6178 - val_accuracy: 0.3750 - 20s/epoch - 10s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 1.9319 - accuracy: 0.8636 - val_loss: 2.6787 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 2.7819 - accuracy: 0.7045 - val_loss: 1.3174 - val_accuracy: 0.5625 - 19s/epoch - 10s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.1772 - accuracy: 0.7273 - val_loss: 1.5650 - val_accuracy: 0.5000 - 20s/epoch - 10s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 1.0724 - accuracy: 0.8636 - val_loss: 1.1737 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4415 - accuracy: 0.9545 - val_loss: 1.5253 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4122 - accuracy: 1.0000 - val_loss: 1.7718 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3823 - accuracy: 1.0000 - val_loss: 1.8601 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3999 - accuracy: 1.0000 - val_loss: 1.8770 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4154 - accuracy: 1.0000 - val_loss: 1.8678 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4462 - accuracy: 1.0000 - val_loss: 1.8583 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4148 - accuracy: 1.0000 - val_loss: 1.8322 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.4454 - accuracy: 1.0000 - val_loss: 1.8115 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3863 - accuracy: 1.0000 - val_loss: 1.7807 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4161 - accuracy: 1.0000 - val_loss: 1.7550 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.4047 - accuracy: 1.0000 - val_loss: 1.7438 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4044 - accuracy: 1.0000 - val_loss: 1.7490 - val_accuracy: 0.7500 - 21s/epoch - 10s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.81672\n",
      "2/2 - 21s - loss: 0.4033 - accuracy: 1.0000 - val_loss: 1.7299 - val_accuracy: 0.6875 - 21s/epoch - 11s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3924 - accuracy: 1.0000 - val_loss: 1.7112 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3680 - accuracy: 1.0000 - val_loss: 1.6761 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3863 - accuracy: 1.0000 - val_loss: 1.6428 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3840 - accuracy: 1.0000 - val_loss: 1.6157 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3546 - accuracy: 1.0000 - val_loss: 1.5906 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3527 - accuracy: 1.0000 - val_loss: 1.5687 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3730 - accuracy: 1.0000 - val_loss: 1.5423 - val_accuracy: 0.7500 - 19s/epoch - 10s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3720 - accuracy: 1.0000 - val_loss: 1.5121 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3424 - accuracy: 1.0000 - val_loss: 1.4766 - val_accuracy: 0.7500 - 20s/epoch - 10s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3672 - accuracy: 1.0000 - val_loss: 1.4390 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3340 - accuracy: 1.0000 - val_loss: 1.4057 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3600 - accuracy: 1.0000 - val_loss: 1.3778 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3545 - accuracy: 1.0000 - val_loss: 1.3527 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3471 - accuracy: 1.0000 - val_loss: 1.3316 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3942 - accuracy: 1.0000 - val_loss: 1.3089 - val_accuracy: 0.6875 - 19s/epoch - 10s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3207 - accuracy: 1.0000 - val_loss: 1.2901 - val_accuracy: 0.6250 - 20s/epoch - 10s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3177 - accuracy: 1.0000 - val_loss: 1.2717 - val_accuracy: 0.6250 - 19s/epoch - 10s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.81672\n",
      "2/2 - 19s - loss: 0.3141 - accuracy: 1.0000 - val_loss: 1.2560 - val_accuracy: 0.6250 - 19s/epoch - 9s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.81672\n",
      "2/2 - 20s - loss: 0.3204 - accuracy: 1.0000 - val_loss: 1.2417 - val_accuracy: 0.6875 - 20s/epoch - 10s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3144 - accuracy: 1.0000 - val_loss: 1.2373 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3271 - accuracy: 1.0000 - val_loss: 1.2335 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3076 - accuracy: 1.0000 - val_loss: 1.2269 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3075 - accuracy: 1.0000 - val_loss: 1.2168 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3045 - accuracy: 1.0000 - val_loss: 1.2029 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3001 - accuracy: 1.0000 - val_loss: 1.1845 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3048 - accuracy: 1.0000 - val_loss: 1.1650 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3584 - accuracy: 1.0000 - val_loss: 1.1488 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.2979 - accuracy: 1.0000 - val_loss: 1.1346 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3129 - accuracy: 1.0000 - val_loss: 1.1125 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3224 - accuracy: 1.0000 - val_loss: 1.0881 - val_accuracy: 0.6875 - 17s/epoch - 9s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.2921 - accuracy: 1.0000 - val_loss: 1.0705 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3023 - accuracy: 1.0000 - val_loss: 1.0557 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.81672\n",
      "2/2 - 17s - loss: 0.3084 - accuracy: 1.0000 - val_loss: 1.0433 - val_accuracy: 0.6250 - 17s/epoch - 9s/step\n",
      "1/1 [==============================] - 1s 980ms/step\n",
      "Classification accuracy: 0.495845 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_TSGL = EEGNet_TSGL_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34468523 0.65531474]\n",
      " [0.4223048  0.5776952 ]\n",
      " [0.66058546 0.33941454]\n",
      " [0.82226074 0.1777392 ]\n",
      " [0.8576841  0.142316  ]\n",
      " [0.79356056 0.2064394 ]\n",
      " [0.43697017 0.5630298 ]\n",
      " [0.23311326 0.7668868 ]\n",
      " [0.06507578 0.93492424]\n",
      " [0.48482734 0.5151727 ]\n",
      " [0.5046285  0.49537155]\n",
      " [0.68665516 0.3133448 ]\n",
      " [0.46980935 0.5301907 ]\n",
      " [0.55152243 0.4484776 ]\n",
      " [0.11421819 0.8857818 ]\n",
      " [0.25776443 0.74223554]\n",
      " [0.45055872 0.5494413 ]\n",
      " [0.7503406  0.24965943]\n",
      " [0.5944622  0.40553778]]\n",
      "[1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0]\n",
      "[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]\n",
      "\n",
      " Confusion matrix:\n",
      "[[7 4]\n",
      " [2 6]]\n",
      "[68.42 77.78 60.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_TSGL)\n",
    "preds_TSGL = probs_TSGL.argmax(axis = -1)  \n",
    "print(preds_TSGL)\n",
    "print(test_labels[:,0].T)\n",
    "\n",
    "performance_TSGL = compute_metrics(test_labels, preds_TSGL)\n",
    "print(performance_TSGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with init data, 300 epochs\n",
    "probs_TSGL_init = np.array([[0.34468523, 0.65531474],\n",
    "                            [0.4223048,  0.5776952 ],\n",
    "                            [0.66058546, 0.33941454],\n",
    "                            [0.82226074, 0.1777392 ],\n",
    "                            [0.85768410, 0.142316  ],\n",
    "                            [0.79356056, 0.2064394 ],\n",
    "                            [0.43697017, 0.5630298 ],\n",
    "                            [0.23311326, 0.7668868 ],\n",
    "                            [0.06507578, 0.93492424],\n",
    "                            [0.48482734, 0.5151727 ],\n",
    "                            [0.50462850, 0.49537155],\n",
    "                            [0.68665516, 0.3133448 ],\n",
    "                            [0.46980935, 0.5301907 ],\n",
    "                            [0.55152243, 0.4484776 ],\n",
    "                            [0.11421819, 0.8857818 ],\n",
    "                            [0.25776443, 0.74223554],\n",
    "                            [0.45055872, 0.5494413 ],\n",
    "                            [0.75034060, 0.24965943],\n",
    "                            [0.59446220, 0.40553778]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 7.76122, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 3.6783 - accuracy: 0.5227 - val_loss: 7.7612 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss did not improve from 7.76122\n",
      "2/2 - 12s - loss: 67.7632 - accuracy: 0.4773 - val_loss: 8.2938 - val_accuracy: 0.6875 - 12s/epoch - 6s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss did not improve from 7.76122\n",
      "2/2 - 12s - loss: 21.4602 - accuracy: 0.5682 - val_loss: 14.4481 - val_accuracy: 0.5625 - 12s/epoch - 6s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss did not improve from 7.76122\n",
      "2/2 - 12s - loss: 16.4019 - accuracy: 0.5455 - val_loss: 18.4597 - val_accuracy: 0.4375 - 12s/epoch - 6s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss improved from 7.76122 to 5.28741, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 12s - loss: 10.7277 - accuracy: 0.6818 - val_loss: 5.2874 - val_accuracy: 0.5625 - 12s/epoch - 6s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss improved from 5.28741 to 3.67839, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 1.5927 - accuracy: 0.8409 - val_loss: 3.6784 - val_accuracy: 0.5625 - 16s/epoch - 8s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 3.67839\n",
      "2/2 - 18s - loss: 0.1493 - accuracy: 0.9545 - val_loss: 4.4617 - val_accuracy: 0.7500 - 18s/epoch - 9s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 3.67839\n",
      "2/2 - 18s - loss: 0.5585 - accuracy: 0.9091 - val_loss: 5.4611 - val_accuracy: 0.5625 - 18s/epoch - 9s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss improved from 3.67839 to 2.47593, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 0.1588 - accuracy: 0.9318 - val_loss: 2.4759 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss improved from 2.47593 to 1.95503, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 1.5375e-04 - accuracy: 1.0000 - val_loss: 1.9550 - val_accuracy: 0.2500 - 15s/epoch - 7s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.95503\n",
      "2/2 - 15s - loss: 1.0838e-04 - accuracy: 1.0000 - val_loss: 3.1296 - val_accuracy: 0.4375 - 15s/epoch - 7s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.95503\n",
      "2/2 - 14s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.9616 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.95503\n",
      "2/2 - 14s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 3.3959 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.95503\n",
      "2/2 - 14s - loss: 9.0020e-04 - accuracy: 1.0000 - val_loss: 2.9733 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.95503\n",
      "2/2 - 14s - loss: 8.1320e-04 - accuracy: 1.0000 - val_loss: 2.6103 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.95503\n",
      "2/2 - 14s - loss: 1.7188e-04 - accuracy: 1.0000 - val_loss: 2.3055 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.95503\n",
      "2/2 - 15s - loss: 1.3107e-04 - accuracy: 1.0000 - val_loss: 2.1611 - val_accuracy: 0.2500 - 15s/epoch - 8s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.95503\n",
      "2/2 - 14s - loss: 6.0791e-05 - accuracy: 1.0000 - val_loss: 2.0176 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss improved from 1.95503 to 1.93426, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 6.0667e-05 - accuracy: 1.0000 - val_loss: 1.9343 - val_accuracy: 0.4375 - 15s/epoch - 7s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.93426\n",
      "2/2 - 14s - loss: 4.1947e-05 - accuracy: 1.0000 - val_loss: 1.9395 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss improved from 1.93426 to 1.84527, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 1.2235e-04 - accuracy: 1.0000 - val_loss: 1.8453 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss improved from 1.84527 to 1.83790, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 7.9069e-05 - accuracy: 1.0000 - val_loss: 1.8379 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss improved from 1.83790 to 1.78416, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 2.6391e-05 - accuracy: 1.0000 - val_loss: 1.7842 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.78416\n",
      "2/2 - 14s - loss: 8.0955e-05 - accuracy: 1.0000 - val_loss: 1.7862 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss improved from 1.78416 to 1.74368, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 2.1074e-04 - accuracy: 1.0000 - val_loss: 1.7437 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.0951e-04 - accuracy: 1.0000 - val_loss: 1.7773 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 2.1746e-04 - accuracy: 1.0000 - val_loss: 1.7911 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 3.6463e-05 - accuracy: 1.0000 - val_loss: 1.7505 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 8.8460e-05 - accuracy: 1.0000 - val_loss: 1.7754 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.0814e-04 - accuracy: 1.0000 - val_loss: 1.8143 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.74368\n",
      "2/2 - 15s - loss: 5.7314e-05 - accuracy: 1.0000 - val_loss: 1.8268 - val_accuracy: 0.4375 - 15s/epoch - 8s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.74368\n",
      "2/2 - 15s - loss: 3.1465e-05 - accuracy: 1.0000 - val_loss: 1.8566 - val_accuracy: 0.3750 - 15s/epoch - 7s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.2777e-05 - accuracy: 1.0000 - val_loss: 1.8875 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 2.0063e-05 - accuracy: 1.0000 - val_loss: 1.8920 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 3.1685e-05 - accuracy: 1.0000 - val_loss: 1.8979 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 2.0287e-05 - accuracy: 1.0000 - val_loss: 1.9043 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.1665e-05 - accuracy: 1.0000 - val_loss: 1.9014 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 3.0081e-05 - accuracy: 1.0000 - val_loss: 1.9169 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 2.6452e-05 - accuracy: 1.0000 - val_loss: 1.8613 - val_accuracy: 0.2500 - 14s/epoch - 7s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 2.5802e-05 - accuracy: 1.0000 - val_loss: 1.8748 - val_accuracy: 0.2500 - 14s/epoch - 7s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.9550e-05 - accuracy: 1.0000 - val_loss: 1.8670 - val_accuracy: 0.2500 - 14s/epoch - 7s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.1272e-05 - accuracy: 1.0000 - val_loss: 1.8540 - val_accuracy: 0.2500 - 14s/epoch - 7s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.74368\n",
      "2/2 - 15s - loss: 2.2747e-05 - accuracy: 1.0000 - val_loss: 1.8683 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.74368\n",
      "2/2 - 15s - loss: 2.0944e-05 - accuracy: 1.0000 - val_loss: 1.8210 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.74368\n",
      "2/2 - 15s - loss: 9.1677e-06 - accuracy: 1.0000 - val_loss: 1.8299 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.74368\n",
      "2/2 - 16s - loss: 1.2242e-05 - accuracy: 1.0000 - val_loss: 1.8014 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.74368\n",
      "2/2 - 15s - loss: 3.0576e-05 - accuracy: 1.0000 - val_loss: 1.8290 - val_accuracy: 0.3125 - 15s/epoch - 7s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.74368\n",
      "2/2 - 16s - loss: 2.6318e-05 - accuracy: 1.0000 - val_loss: 1.8454 - val_accuracy: 0.3125 - 16s/epoch - 8s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 9.4274e-06 - accuracy: 1.0000 - val_loss: 1.8263 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 4.1165e-05 - accuracy: 1.0000 - val_loss: 1.7764 - val_accuracy: 0.3125 - 14s/epoch - 7s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.0256e-05 - accuracy: 1.0000 - val_loss: 1.7763 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 1.3317e-05 - accuracy: 1.0000 - val_loss: 1.7887 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1.74368\n",
      "2/2 - 14s - loss: 5.3181e-06 - accuracy: 1.0000 - val_loss: 1.7475 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss improved from 1.74368 to 1.73458, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 3.1671e-06 - accuracy: 1.0000 - val_loss: 1.7346 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss improved from 1.73458 to 1.70690, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 3.5870e-06 - accuracy: 1.0000 - val_loss: 1.7069 - val_accuracy: 0.3750 - 14s/epoch - 7s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1.70690\n",
      "2/2 - 13s - loss: 1.4389e-05 - accuracy: 1.0000 - val_loss: 1.7269 - val_accuracy: 0.4375 - 13s/epoch - 7s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1.70690\n",
      "2/2 - 14s - loss: 1.8371e-05 - accuracy: 1.0000 - val_loss: 1.7470 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 1.70690\n",
      "2/2 - 14s - loss: 5.3073e-06 - accuracy: 1.0000 - val_loss: 1.7477 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1.70690\n",
      "2/2 - 13s - loss: 7.1115e-05 - accuracy: 1.0000 - val_loss: 1.7095 - val_accuracy: 0.4375 - 13s/epoch - 7s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1.70690\n",
      "2/2 - 14s - loss: 8.9562e-06 - accuracy: 1.0000 - val_loss: 1.7130 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 1.70690\n",
      "2/2 - 14s - loss: 7.6315e-06 - accuracy: 1.0000 - val_loss: 1.7120 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss improved from 1.70690 to 1.68251, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 1.7396e-05 - accuracy: 1.0000 - val_loss: 1.6825 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1.68251\n",
      "2/2 - 13s - loss: 1.9302e-05 - accuracy: 1.0000 - val_loss: 1.7044 - val_accuracy: 0.4375 - 13s/epoch - 7s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss improved from 1.68251 to 1.67239, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 3.7929e-06 - accuracy: 1.0000 - val_loss: 1.6724 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 8.7638e-06 - accuracy: 1.0000 - val_loss: 1.6885 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 6.3987e-06 - accuracy: 1.0000 - val_loss: 1.6940 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.2029e-06 - accuracy: 1.0000 - val_loss: 1.7114 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 9.7172e-06 - accuracy: 1.0000 - val_loss: 1.7213 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.0061e-05 - accuracy: 1.0000 - val_loss: 1.7269 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 1.67239\n",
      "2/2 - 15s - loss: 9.9931e-06 - accuracy: 1.0000 - val_loss: 1.7423 - val_accuracy: 0.4375 - 15s/epoch - 7s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 1.67239\n",
      "2/2 - 15s - loss: 4.2317e-06 - accuracy: 1.0000 - val_loss: 1.7409 - val_accuracy: 0.4375 - 15s/epoch - 7s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 8.3706e-06 - accuracy: 1.0000 - val_loss: 1.7506 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 3.0858e-06 - accuracy: 1.0000 - val_loss: 1.7457 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.5849e-06 - accuracy: 1.0000 - val_loss: 1.7404 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.3408e-05 - accuracy: 1.0000 - val_loss: 1.7386 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 1.67239\n",
      "2/2 - 16s - loss: 6.9353e-06 - accuracy: 1.0000 - val_loss: 1.7362 - val_accuracy: 0.4375 - 16s/epoch - 8s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 1.67239\n",
      "2/2 - 16s - loss: 4.9496e-06 - accuracy: 1.0000 - val_loss: 1.7341 - val_accuracy: 0.4375 - 16s/epoch - 8s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 2.3548e-05 - accuracy: 1.0000 - val_loss: 1.6865 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 5.0526e-06 - accuracy: 1.0000 - val_loss: 1.6792 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 1.67239\n",
      "2/2 - 15s - loss: 8.0865e-06 - accuracy: 1.0000 - val_loss: 1.6938 - val_accuracy: 0.4375 - 15s/epoch - 8s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 1.67239\n",
      "2/2 - 15s - loss: 6.9300e-06 - accuracy: 1.0000 - val_loss: 1.7217 - val_accuracy: 0.4375 - 15s/epoch - 7s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.0783e-06 - accuracy: 1.0000 - val_loss: 1.7246 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 2.7580e-06 - accuracy: 1.0000 - val_loss: 1.7291 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 8.4385e-06 - accuracy: 1.0000 - val_loss: 1.7521 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 8.1212e-06 - accuracy: 1.0000 - val_loss: 1.7683 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 6.9434e-06 - accuracy: 1.0000 - val_loss: 1.7727 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 2.8122e-06 - accuracy: 1.0000 - val_loss: 1.7344 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 2.0130e-06 - accuracy: 1.0000 - val_loss: 1.7377 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 5.8896e-06 - accuracy: 1.0000 - val_loss: 1.7514 - val_accuracy: 0.4375 - 14s/epoch - 7s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 4.5959e-05 - accuracy: 1.0000 - val_loss: 1.6970 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.7692e-06 - accuracy: 1.0000 - val_loss: 1.7154 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 5.6187e-06 - accuracy: 1.0000 - val_loss: 1.7072 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.9669e-06 - accuracy: 1.0000 - val_loss: 1.7073 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 1.67239\n",
      "2/2 - 15s - loss: 5.2719e-06 - accuracy: 1.0000 - val_loss: 1.7027 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 5.8978e-06 - accuracy: 1.0000 - val_loss: 1.7054 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 1.67239\n",
      "2/2 - 14s - loss: 1.9398e-06 - accuracy: 1.0000 - val_loss: 1.7081 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 1.67239\n",
      "2/2 - 15s - loss: 3.7740e-06 - accuracy: 1.0000 - val_loss: 1.7153 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss improved from 1.67239 to 1.65524, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 16s - loss: 1.8672e-05 - accuracy: 1.0000 - val_loss: 1.6552 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 1.65524\n",
      "2/2 - 16s - loss: 3.6899e-06 - accuracy: 1.0000 - val_loss: 1.6711 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 1.65524\n",
      "2/2 - 15s - loss: 7.6673e-07 - accuracy: 1.0000 - val_loss: 1.6773 - val_accuracy: 0.5000 - 15s/epoch - 8s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss improved from 1.65524 to 1.62015, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 5.7082e-06 - accuracy: 1.0000 - val_loss: 1.6201 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 1.62015\n",
      "2/2 - 14s - loss: 3.3730e-06 - accuracy: 1.0000 - val_loss: 1.6327 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss improved from 1.62015 to 1.59444, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 5.7677e-06 - accuracy: 1.0000 - val_loss: 1.5944 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 1.59444\n",
      "2/2 - 13s - loss: 7.0442e-07 - accuracy: 1.0000 - val_loss: 1.6034 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 1.59444\n",
      "2/2 - 13s - loss: 2.3408e-06 - accuracy: 1.0000 - val_loss: 1.6308 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 1.59444\n",
      "2/2 - 14s - loss: 3.7306e-06 - accuracy: 1.0000 - val_loss: 1.6704 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 1.59444\n",
      "2/2 - 13s - loss: 4.5596e-06 - accuracy: 1.0000 - val_loss: 1.6227 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 1.59444\n",
      "2/2 - 14s - loss: 5.6895e-07 - accuracy: 1.0000 - val_loss: 1.6082 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 1.59444\n",
      "2/2 - 14s - loss: 2.5467e-07 - accuracy: 1.0000 - val_loss: 1.5989 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 1.59444\n",
      "2/2 - 14s - loss: 3.3188e-06 - accuracy: 1.0000 - val_loss: 1.5963 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 1.59444\n",
      "2/2 - 14s - loss: 4.3239e-06 - accuracy: 1.0000 - val_loss: 1.6197 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss improved from 1.59444 to 1.59077, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 1.0714e-05 - accuracy: 1.0000 - val_loss: 1.5908 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.4007e-06 - accuracy: 1.0000 - val_loss: 1.6326 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.0807e-06 - accuracy: 1.0000 - val_loss: 1.6675 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.2246e-06 - accuracy: 1.0000 - val_loss: 1.6262 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 4.3942e-06 - accuracy: 1.0000 - val_loss: 1.6041 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.2080e-06 - accuracy: 1.0000 - val_loss: 1.6325 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 8.0784e-06 - accuracy: 1.0000 - val_loss: 1.5956 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.4955e-06 - accuracy: 1.0000 - val_loss: 1.6241 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 3.5517e-06 - accuracy: 1.0000 - val_loss: 1.6648 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.5687e-06 - accuracy: 1.0000 - val_loss: 1.6978 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 3.6980e-06 - accuracy: 1.0000 - val_loss: 1.6658 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 1.4224e-06 - accuracy: 1.0000 - val_loss: 1.6700 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 5.5269e-07 - accuracy: 1.0000 - val_loss: 1.6789 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 4.2807e-07 - accuracy: 1.0000 - val_loss: 1.6522 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 5.6323e-06 - accuracy: 1.0000 - val_loss: 1.6358 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.4847e-06 - accuracy: 1.0000 - val_loss: 1.6456 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.1785e-06 - accuracy: 1.0000 - val_loss: 1.6535 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.5416e-06 - accuracy: 1.0000 - val_loss: 1.6546 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 6.9087e-07 - accuracy: 1.0000 - val_loss: 1.6618 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.9774e-06 - accuracy: 1.0000 - val_loss: 1.6958 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.0675e-06 - accuracy: 1.0000 - val_loss: 1.7066 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 1.5063e-06 - accuracy: 1.0000 - val_loss: 1.7134 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.6632e-06 - accuracy: 1.0000 - val_loss: 1.6811 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 1.2679e-06 - accuracy: 1.0000 - val_loss: 1.7092 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 1.1350e-05 - accuracy: 1.0000 - val_loss: 1.6619 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.2026e-06 - accuracy: 1.0000 - val_loss: 1.6933 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 1.0602e-05 - accuracy: 1.0000 - val_loss: 1.6566 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 7.9938e-06 - accuracy: 1.0000 - val_loss: 1.6799 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.2839e-06 - accuracy: 1.0000 - val_loss: 1.6896 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 8.8594e-07 - accuracy: 1.0000 - val_loss: 1.7215 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 2.5033e-06 - accuracy: 1.0000 - val_loss: 1.7534 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 9.9972e-07 - accuracy: 1.0000 - val_loss: 1.7304 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 3.0912e-06 - accuracy: 1.0000 - val_loss: 1.7383 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 3.2917e-06 - accuracy: 1.0000 - val_loss: 1.7535 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.1542e-06 - accuracy: 1.0000 - val_loss: 1.7418 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.8748e-06 - accuracy: 1.0000 - val_loss: 1.7708 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 3.9014e-07 - accuracy: 1.0000 - val_loss: 1.7678 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.5416e-06 - accuracy: 1.0000 - val_loss: 1.7662 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 9.0219e-07 - accuracy: 1.0000 - val_loss: 1.7678 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 2.5386e-06 - accuracy: 1.0000 - val_loss: 1.7639 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 9.5366e-07 - accuracy: 1.0000 - val_loss: 1.7756 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.3167e-06 - accuracy: 1.0000 - val_loss: 1.7640 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.0130e-06 - accuracy: 1.0000 - val_loss: 1.7513 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 5.5725e-06 - accuracy: 1.0000 - val_loss: 1.6868 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 8.5614e-07 - accuracy: 1.0000 - val_loss: 1.7001 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.7610e-06 - accuracy: 1.0000 - val_loss: 1.6521 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.4007e-06 - accuracy: 1.0000 - val_loss: 1.6781 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.1325e-06 - accuracy: 1.0000 - val_loss: 1.6766 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 1.8992e-06 - accuracy: 1.0000 - val_loss: 1.6892 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.5930e-06 - accuracy: 1.0000 - val_loss: 1.6978 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.7583e-06 - accuracy: 1.0000 - val_loss: 1.7137 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 3.2863e-06 - accuracy: 1.0000 - val_loss: 1.6707 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.9206e-06 - accuracy: 1.0000 - val_loss: 1.7036 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 1.59077\n",
      "2/2 - 13s - loss: 1.1921e-06 - accuracy: 1.0000 - val_loss: 1.7142 - val_accuracy: 0.5000 - 13s/epoch - 7s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 4.9038e-07 - accuracy: 1.0000 - val_loss: 1.7318 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.4738e-06 - accuracy: 1.0000 - val_loss: 1.7292 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 6.9087e-07 - accuracy: 1.0000 - val_loss: 1.7196 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 4.6298e-06 - accuracy: 1.0000 - val_loss: 1.7313 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.7583e-06 - accuracy: 1.0000 - val_loss: 1.7259 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.6147e-06 - accuracy: 1.0000 - val_loss: 1.6706 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.3167e-06 - accuracy: 1.0000 - val_loss: 1.6976 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.8695e-05 - accuracy: 1.0000 - val_loss: 1.6379 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 3.4759e-06 - accuracy: 1.0000 - val_loss: 1.6535 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 7.4235e-07 - accuracy: 1.0000 - val_loss: 1.6864 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 3.4732e-06 - accuracy: 1.0000 - val_loss: 1.7191 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 8.0466e-07 - accuracy: 1.0000 - val_loss: 1.7066 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 1.59077\n",
      "2/2 - 15s - loss: 4.4161e-07 - accuracy: 1.0000 - val_loss: 1.7184 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.3980e-06 - accuracy: 1.0000 - val_loss: 1.7254 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 9.6721e-07 - accuracy: 1.0000 - val_loss: 1.6802 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.4901e-06 - accuracy: 1.0000 - val_loss: 1.6812 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 1.59077\n",
      "2/2 - 15s - loss: 1.7150e-06 - accuracy: 1.0000 - val_loss: 1.6891 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 1.59077\n",
      "2/2 - 15s - loss: 9.7263e-07 - accuracy: 1.0000 - val_loss: 1.7046 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.9654e-05 - accuracy: 1.0000 - val_loss: 1.6354 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.8393e-06 - accuracy: 1.0000 - val_loss: 1.6036 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.9371e-06 - accuracy: 1.0000 - val_loss: 1.6393 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 6.7399e-06 - accuracy: 1.0000 - val_loss: 1.5999 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.6526e-06 - accuracy: 1.0000 - val_loss: 1.6235 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.4901e-07 - accuracy: 1.0000 - val_loss: 1.6046 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.3032e-06 - accuracy: 1.0000 - val_loss: 1.6285 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 6.6017e-06 - accuracy: 1.0000 - val_loss: 1.6021 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.1216e-06 - accuracy: 1.0000 - val_loss: 1.6176 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.2351e-06 - accuracy: 1.0000 - val_loss: 1.6303 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 1.0918e-06 - accuracy: 1.0000 - val_loss: 1.6375 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 1.59077\n",
      "2/2 - 14s - loss: 2.2351e-06 - accuracy: 1.0000 - val_loss: 1.6492 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 1.59077\n",
      "2/2 - 15s - loss: 5.6269e-06 - accuracy: 1.0000 - val_loss: 1.6024 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss improved from 1.59077 to 1.57502, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 15s - loss: 7.0171e-07 - accuracy: 1.0000 - val_loss: 1.5750 - val_accuracy: 0.5000 - 15s/epoch - 8s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 1.57502\n",
      "2/2 - 15s - loss: 7.0983e-07 - accuracy: 1.0000 - val_loss: 1.5960 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 1.57502\n",
      "2/2 - 14s - loss: 1.5091e-06 - accuracy: 1.0000 - val_loss: 1.6096 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 1.57502\n",
      "2/2 - 14s - loss: 1.0322e-06 - accuracy: 1.0000 - val_loss: 1.5938 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss improved from 1.57502 to 1.56580, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 6.3691e-06 - accuracy: 1.0000 - val_loss: 1.5658 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 1.56580\n",
      "2/2 - 14s - loss: 1.4630e-06 - accuracy: 1.0000 - val_loss: 1.5921 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss improved from 1.56580 to 1.56577, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 1.4301e-05 - accuracy: 1.0000 - val_loss: 1.5658 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 2.9531e-07 - accuracy: 1.0000 - val_loss: 1.5950 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 8.4800e-07 - accuracy: 1.0000 - val_loss: 1.6090 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.5226e-06 - accuracy: 1.0000 - val_loss: 1.6530 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.1000e-06 - accuracy: 1.0000 - val_loss: 1.6691 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.9750e-06 - accuracy: 1.0000 - val_loss: 1.6400 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 2.2297e-06 - accuracy: 1.0000 - val_loss: 1.6130 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.2490e-06 - accuracy: 1.0000 - val_loss: 1.6304 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.6770e-06 - accuracy: 1.0000 - val_loss: 1.6005 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 7.7756e-07 - accuracy: 1.0000 - val_loss: 1.6161 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.1487e-06 - accuracy: 1.0000 - val_loss: 1.6245 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 1.56577\n",
      "2/2 - 15s - loss: 1.6391e-06 - accuracy: 1.0000 - val_loss: 1.6623 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 2.4492e-06 - accuracy: 1.0000 - val_loss: 1.6995 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.6066e-06 - accuracy: 1.0000 - val_loss: 1.7199 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 6.4960e-06 - accuracy: 1.0000 - val_loss: 1.6780 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 2.4600e-06 - accuracy: 1.0000 - val_loss: 1.6490 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.8531e-06 - accuracy: 1.0000 - val_loss: 1.6802 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.9154e-06 - accuracy: 1.0000 - val_loss: 1.6633 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.0268e-06 - accuracy: 1.0000 - val_loss: 1.6653 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.3303e-06 - accuracy: 1.0000 - val_loss: 1.6750 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 1.56577\n",
      "2/2 - 15s - loss: 1.3844e-06 - accuracy: 1.0000 - val_loss: 1.7010 - val_accuracy: 0.5000 - 15s/epoch - 8s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 1.56577\n",
      "2/2 - 16s - loss: 2.2460e-06 - accuracy: 1.0000 - val_loss: 1.7268 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 1.56577\n",
      "2/2 - 16s - loss: 6.9087e-07 - accuracy: 1.0000 - val_loss: 1.7115 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 1.56577\n",
      "2/2 - 16s - loss: 1.2002e-06 - accuracy: 1.0000 - val_loss: 1.7308 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 1.56577\n",
      "2/2 - 17s - loss: 8.4529e-07 - accuracy: 1.0000 - val_loss: 1.7561 - val_accuracy: 0.5000 - 17s/epoch - 8s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 1.56577\n",
      "2/2 - 16s - loss: 6.9087e-07 - accuracy: 1.0000 - val_loss: 1.7691 - val_accuracy: 0.5000 - 16s/epoch - 8s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 1.56577\n",
      "2/2 - 15s - loss: 1.6825e-06 - accuracy: 1.0000 - val_loss: 1.7839 - val_accuracy: 0.5000 - 15s/epoch - 8s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 1.56577\n",
      "2/2 - 15s - loss: 1.6851e-06 - accuracy: 1.0000 - val_loss: 1.7318 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 1.56577\n",
      "2/2 - 15s - loss: 1.5416e-06 - accuracy: 1.0000 - val_loss: 1.7493 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 2.2487e-06 - accuracy: 1.0000 - val_loss: 1.7428 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.6662e-06 - accuracy: 1.0000 - val_loss: 1.7605 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 2.8690e-06 - accuracy: 1.0000 - val_loss: 1.6940 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 7.9382e-07 - accuracy: 1.0000 - val_loss: 1.7018 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 1.56577\n",
      "2/2 - 15s - loss: 2.3571e-06 - accuracy: 1.0000 - val_loss: 1.7404 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.0702e-06 - accuracy: 1.0000 - val_loss: 1.7639 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 2.6794e-06 - accuracy: 1.0000 - val_loss: 1.6921 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.0810e-06 - accuracy: 1.0000 - val_loss: 1.7137 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 1.56577\n",
      "2/2 - 15s - loss: 4.2210e-06 - accuracy: 1.0000 - val_loss: 1.6462 - val_accuracy: 0.5000 - 15s/epoch - 7s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 6.6648e-07 - accuracy: 1.0000 - val_loss: 1.6799 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 9.4283e-07 - accuracy: 1.0000 - val_loss: 1.7090 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 4.6571e-06 - accuracy: 1.0000 - val_loss: 1.6411 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 9.7283e-06 - accuracy: 1.0000 - val_loss: 1.6172 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 8.1279e-07 - accuracy: 1.0000 - val_loss: 1.6319 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 1.56577\n",
      "2/2 - 14s - loss: 1.9391e-05 - accuracy: 1.0000 - val_loss: 1.5881 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss improved from 1.56577 to 1.56558, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 2.8312e-06 - accuracy: 1.0000 - val_loss: 1.5656 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss improved from 1.56558 to 1.54067, saving model to /tmp\\checkpoint.h5\n",
      "2/2 - 14s - loss: 6.4608e-06 - accuracy: 1.0000 - val_loss: 1.5407 - val_accuracy: 0.5000 - 14s/epoch - 7s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.6797e-06 - accuracy: 1.0000 - val_loss: 1.5930 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.5958e-06 - accuracy: 1.0000 - val_loss: 1.6288 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 5.2018e-07 - accuracy: 1.0000 - val_loss: 1.6460 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.4630e-06 - accuracy: 1.0000 - val_loss: 1.6875 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 2.8663e-06 - accuracy: 1.0000 - val_loss: 1.6544 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.3357e-06 - accuracy: 1.0000 - val_loss: 1.6930 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 6.6107e-07 - accuracy: 1.0000 - val_loss: 1.6982 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 3.1400e-06 - accuracy: 1.0000 - val_loss: 1.7343 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.1758e-06 - accuracy: 1.0000 - val_loss: 1.7398 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 3.8472e-07 - accuracy: 1.0000 - val_loss: 1.7396 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.3465e-06 - accuracy: 1.0000 - val_loss: 1.7601 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 9.6180e-07 - accuracy: 1.0000 - val_loss: 1.7563 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 5.6082e-07 - accuracy: 1.0000 - val_loss: 1.7488 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 3.5221e-07 - accuracy: 1.0000 - val_loss: 1.7459 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 3.3756e-06 - accuracy: 1.0000 - val_loss: 1.6899 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 8.1278e-07 - accuracy: 1.0000 - val_loss: 1.6993 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 2.9882e-06 - accuracy: 1.0000 - val_loss: 1.6715 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 7.7234e-06 - accuracy: 1.0000 - val_loss: 1.6373 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 6.3668e-07 - accuracy: 1.0000 - val_loss: 1.6697 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 6.8545e-07 - accuracy: 1.0000 - val_loss: 1.7079 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 7.4234e-07 - accuracy: 1.0000 - val_loss: 1.7423 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 4.8711e-06 - accuracy: 1.0000 - val_loss: 1.6807 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.1650e-06 - accuracy: 1.0000 - val_loss: 1.7182 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 4.0639e-07 - accuracy: 1.0000 - val_loss: 1.7235 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 8.5614e-07 - accuracy: 1.0000 - val_loss: 1.7380 - val_accuracy: 0.5625 - 15s/epoch - 7s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 5.6353e-07 - accuracy: 1.0000 - val_loss: 1.7400 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 1.3059e-06 - accuracy: 1.0000 - val_loss: 1.7452 - val_accuracy: 0.5625 - 15s/epoch - 7s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 2.7228e-06 - accuracy: 1.0000 - val_loss: 1.7698 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 7.5047e-07 - accuracy: 1.0000 - val_loss: 1.7790 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 1.3519e-06 - accuracy: 1.0000 - val_loss: 1.8119 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 1.1108e-06 - accuracy: 1.0000 - val_loss: 1.7867 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 3.1128e-06 - accuracy: 1.0000 - val_loss: 1.7085 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 8.2904e-07 - accuracy: 1.0000 - val_loss: 1.7359 - val_accuracy: 0.5625 - 15s/epoch - 7s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.9073e-06 - accuracy: 1.0000 - val_loss: 1.7037 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 4.2807e-07 - accuracy: 1.0000 - val_loss: 1.7170 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.6987e-06 - accuracy: 1.0000 - val_loss: 1.6607 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.3411e-06 - accuracy: 1.0000 - val_loss: 1.6900 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 5.4186e-07 - accuracy: 1.0000 - val_loss: 1.6744 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 8.4258e-07 - accuracy: 1.0000 - val_loss: 1.6846 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.4847e-06 - accuracy: 1.0000 - val_loss: 1.7239 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 1.54067\n",
      "2/2 - 13s - loss: 1.1650e-06 - accuracy: 1.0000 - val_loss: 1.7502 - val_accuracy: 0.5625 - 13s/epoch - 7s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 1.54067\n",
      "2/2 - 13s - loss: 2.4113e-07 - accuracy: 1.0000 - val_loss: 1.7573 - val_accuracy: 0.5625 - 13s/epoch - 7s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 7.1254e-07 - accuracy: 1.0000 - val_loss: 1.7529 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 1.54067\n",
      "2/2 - 13s - loss: 6.7461e-07 - accuracy: 1.0000 - val_loss: 1.7480 - val_accuracy: 0.5625 - 13s/epoch - 7s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 8.8323e-07 - accuracy: 1.0000 - val_loss: 1.7497 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 5.4457e-07 - accuracy: 1.0000 - val_loss: 1.7772 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 4.2265e-07 - accuracy: 1.0000 - val_loss: 1.7724 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 1.3086e-06 - accuracy: 1.0000 - val_loss: 1.7093 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 2.0265e-06 - accuracy: 1.0000 - val_loss: 1.6738 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 1.54067\n",
      "2/2 - 14s - loss: 9.8347e-07 - accuracy: 1.0000 - val_loss: 1.7085 - val_accuracy: 0.5625 - 14s/epoch - 7s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 7.6673e-07 - accuracy: 1.0000 - val_loss: 1.7368 - val_accuracy: 0.5625 - 15s/epoch - 8s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 1.54067\n",
      "2/2 - 15s - loss: 1.0133e-06 - accuracy: 1.0000 - val_loss: 1.7676 - val_accuracy: 0.5625 - 15s/epoch - 7s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Classification accuracy: 0.545706 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_Deep = EEGNet_DeepConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.0045400e-01 2.9954603e-01]\n",
      " [7.8413427e-01 2.1586572e-01]\n",
      " [7.1874863e-01 2.8125137e-01]\n",
      " [9.9979991e-01 2.0014797e-04]\n",
      " [2.9218609e-02 9.7078133e-01]\n",
      " [9.9454159e-01 5.4584546e-03]\n",
      " [5.5008806e-02 9.4499117e-01]\n",
      " [9.6587259e-01 3.4127403e-02]\n",
      " [6.3271725e-01 3.6728275e-01]\n",
      " [9.8455411e-01 1.5445878e-02]\n",
      " [8.1000119e-01 1.8999882e-01]\n",
      " [9.9752015e-01 2.4798173e-03]\n",
      " [9.8586375e-01 1.4136297e-02]\n",
      " [4.3618846e-01 5.6381154e-01]\n",
      " [1.4959927e-01 8.5040075e-01]\n",
      " [9.5763546e-01 4.2364582e-02]\n",
      " [9.9997401e-01 2.5972882e-05]\n",
      " [9.9117404e-01 8.8259308e-03]\n",
      " [9.9719328e-01 2.8067816e-03]]\n",
      "[0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      "[[1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0]]\n",
      "\n",
      " Confusion matrix:\n",
      "[[9 2]\n",
      " [6 2]]\n",
      "[57.89 60.   50.  ]\n"
     ]
    }
   ],
   "source": [
    "print(probs_Deep)\n",
    "preds_Deep = probs_Deep.argmax(axis = -1)  \n",
    "print(preds_Deep)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Deep = compute_metrics(test_labels, preds_Deep)\n",
    "print(performance_Deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "<>:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "C:\\Users\\annej\\AppData\\Local\\Temp\\ipykernel_14032\\2687471855.py:5: SyntaxWarning: list indices must be integers or slices, not tuple; perhaps you missed a comma?\n",
      "  [2.9218609e-02, 9.7078133e-01]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14032\\2687471855.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m7.1874863e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.8125137e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m9.9979991e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0014797e-04\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                             \u001b[1;33m[\u001b[0m\u001b[1;36m2.9218609e-02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9.7078133e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m9.9454159e-01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.4584546e-03\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                             \u001b[1;33m[\u001b[0m\u001b[1;36m5.5008806e-02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9.4499117e-01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "probs_Deep_init = np.array([[7.0045400e-01, 2.9954603e-01],\n",
    "                            [7.8413427e-01, 2.1586572e-01],\n",
    "                            [7.1874863e-01, 2.8125137e-01],\n",
    "                            [9.9979991e-01, 2.0014797e-04],\n",
    "                            [2.9218609e-02, 9.7078133e-01]\n",
    "                            [9.9454159e-01, 5.4584546e-03],\n",
    "                            [5.5008806e-02, 9.4499117e-01],\n",
    "                            [9.6587259e-01, 3.4127403e-02],\n",
    "                            [6.3271725e-01, 3.6728275e-01],\n",
    "                            [9.8455411e-01, 1.5445878e-02],\n",
    "                            [8.1000119e-01, 1.8999882e-01],\n",
    "                            [9.9752015e-01, 2.4798173e-03],\n",
    "                            [9.8586375e-01, 1.4136297e-02],\n",
    "                            [4.3618846e-01, 5.6381154e-01],\n",
    "                            [1.4959927e-01, 8.5040075e-01],\n",
    "                            [9.5763546e-01, 4.2364582e-02],\n",
    "                            [9.9997401e-01, 2.5972882e-05],\n",
    "                            [9.9117404e-01, 8.8259308e-03],\n",
    "                            [9.9719328e-01, 2.8067816e-03]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_Shallow = EEGNet_ShallowConvNet_classification(train_data, test_data, val_data, train_labels, test_labels, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs_Shallow)\n",
    "preds_Shallow = probs_Shallow.argmax(axis = -1)  \n",
    "print(preds_Shallow)\n",
    "print(test_labels.T)\n",
    "\n",
    "performance_Shallow = compute_metrics(test_labels, preds_Shallow)\n",
    "print(performance_Shallow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
