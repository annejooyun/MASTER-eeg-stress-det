{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msvm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SVC\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LogisticRegression\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_model\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend \u001B[38;5;28;01mas\u001B[39;00m K\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras import models\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mne_features'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset, load_labels, convert_to_epochs\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfeatures\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m time_series_features, nonlinear_features, entropy_features, hjorth_features, freq_band_features\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\eeg_stress_detection\\features.py:2\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmne\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmne_features\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtime_series_features\u001B[39m(data, channels):\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'mne_features'"
     ]
    }
   ],
   "source": [
    "from dataset import load_dataset, load_labels, convert_to_epochs\n",
    "from features import time_series_features, nonlinear_features, entropy_features, hjorth_features, freq_band_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "channels = 32\n",
    "sfreq = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'io'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Filtered from SAM40\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m dataset_ \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m convert_to_epochs(dataset_, channels, sfreq)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#ICA filtered\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\eeg_stress_detection\\dataset.py:47\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(raw, test_type)\u001B[0m\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     46\u001B[0m f \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mdir\u001B[39m, filename)\n\u001B[1;32m---> 47\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mscipy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241m.\u001B[39mloadmat(f)[data_key]\n\u001B[0;32m     48\u001B[0m dataset[counter] \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     49\u001B[0m counter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'scipy' has no attribute 'io'"
     ]
    }
   ],
   "source": [
    "#Filtered from SAM40\n",
    "dataset_ = load_dataset(raw = False)\n",
    "dataset = convert_to_epochs(dataset_, channels, sfreq)\n",
    "\n",
    "#ICA filtered\n",
    "dataset_ica_ = load_dataset()\n",
    "dataset_ica = convert_to_epochs(dataset_ica_, channels, sfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_series_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[43mtime_series_features\u001B[49m(dataset, channels)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# freq_bands = np.array([1, 4, 8, 13, 31, 50])\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# features = freq_band_features(dataset, channels, sfreq, freq_bands)\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# features = nonlinear_features(dataset, channels)\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# features = hjorth_features(dataset, channels, sfreq)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# features = entropy_features(dataset, channels, sfreq)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m data \u001B[38;5;241m=\u001B[39m features\n",
      "\u001B[1;31mNameError\u001B[0m: name 'time_series_features' is not defined"
     ]
    }
   ],
   "source": [
    "features = time_series_features(dataset, channels)\n",
    "# freq_bands = np.array([1, 4, 8, 13, 31, 50])\n",
    "# features = freq_band_features(dataset, channels, sfreq, freq_bands)\n",
    "# features = nonlinear_features(dataset, channels)\n",
    "# features = hjorth_features(dataset, channels, sfreq)\n",
    "# features = entropy_features(dataset, channels, sfreq)\n",
    "data = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m labels \u001B[38;5;241m=\u001B[39m load_labels()\n\u001B[0;32m      2\u001B[0m label \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([labels[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt1_math\u001B[39m\u001B[38;5;124m'\u001B[39m], labels[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt2_math\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m      3\u001B[0m                   labels[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt3_math\u001B[39m\u001B[38;5;124m'\u001B[39m]])\u001B[38;5;241m.\u001B[39mto_numpy()\n\u001B[1;32m----> 4\u001B[0m label \u001B[38;5;241m=\u001B[39m label\u001B[38;5;241m.\u001B[39mrepeat(\u001B[43mdataset\u001B[49m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "labels = load_labels()\n",
    "label = pd.concat([labels['t1_math'], labels['t2_math'],\n",
    "                  labels['t3_math']]).to_numpy()\n",
    "label = label.repeat(dataset.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m x_train, x_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mdata\u001B[49m, label, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.33\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m      3\u001B[0m scaler \u001B[38;5;241m=\u001B[39m StandardScaler()\n\u001B[0;32m      4\u001B[0m scaler\u001B[38;5;241m.\u001B[39mfit(x_train)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.33, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "lr_clf = LogisticRegression(max_iter=1000).fit(x_train, y_train)\n",
    "y_pred = lr_clf.predict(x_test)\n",
    "y_true = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m scores_lr \u001B[38;5;241m=\u001B[39m \u001B[43mlr_clf\u001B[49m\u001B[38;5;241m.\u001B[39mscore(x_test, y_test)\n\u001B[0;32m      2\u001B[0m precision_lr \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39mprecision_score(y_true, y_pred, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmacro\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m recall_lr \u001B[38;5;241m=\u001B[39m metrics\u001B[38;5;241m.\u001B[39mrecall_score(y_true, y_pred, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmicro\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'lr_clf' is not defined"
     ]
    }
   ],
   "source": [
    "scores_lr = lr_clf.score(x_test, y_test)\n",
    "precision_lr = metrics.precision_score(y_true, y_pred, average='macro')\n",
    "recall_lr = metrics.recall_score(y_true, y_pred, average='micro')\n",
    "f1_score_lr = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "print('accuracy is:', scores_lr)\n",
    "print('precision is:', precision_lr)\n",
    "print('recall is:', recall_lr)\n",
    "print('f1_score is:', f1_score_lr)\n",
    "metrics.confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data, label, test_size=0.33, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "neigh_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh_clf.fit(x_train, y_train)\n",
    "y_pred = neigh_clf.predict(x_test)\n",
    "y_true = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.6323232323232323\n",
      "precision is: 0.642935839115468\n",
      "recall is: 0.6323232323232323\n",
      "f1_score is: 0.6165170220725775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[430,  97],\n",
       "       [267, 196]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_neigh = neigh_clf.score(x_test, y_test)\n",
    "precision_neigh = metrics.precision_score(y_true, y_pred, average='macro')\n",
    "recall_neigh = metrics.recall_score(y_true, y_pred, average='micro')\n",
    "f1_score_neigh = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "print('accuracy is:', scores_neigh)\n",
    "print('precision is:', precision_neigh)\n",
    "print('recall is:', recall_neigh)\n",
    "print('f1_score is:', f1_score_neigh)\n",
    "metrics.confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "data, label, test_size=0.33, random_state=42)\n",
    "svm_clf = SVC(C=10, kernel='rbf')\n",
    "svm_clf.fit(x_train, y_train)\n",
    "y_pred = svm_clf.predict(x_test)\n",
    "y_pred_train = svm_clf.predict(x_train)\n",
    "y_true = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.8084577114427861\n",
      "precision is: 0.8083113276835878\n",
      "recall is: 0.8083113276835878\n",
      "f1_score is: 0.8083628691115632\n"
     ]
    }
   ],
   "source": [
    "scores_svm_train = svm_clf.score(x_train, y_train)\n",
    "precision_svm_train = metrics.precision_score(y_train, y_pred_train, average='macro')\n",
    "recall_svm_train = metrics.recall_score(y_train, y_pred_train, average='micro')\n",
    "f1_score_svm_train = metrics.f1_score(y_train, y_pred_train, average='weighted')\n",
    "print('accuracy is:', scores_svm_train)\n",
    "print('precision is:', precision_svm_train)\n",
    "print('recall is:', precision_svm_train)\n",
    "print('f1_score is:', f1_score_svm_train)\n",
    "# metrics.confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.7575757575757576\n",
      "precision is: 0.7569322750231935\n",
      "recall is: 0.7575757575757576\n",
      "f1_score is: 0.7572081522702866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[416, 111],\n",
       "       [129, 334]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_svm = svm_clf.score(x_test, y_test)\n",
    "precision_svm = metrics.precision_score(y_true, y_pred, average='macro')\n",
    "recall_svm = metrics.recall_score(y_true, y_pred, average='micro')\n",
    "f1_score_svm = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "print('accuracy is:', scores_svm)\n",
    "print('precision is:', precision_svm)\n",
    "print('recall is:', recall_svm)\n",
    "print('f1_score is:', f1_score_svm)\n",
    "metrics.confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "y_v = label\n",
    "y_v = to_categorical(y_v, num_classes)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, y_v, test_size=0.2, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(x_train.shape[1],)))\n",
    "\n",
    "    for i in range(hp.Int('layers', 2, 6)):\n",
    "      model.add(keras.layers.Dense(units=hp.Int('units_' + str(i), 32, 1024, step=32),\n",
    "                                    activation=hp.Choice('act_' + str(i), ['relu', 'sigmoid'])))\n",
    "\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax', name='out'))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.adam_v2.Adam(learning_rate=hp_learning_rate),\n",
    "                loss = \"binary_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 10:05:26.383610: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-02 10:05:26.383742: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "tuner = RandomSearch(\n",
    "    model_builder,\n",
    "    objective = 'val_accuracy',\n",
    "    max_trials = 5,\n",
    "    executions_per_trial = 2,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 6, 'step': 1, 'sampling': None}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 1024, 'step': 32, 'sampling': None}\n",
      "act_0 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid'], 'ordered': False}\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 1024, 'step': 32, 'sampling': None}\n",
      "act_1 (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'sigmoid'], 'ordered': False}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 12s]\n",
      "val_accuracy: 0.5658333599567413\n",
      "\n",
      "Best val_accuracy So Far: 0.9291666746139526\n",
      "Total elapsed time: 00h 05m 19s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, epochs = 50, validation_data= [x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 10:10:46.007728: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_true = y_test\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 10:10:46.205803: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.9283333420753479\n",
      "precision is: 0.9287769302823499\n",
      "recall is: 0.9283333333333333\n",
      "f1_score is: 0.9282928266682259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[293,  18],\n",
       "       [ 25, 264]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dnn = model.evaluate(x_test, y_test, verbose=0)\n",
    "precision_dnn = metrics.precision_score(y_true, y_pred, average='macro')\n",
    "recall_dnn = metrics.recall_score(y_true, y_pred, average='micro')\n",
    "f1_score_dnn = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "print('accuracy is:', scores_dnn[1])\n",
    "print('precision is:', precision_dnn)\n",
    "print('recall is:', recall_dnn)\n",
    "print('f1_score is:', f1_score_dnn)\n",
    "metrics.confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a0b16b431f91af56543167d2335ade6a4f69621936ac10d0388e1e58aabcd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}