{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow.keras import utils as np_utils\n",
    "\n",
    "from utils.data import extract_eeg_data, multi_to_binary_classification, split_dataset, dict_to_arr\n",
    "from utils.labels import get_pss_labels, get_stai_labels\n",
    "from utils.valid_recs import get_valid_recs\n",
    "from features import all_features, hjorth_features\n",
    "from classifiers import knn_classification, svm_classification, cnn_classification, EEGNet_classification_2\n",
    "import utils.variables as v\n",
    "\n",
    "from EEGModels import EEGNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/ICA_data\\sub-P010_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P013_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P013_ses-S001_run-002.mat not valid\n",
      "Data/ICA_data\\sub-P020_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P023_ses-S002_run-002.mat not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n",
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n"
     ]
    }
   ],
   "source": [
    "valid_recs = get_valid_recs(data_type='ica', output_type = 'np')\n",
    "print(f'Valid recs {valid_recs}')\n",
    "\n",
    "x_dict_ = extract_eeg_data(valid_recs, data_type='ica', output_type='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "Invalid value for label\n",
      "Invalid value for label\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid value for label\n",
      "Invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n"
     ]
    }
   ],
   "source": [
    "y_dict_ = get_stai_labels(valid_recs) \n",
    "#y_dict = get_pss_labels(valid_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing invalid labels: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing invalid labels: {len(y_dict_)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n"
     ]
    }
   ],
   "source": [
    "x_dict, y_dict = multi_to_binary_classification(x_dict_, y_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing mildly stressed subjects: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing  mildly stressed subjects: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict, test_data_dict, val_data_dict, train_labels_dict, test_labels_dict, val_labels_dict = split_dataset(x_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data set: 47\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train data set: {len(train_data_dict)}\")\n",
    "print(f\"Length of validation data set: {len(val_data_dict)}\")\n",
    "print(f\"Length of test data set: {len(test_data_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (47, 8, 75000)\n",
      "Shape of validation data set: (16, 8, 75000)\n",
      "Shape of test data set: (16, 8, 75000)\n"
     ]
    }
   ],
   "source": [
    "train_data_arr = dict_to_arr(train_data_dict)\n",
    "test_data_arr = dict_to_arr(test_data_dict)\n",
    "val_data_arr = dict_to_arr(val_data_dict)\n",
    "\n",
    "train_labels_arr = np.reshape(np.array(list(train_labels_dict.values())), (len(train_data_arr),1))\n",
    "test_labels_arr = np.reshape(np.array(list(test_labels_dict.values())), (len(test_data_arr),1))\n",
    "val_labels_arr = np.reshape(np.array(list(val_labels_dict.values())), (len(val_data_arr),1))\n",
    "\n",
    "train_labels_arr[train_labels_arr == 2] = 1\n",
    "test_labels_arr[test_labels_arr == 2] = 1\n",
    "val_labels_arr[val_labels_arr == 2] = 1\n",
    "\n",
    "\n",
    "print(f\"Shape of train data set: {train_data_arr.shape}\")\n",
    "print(f\"Shape of validation data set: {val_data_arr.shape}\")\n",
    "print(f\"Shape of test data set: {test_data_arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 1)\n",
      "(16, 1)\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_arr.shape)\n",
    "print(val_labels_arr.shape)\n",
    "print(test_labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# convert labels to one-hot encodings.\\ntrain_labels = np_utils.to_categorical(train_labels_arr-1)\\nval_labels   = np_utils.to_categorical(val_labels_arr-1)\\ntest_labels  = np_utils.to_categorical(test_labels_arr-1)\\n\\n# convert data to NHWC (trials, channels, samples, kernels) format. Data \\n# contains 60 channels and 151 time-points. Set the number of kernels to 1.\\nkernels = 1\\n\\ntrain_data      = train_data_arr.reshape(train_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\\nval_data        = val_data_arr.reshape(val_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\\ntest_data       = test_data_arr.reshape(test_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# convert labels to one-hot encodings.\n",
    "train_labels = np_utils.to_categorical(train_labels_arr-1)\n",
    "val_labels   = np_utils.to_categorical(val_labels_arr-1)\n",
    "test_labels  = np_utils.to_categorical(test_labels_arr-1)\n",
    "\n",
    "# convert data to NHWC (trials, channels, samples, kernels) format. Data \n",
    "# contains 60 channels and 151 time-points. Set the number of kernels to 1.\n",
    "kernels = 1\n",
    "\n",
    "train_data      = train_data_arr.reshape(train_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\n",
    "val_data        = val_data_arr.reshape(val_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\n",
    "test_data       = test_data_arr.reshape(test_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.67070, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 21s - loss: 0.8938 - accuracy: 0.5319 - val_loss: 0.6707 - val_accuracy: 0.6250 - 21s/epoch - 7s/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.2337 - accuracy: 0.9787 - val_loss: 0.6995 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.1559 - accuracy: 1.0000 - val_loss: 0.6922 - val_accuracy: 0.5625 - 16s/epoch - 5s/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.0988 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 0.6250 - 16s/epoch - 5s/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.0769 - accuracy: 1.0000 - val_loss: 0.6783 - val_accuracy: 0.5625 - 16s/epoch - 5s/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.6759 - val_accuracy: 0.6250 - 16s/epoch - 5s/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.6755 - val_accuracy: 0.6250 - 15s/epoch - 5s/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.6753 - val_accuracy: 0.6250 - 15s/epoch - 5s/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.6754 - val_accuracy: 0.6250 - 14s/epoch - 5s/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.6764 - val_accuracy: 0.5625 - 15s/epoch - 5s/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.6772 - val_accuracy: 0.5625 - 14s/epoch - 5s/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.6779 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.6785 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.6799 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.6806 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.6812 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.6812 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.6808 - val_accuracy: 0.5000 - 16s/epoch - 5s/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.67070\n",
      "3/3 - 368s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.6810 - val_accuracy: 0.5000 - 368s/epoch - 123s/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.6827 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 0.5000 - 16s/epoch - 5s/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.67070\n",
      "3/3 - 18s - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.5000 - 18s/epoch - 6s/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6840 - val_accuracy: 0.5000 - 16s/epoch - 5s/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6849 - val_accuracy: 0.4375 - 15s/epoch - 5s/step\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.6847 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6843 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6830 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6821 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.6823 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6835 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.67070\n",
      "3/3 - 15s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 0.4375 - 15s/epoch - 5s/step\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6846 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.67070\n",
      "3/3 - 16s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.4375 - 16s/epoch - 5s/step\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.67070\n",
      "3/3 - 17s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.4375 - 17s/epoch - 6s/step\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.67070\n",
      "3/3 - 14s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.67070\n",
      "3/3 - 13s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.67070\n",
      "3/3 - 13s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.6809 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.67070\n",
      "3/3 - 13s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.6768 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.67070\n",
      "3/3 - 13s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: val_loss improved from 0.67070 to 0.66902, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 13s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6690 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.6707 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.66902\n",
      "3/3 - 12s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6711 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.6703 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.66902\n",
      "3/3 - 12s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.6727 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.6709 - val_accuracy: 0.3750 - 13s/epoch - 4s/step\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.6699 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.66902\n",
      "3/3 - 14s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.6703 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.66902\n",
      "3/3 - 16s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6712 - val_accuracy: 0.4375 - 16s/epoch - 5s/step\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6726 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.6714 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6724 - val_accuracy: 0.3750 - 13s/epoch - 4s/step\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.66902\n",
      "3/3 - 13s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6710 - val_accuracy: 0.3750 - 13s/epoch - 4s/step\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 56: val_loss improved from 0.66902 to 0.66801, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 13s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.6680 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: val_loss improved from 0.66801 to 0.66425, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 13s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6643 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 58: val_loss improved from 0.66425 to 0.66049, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 13s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6605 - val_accuracy: 0.3750 - 13s/epoch - 4s/step\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 59: val_loss improved from 0.66049 to 0.65893, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 13s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6589 - val_accuracy: 0.3750 - 13s/epoch - 4s/step\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.65893\n",
      "3/3 - 13s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 0.3750 - 13s/epoch - 4s/step\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.65893\n",
      "3/3 - 13s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6632 - val_accuracy: 0.3750 - 13s/epoch - 4s/step\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.65893\n",
      "3/3 - 13s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6674 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.65893\n",
      "3/3 - 13s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.65893\n",
      "3/3 - 13s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6708 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.65893\n",
      "3/3 - 13s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6719 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.65893\n",
      "3/3 - 16s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.4375 - 16s/epoch - 5s/step\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.65893\n",
      "3/3 - 166s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6721 - val_accuracy: 0.4375 - 166s/epoch - 55s/step\n",
      "Epoch 68/100\n"
     ]
    }
   ],
   "source": [
    "probs = EEGNet_classification_2(train_data_arr, test_data_arr, val_data_arr, train_labels_arr, test_labels_arr, val_labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = probs.argmax(axis = -1)  \n",
    "print(preds)\n",
    "\n",
    "print(test_labels_arr.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(preds == test_labels_arr)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
