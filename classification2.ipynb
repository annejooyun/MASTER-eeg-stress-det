{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tensorflow.keras import utils as np_utils\n",
    "\n",
    "from utils.data import extract_eeg_data, multi_to_binary_classification, split_dataset, dict_to_arr\n",
    "from utils.labels import get_pss_labels, get_stai_labels\n",
    "from utils.valid_recs import get_valid_recs\n",
    "from features import all_features, hjorth_features\n",
    "from classifiers import knn_classification, svm_classification, cnn_classification, EEGNet_classification_2\n",
    "import utils.variables as v\n",
    "\n",
    "from EEGModels import EEGNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P006_S002_001\n",
      "ERROR:root:1) Failed to read data for recording P006_S002_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out invalid recordings\n",
      "\n",
      "Data/ICA_data\\sub-P010_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P013_ses-S001_run-001.mat not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/ICA_data\\sub-P013_ses-S001_run-002.mat not valid\n",
      "Data/ICA_data\\sub-P020_ses-S001_run-001.mat not valid\n",
      "Data/ICA_data\\sub-P023_ses-S002_run-002.mat not valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:1) Failed to read data for recording P028_S001_002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning valid recordings\n",
      "\n",
      "Valid recs ['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S001_001', 'P002_S001_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S001_001', 'P004_S001_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P005_S002_001', 'P005_S002_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S001_002', 'P008_S002_001', 'P008_S002_002', 'P009_S001_001', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_001', 'P012_S001_002', 'P012_S002_001', 'P012_S002_002', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P015_S002_002', 'P016_S001_001', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_001', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S001_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_001', 'P021_S001_002', 'P021_S002_001', 'P021_S002_002', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P024_S002_002', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S001_001', 'P026_S001_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S001_002', 'P027_S002_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002']\n"
     ]
    }
   ],
   "source": [
    "valid_recs = get_valid_recs(data_type='ica', output_type = 'np')\n",
    "print(f'Valid recs {valid_recs}')\n",
    "\n",
    "x_dict_ = extract_eeg_data(valid_recs, data_type='ica', output_type='np')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SubjectNo  D1Y1  D2Y1  J1Y1  J2Y1\n",
      "0           1    26    30    29    31\n",
      "1           2    38    41    26    34\n",
      "2           3    58    56    36    35\n",
      "3           4    40    45    24    24\n",
      "4           5    25    31    38    37\n",
      "5           6    49    58     0     0\n",
      "6           7    56    50    28    28\n",
      "7           8    46    37    23    27\n",
      "8           9    41    47    27    22\n",
      "9          10    37    20    23    21\n",
      "10         11    50    49    31    47\n",
      "11         12    42    47    47    41\n",
      "12         13    35    35    28    33\n",
      "13         14    54    35    26    26\n",
      "14         15    51    55    33    42\n",
      "15         16    35    38    42    45\n",
      "16         17    37    35    24    20\n",
      "17         18    54    62    41    48\n",
      "18         19    47    52    30    36\n",
      "19         20    46    38    24    25\n",
      "20         21    44    54    33    39\n",
      "21         22    49    51    28    34\n",
      "22         23    56    53    33    28\n",
      "23         24    52    58    36    41\n",
      "24         25    48    62    29    56\n",
      "25         26    43    37    25    26\n",
      "26         27    52    41    41    34\n",
      "27         28     0     0    29    29\n",
      "Invalid value for label\n",
      "Invalid value for label\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid record\n",
      "Invalid value for label\n",
      "Invalid value for label\n",
      "{'P001_S001_001': 0, 'P001_S001_002': 0, 'P001_S002_001': 0, 'P001_S002_002': 0, 'P002_S001_001': 1, 'P002_S001_002': 1, 'P002_S002_001': 0, 'P002_S002_002': 0, 'P003_S001_001': 2, 'P003_S001_002': 2, 'P003_S002_001': 0, 'P003_S002_002': 0, 'P004_S001_001': 1, 'P004_S001_002': 1, 'P004_S002_001': 0, 'P004_S002_002': 0, 'P005_S001_001': 0, 'P005_S001_002': 0, 'P005_S002_001': 1, 'P005_S002_002': 1, 'P006_S001_001': 2, 'P006_S001_002': 2, 'P007_S001_001': 2, 'P007_S001_002': 2, 'P007_S002_001': 0, 'P007_S002_002': 0, 'P008_S001_001': 2, 'P008_S001_002': 1, 'P008_S002_001': 0, 'P008_S002_002': 0, 'P009_S001_001': 1, 'P009_S001_002': 2, 'P009_S002_001': 0, 'P009_S002_002': 0, 'P010_S001_002': 0, 'P010_S002_001': 0, 'P010_S002_002': 0, 'P011_S001_001': 2, 'P011_S001_002': 2, 'P011_S002_001': 0, 'P011_S002_002': 2, 'P012_S001_001': 1, 'P012_S001_002': 2, 'P012_S002_001': 2, 'P012_S002_002': 1, 'P013_S002_001': 0, 'P013_S002_002': 0, 'P014_S001_001': 2, 'P014_S001_002': 0, 'P014_S002_001': 0, 'P014_S002_002': 0, 'P015_S001_001': 2, 'P015_S001_002': 2, 'P015_S002_001': 0, 'P015_S002_002': 1, 'P016_S001_001': 0, 'P016_S001_002': 1, 'P016_S002_001': 1, 'P016_S002_002': 1, 'P017_S001_001': 1, 'P017_S001_002': 0, 'P017_S002_001': 0, 'P017_S002_002': 0, 'P018_S001_001': 2, 'P018_S001_002': 2, 'P018_S002_001': 1, 'P018_S002_002': 2, 'P019_S001_001': 2, 'P019_S001_002': 2, 'P019_S002_001': 0, 'P019_S002_002': 0, 'P020_S001_002': 1, 'P020_S002_001': 0, 'P020_S002_002': 0, 'P021_S001_001': 1, 'P021_S001_002': 2, 'P021_S002_001': 0, 'P021_S002_002': 1, 'P022_S001_001': 2, 'P022_S001_002': 2, 'P022_S002_001': 0, 'P022_S002_002': 0, 'P023_S001_001': 2, 'P023_S001_002': 2, 'P023_S002_001': 0, 'P024_S001_001': 2, 'P024_S001_002': 2, 'P024_S002_001': 0, 'P024_S002_002': 1, 'P025_S001_001': 2, 'P025_S001_002': 2, 'P025_S002_001': 0, 'P025_S002_002': 2, 'P026_S001_001': 1, 'P026_S001_002': 1, 'P026_S002_001': 0, 'P026_S002_002': 0, 'P027_S001_001': 2, 'P027_S001_002': 1, 'P027_S002_001': 1, 'P027_S002_002': 0, 'P028_S002_001': 0, 'P028_S002_002': 0}\n"
     ]
    }
   ],
   "source": [
    "y_dict_ = get_stai_labels(valid_recs) \n",
    "#y_dict = get_pss_labels(valid_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing invalid labels: 103\n",
      " Lenght og labels after removing invalid labels: 103\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing invalid labels: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing invalid labels: {len(y_dict_)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The extracted keys : \n",
      "['P002_S001_001', 'P002_S001_002', 'P004_S001_001', 'P004_S001_002', 'P005_S002_001', 'P005_S002_002', 'P008_S001_002', 'P009_S001_001', 'P012_S001_001', 'P012_S002_002', 'P015_S002_002', 'P016_S001_002', 'P016_S002_001', 'P016_S002_002', 'P017_S001_001', 'P018_S002_001', 'P020_S001_002', 'P021_S001_001', 'P021_S002_002', 'P024_S002_002', 'P026_S001_001', 'P026_S001_002', 'P027_S001_002', 'P027_S002_001']\n",
      "\n",
      "Dictionary after removal of keys from y_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n",
      "\n",
      "Dictionary after removal of keys from x_dict: \n",
      " dict_keys(['P001_S001_001', 'P001_S001_002', 'P001_S002_001', 'P001_S002_002', 'P002_S002_001', 'P002_S002_002', 'P003_S001_001', 'P003_S001_002', 'P003_S002_001', 'P003_S002_002', 'P004_S002_001', 'P004_S002_002', 'P005_S001_001', 'P005_S001_002', 'P006_S001_001', 'P006_S001_002', 'P007_S001_001', 'P007_S001_002', 'P007_S002_001', 'P007_S002_002', 'P008_S001_001', 'P008_S002_001', 'P008_S002_002', 'P009_S001_002', 'P009_S002_001', 'P009_S002_002', 'P010_S001_002', 'P010_S002_001', 'P010_S002_002', 'P011_S001_001', 'P011_S001_002', 'P011_S002_001', 'P011_S002_002', 'P012_S001_002', 'P012_S002_001', 'P013_S002_001', 'P013_S002_002', 'P014_S001_001', 'P014_S001_002', 'P014_S002_001', 'P014_S002_002', 'P015_S001_001', 'P015_S001_002', 'P015_S002_001', 'P016_S001_001', 'P017_S001_002', 'P017_S002_001', 'P017_S002_002', 'P018_S001_001', 'P018_S001_002', 'P018_S002_002', 'P019_S001_001', 'P019_S001_002', 'P019_S002_001', 'P019_S002_002', 'P020_S002_001', 'P020_S002_002', 'P021_S001_002', 'P021_S002_001', 'P022_S001_001', 'P022_S001_002', 'P022_S002_001', 'P022_S002_002', 'P023_S001_001', 'P023_S001_002', 'P023_S002_001', 'P024_S001_001', 'P024_S001_002', 'P024_S002_001', 'P025_S001_001', 'P025_S001_002', 'P025_S002_001', 'P025_S002_002', 'P026_S002_001', 'P026_S002_002', 'P027_S001_001', 'P027_S002_002', 'P028_S002_001', 'P028_S002_002'])\n"
     ]
    }
   ],
   "source": [
    "x_dict, y_dict = multi_to_binary_classification(x_dict_, y_dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Length of data after removing mildly stressed subjects: 79\n",
      " Lenght og labels after removing  mildly stressed subjects: 79\n"
     ]
    }
   ],
   "source": [
    "print(f\" Length of data after removing mildly stressed subjects: {len(x_dict_)}\")\n",
    "print(f\" Lenght og labels after removing  mildly stressed subjects: {len(y_dict_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict, test_data_dict, val_data_dict, train_labels_dict, test_labels_dict, val_labels_dict = split_dataset(x_dict, y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data set: 47\n",
      "Length of validation data set: 16\n",
      "Length of test data set: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train data set: {len(train_data_dict)}\")\n",
    "print(f\"Length of validation data set: {len(val_data_dict)}\")\n",
    "print(f\"Length of test data set: {len(test_data_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (47, 8, 75000)\n",
      "Shape of validation data set: (16, 8, 75000)\n",
      "Shape of test data set: (16, 8, 75000)\n"
     ]
    }
   ],
   "source": [
    "train_data_arr = dict_to_arr(train_data_dict)\n",
    "test_data_arr = dict_to_arr(test_data_dict)\n",
    "val_data_arr = dict_to_arr(val_data_dict)\n",
    "\n",
    "train_labels_arr = np.reshape(np.array(list(train_labels_dict.values())), (len(train_data_arr),1))\n",
    "test_labels_arr = np.reshape(np.array(list(test_labels_dict.values())), (len(test_data_arr),1))\n",
    "val_labels_arr = np.reshape(np.array(list(val_labels_dict.values())), (len(val_data_arr),1))\n",
    "\n",
    "train_labels_arr[train_labels_arr == 2] = 1\n",
    "test_labels_arr[test_labels_arr == 2] = 1\n",
    "val_labels_arr[val_labels_arr == 2] = 1\n",
    "\n",
    "\n",
    "print(f\"Shape of train data set: {train_data_arr.shape}\")\n",
    "print(f\"Shape of validation data set: {val_data_arr.shape}\")\n",
    "print(f\"Shape of test data set: {test_data_arr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 1)\n",
      "(16, 1)\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_arr.shape)\n",
    "print(val_labels_arr.shape)\n",
    "print(test_labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# convert labels to one-hot encodings.\n",
    "train_labels = np_utils.to_categorical(train_labels_arr-1)\n",
    "val_labels   = np_utils.to_categorical(val_labels_arr-1)\n",
    "test_labels  = np_utils.to_categorical(test_labels_arr-1)\n",
    "\n",
    "# convert data to NHWC (trials, channels, samples, kernels) format. Data \n",
    "# contains 60 channels and 151 time-points. Set the number of kernels to 1.\n",
    "kernels = 1\n",
    "\n",
    "train_data      = train_data_arr.reshape(train_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\n",
    "val_data        = val_data_arr.reshape(val_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\n",
    "test_data       = test_data_arr.reshape(test_data_arr.shape[0], v.NUM_CHANNELS, v.NUM_SAMPLES, kernels)\"\"\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data set: (47, 8, 75000, 1)\n",
      "Shape of validation data set: (16, 8, 75000, 1)\n",
      "Shape of test data set: (16, 8, 75000, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Shape of train data set: {train_data.shape}\")\n",
    "print(f\"Shape of validation data set: {val_data.shape}\")\n",
    "print(f\"Shape of test data set: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the EEGNet-8,2,16 model with kernel length of 32 samples (other \n",
    "# model configurations may do better, but this is a good starting point)\n",
    "model = EEGNet(nb_classes = 2, Chans = v.NUM_CHANNELS , Samples = v.NUM_SAMPLES, \n",
    "            dropoutRate = 0.5, kernLength = int(v.SFREQ / 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 21s 7s/step - loss: 0.7811 - accuracy: 0.5106 - val_loss: 0.6915 - val_accuracy: 0.5625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26d97ddac80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_arr, train_labels_arr, validation_data=(val_data_arr, val_labels_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 479ms/step\n",
      "Classification accuracy: 0.312500 \n"
     ]
    }
   ],
   "source": [
    "probs       = model.predict(test_data_arr)\n",
    "preds       = probs.argmax(axis = -1)  \n",
    "acc         = np.mean(preds == test_labels_arr.argmax(axis=-1))\n",
    "print(\"Classification accuracy: %f \" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.65674, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 12s - loss: 0.8180 - accuracy: 0.5319 - val_loss: 0.6567 - val_accuracy: 0.5625 - 12s/epoch - 4s/step\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 2: val_loss improved from 0.65674 to 0.64559, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 11s - loss: 0.2255 - accuracy: 0.9574 - val_loss: 0.6456 - val_accuracy: 0.6875 - 11s/epoch - 4s/step\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 3: val_loss improved from 0.64559 to 0.62344, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 11s - loss: 0.1146 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 0.6250 - 11s/epoch - 4s/step\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 4: val_loss improved from 0.62344 to 0.62218, saving model to /tmp\\checkpoint.h5\n",
      "3/3 - 11s - loss: 0.0915 - accuracy: 1.0000 - val_loss: 0.6222 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0537 - accuracy: 1.0000 - val_loss: 0.6268 - val_accuracy: 0.6875 - 11s/epoch - 4s/step\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 0.0482 - accuracy: 1.0000 - val_loss: 0.6296 - val_accuracy: 0.6875 - 10s/epoch - 3s/step\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.6321 - val_accuracy: 0.6875 - 10s/epoch - 3s/step\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.6350 - val_accuracy: 0.6875 - 10s/epoch - 3s/step\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.6353 - val_accuracy: 0.6250 - 11s/epoch - 4s/step\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.6358 - val_accuracy: 0.6250 - 10s/epoch - 3s/step\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 0.6250 - 11s/epoch - 4s/step\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.6250 - 11s/epoch - 4s/step\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.6337 - val_accuracy: 0.6250 - 11s/epoch - 4s/step\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 0.6250 - 10s/epoch - 3s/step\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.5625 - 10s/epoch - 3s/step\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.6390 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.6405 - val_accuracy: 0.5625 - 12s/epoch - 4s/step\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.6421 - val_accuracy: 0.5625 - 14s/epoch - 5s/step\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.6473 - val_accuracy: 0.5000 - 11s/epoch - 4s/step\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.6480 - val_accuracy: 0.5000 - 11s/epoch - 4s/step\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.6477 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.6484 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.6494 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.6494 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.6484 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.6492 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 0.5625 - 12s/epoch - 4s/step\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.6552 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.6555 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.6560 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6585 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6569 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.6586 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.6561 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6556 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6566 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6587 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.6599 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6608 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6576 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 0.5625 - 14s/epoch - 5s/step\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.62218\n",
      "3/3 - 16s - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.6576 - val_accuracy: 0.5000 - 16s/epoch - 5s/step\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.62218\n",
      "3/3 - 17s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.6600 - val_accuracy: 0.5000 - 17s/epoch - 6s/step\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.6581 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.6571 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.6549 - val_accuracy: 0.5625 - 14s/epoch - 5s/step\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.6638 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.62218\n",
      "3/3 - 16s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.6672 - val_accuracy: 0.5000 - 16s/epoch - 5s/step\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.62218\n",
      "3/3 - 16s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.6727 - val_accuracy: 0.5000 - 16s/epoch - 5s/step\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.6771 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.6788 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.6818 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.6831 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6849 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.6803 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.6780 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6801 - val_accuracy: 0.5625 - 12s/epoch - 4s/step\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6859 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.6900 - val_accuracy: 0.5625 - 12s/epoch - 4s/step\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6929 - val_accuracy: 0.5625 - 12s/epoch - 4s/step\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6954 - val_accuracy: 0.5625 - 11s/epoch - 4s/step\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.7010 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.7044 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7116 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.7154 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7169 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.62218\n",
      "3/3 - 16s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7163 - val_accuracy: 0.5000 - 16s/epoch - 5s/step\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.62218\n",
      "3/3 - 15s - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7140 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7068 - val_accuracy: 0.5625 - 14s/epoch - 5s/step\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7010 - val_accuracy: 0.5625 - 14s/epoch - 5s/step\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6956 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.6930 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.6950 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.6959 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.6979 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7012 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7038 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.7068 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.7092 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.7093 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.7083 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7105 - val_accuracy: 0.5625 - 13s/epoch - 4s/step\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7108 - val_accuracy: 0.5625 - 13s/epoch - 4s/step\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.7114 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7130 - val_accuracy: 0.5000 - 13s/epoch - 4s/step\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7150 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7163 - val_accuracy: 0.5000 - 11s/epoch - 4s/step\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7129 - val_accuracy: 0.5625 - 12s/epoch - 4s/step\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7111 - val_accuracy: 0.5625 - 14s/epoch - 5s/step\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.62218\n",
      "3/3 - 17s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7122 - val_accuracy: 0.5625 - 17s/epoch - 6s/step\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.62218\n",
      "3/3 - 15s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7145 - val_accuracy: 0.5000 - 15s/epoch - 5s/step\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.62218\n",
      "3/3 - 15s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7197 - val_accuracy: 0.4375 - 15s/epoch - 5s/step\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7241 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7269 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7281 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7312 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7331 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7358 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7388 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7408 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7395 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7409 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7403 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.7398 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7405 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.62218\n",
      "3/3 - 16s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7412 - val_accuracy: 0.4375 - 16s/epoch - 5s/step\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.62218\n",
      "3/3 - 16s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7411 - val_accuracy: 0.4375 - 16s/epoch - 5s/step\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.7393 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7405 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7404 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.62218\n",
      "3/3 - 17s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7412 - val_accuracy: 0.4375 - 17s/epoch - 6s/step\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.62218\n",
      "3/3 - 15s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7427 - val_accuracy: 0.4375 - 15s/epoch - 5s/step\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7432 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7446 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7463 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7484 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7504 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7525 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7527 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.7535 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7562 - val_accuracy: 0.5000 - 11s/epoch - 4s/step\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7588 - val_accuracy: 0.5000 - 11s/epoch - 4s/step\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7631 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7682 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7720 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.62218\n",
      "3/3 - 18s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7751 - val_accuracy: 0.4375 - 18s/epoch - 6s/step\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.62218\n",
      "3/3 - 18s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7777 - val_accuracy: 0.4375 - 18s/epoch - 6s/step\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7780 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.62218\n",
      "3/3 - 16s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7786 - val_accuracy: 0.4375 - 16s/epoch - 5s/step\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7772 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7754 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7730 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7720 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7710 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7696 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7711 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7731 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7717 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7685 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7657 - val_accuracy: 0.5000 - 12s/epoch - 4s/step\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7631 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7601 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7604 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7604 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7623 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7633 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7633 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7658 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7684 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7721 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7763 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.7588e-04 - accuracy: 1.0000 - val_loss: 0.7795 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7827 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7849 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 9.9239e-04 - accuracy: 1.0000 - val_loss: 0.7857 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.0518e-04 - accuracy: 1.0000 - val_loss: 0.7868 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.8326e-04 - accuracy: 1.0000 - val_loss: 0.7895 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7927 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.2914e-04 - accuracy: 1.0000 - val_loss: 0.7938 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 8.7036e-04 - accuracy: 1.0000 - val_loss: 0.7929 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.6949e-04 - accuracy: 1.0000 - val_loss: 0.7912 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.9723e-04 - accuracy: 1.0000 - val_loss: 0.7880 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 8.9833e-04 - accuracy: 1.0000 - val_loss: 0.7869 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.8044e-04 - accuracy: 1.0000 - val_loss: 0.7862 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.9124e-04 - accuracy: 1.0000 - val_loss: 0.7861 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.0083e-04 - accuracy: 1.0000 - val_loss: 0.7842 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 9.2519e-04 - accuracy: 1.0000 - val_loss: 0.7836 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.5268e-04 - accuracy: 1.0000 - val_loss: 0.7832 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.9319e-04 - accuracy: 1.0000 - val_loss: 0.7827 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.6677e-04 - accuracy: 1.0000 - val_loss: 0.7825 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 8.2382e-04 - accuracy: 1.0000 - val_loss: 0.7819 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 8.2056e-04 - accuracy: 1.0000 - val_loss: 0.7808 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 7.2746e-04 - accuracy: 1.0000 - val_loss: 0.7811 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7823 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.7304e-04 - accuracy: 1.0000 - val_loss: 0.7842 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 7.8511e-04 - accuracy: 1.0000 - val_loss: 0.7852 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.2292e-04 - accuracy: 1.0000 - val_loss: 0.7865 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 8.0584e-04 - accuracy: 1.0000 - val_loss: 0.7875 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.62218\n",
      "3/3 - 17s - loss: 7.7935e-04 - accuracy: 1.0000 - val_loss: 0.7882 - val_accuracy: 0.4375 - 17s/epoch - 6s/step\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 8.2710e-04 - accuracy: 1.0000 - val_loss: 0.7870 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.9580e-04 - accuracy: 1.0000 - val_loss: 0.7876 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.6312e-04 - accuracy: 1.0000 - val_loss: 0.7887 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.5368e-04 - accuracy: 1.0000 - val_loss: 0.7899 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.8335e-04 - accuracy: 1.0000 - val_loss: 0.7922 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 8.1201e-04 - accuracy: 1.0000 - val_loss: 0.7945 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.3952e-04 - accuracy: 1.0000 - val_loss: 0.7962 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.0206e-04 - accuracy: 1.0000 - val_loss: 0.7977 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7946 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 6.8099e-04 - accuracy: 1.0000 - val_loss: 0.7927 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 7.5818e-04 - accuracy: 1.0000 - val_loss: 0.7898 - val_accuracy: 0.5000 - 14s/epoch - 5s/step\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 5.7486e-04 - accuracy: 1.0000 - val_loss: 0.7882 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.62218\n",
      "3/3 - 14s - loss: 7.7827e-04 - accuracy: 1.0000 - val_loss: 0.7872 - val_accuracy: 0.4375 - 14s/epoch - 5s/step\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.3571e-04 - accuracy: 1.0000 - val_loss: 0.7915 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.6540e-04 - accuracy: 1.0000 - val_loss: 0.7954 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.9063e-04 - accuracy: 1.0000 - val_loss: 0.7987 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.4257e-04 - accuracy: 1.0000 - val_loss: 0.8011 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.62218\n",
      "3/3 - 13s - loss: 7.0161e-04 - accuracy: 1.0000 - val_loss: 0.8035 - val_accuracy: 0.4375 - 13s/epoch - 4s/step\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.6655e-04 - accuracy: 1.0000 - val_loss: 0.8052 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.1212e-04 - accuracy: 1.0000 - val_loss: 0.8055 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 7.2527e-04 - accuracy: 1.0000 - val_loss: 0.8050 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 6.5749e-04 - accuracy: 1.0000 - val_loss: 0.8037 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.9612e-04 - accuracy: 1.0000 - val_loss: 0.8036 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 5.9007e-04 - accuracy: 1.0000 - val_loss: 0.8034 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.3118e-04 - accuracy: 1.0000 - val_loss: 0.8037 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.6140e-04 - accuracy: 1.0000 - val_loss: 0.8050 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 5.9716e-04 - accuracy: 1.0000 - val_loss: 0.8066 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 6.0611e-04 - accuracy: 1.0000 - val_loss: 0.8085 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.9935e-04 - accuracy: 1.0000 - val_loss: 0.8109 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 6.5389e-04 - accuracy: 1.0000 - val_loss: 0.8131 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.9809e-04 - accuracy: 1.0000 - val_loss: 0.8145 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.6409e-04 - accuracy: 1.0000 - val_loss: 0.8137 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 6.1085e-04 - accuracy: 1.0000 - val_loss: 0.8140 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 6.1650e-04 - accuracy: 1.0000 - val_loss: 0.8151 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.9006e-04 - accuracy: 1.0000 - val_loss: 0.8165 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.9786e-04 - accuracy: 1.0000 - val_loss: 0.8179 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 7.2265e-04 - accuracy: 1.0000 - val_loss: 0.8170 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 6.3331e-04 - accuracy: 1.0000 - val_loss: 0.8167 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.5754e-04 - accuracy: 1.0000 - val_loss: 0.8176 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.7283e-04 - accuracy: 1.0000 - val_loss: 0.8193 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 6.2933e-04 - accuracy: 1.0000 - val_loss: 0.8205 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.4424e-04 - accuracy: 1.0000 - val_loss: 0.8219 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 6.2651e-04 - accuracy: 1.0000 - val_loss: 0.8219 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 6.0586e-04 - accuracy: 1.0000 - val_loss: 0.8184 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.4259e-04 - accuracy: 1.0000 - val_loss: 0.8168 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.7451e-04 - accuracy: 1.0000 - val_loss: 0.8144 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.6794e-04 - accuracy: 1.0000 - val_loss: 0.8134 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.3771e-04 - accuracy: 1.0000 - val_loss: 0.8136 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.3608e-04 - accuracy: 1.0000 - val_loss: 0.8130 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 5.4688e-04 - accuracy: 1.0000 - val_loss: 0.8152 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.4595e-04 - accuracy: 1.0000 - val_loss: 0.8175 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.6819e-04 - accuracy: 1.0000 - val_loss: 0.8199 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.3876e-04 - accuracy: 1.0000 - val_loss: 0.8227 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.3146e-04 - accuracy: 1.0000 - val_loss: 0.8261 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.3459e-04 - accuracy: 1.0000 - val_loss: 0.8298 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.0178e-04 - accuracy: 1.0000 - val_loss: 0.8320 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 5.1571e-04 - accuracy: 1.0000 - val_loss: 0.8344 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 5.1411e-04 - accuracy: 1.0000 - val_loss: 0.8381 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.7356e-04 - accuracy: 1.0000 - val_loss: 0.8404 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 5.1143e-04 - accuracy: 1.0000 - val_loss: 0.8409 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.7365e-04 - accuracy: 1.0000 - val_loss: 0.8418 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.6370e-04 - accuracy: 1.0000 - val_loss: 0.8401 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.7514e-04 - accuracy: 1.0000 - val_loss: 0.8408 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 5.6703e-04 - accuracy: 1.0000 - val_loss: 0.8454 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.5329e-04 - accuracy: 1.0000 - val_loss: 0.8477 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 5.2531e-04 - accuracy: 1.0000 - val_loss: 0.8489 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 6.0265e-04 - accuracy: 1.0000 - val_loss: 0.8484 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.3409e-04 - accuracy: 1.0000 - val_loss: 0.8463 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.2315e-04 - accuracy: 1.0000 - val_loss: 0.8452 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.7054e-04 - accuracy: 1.0000 - val_loss: 0.8446 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.8900e-04 - accuracy: 1.0000 - val_loss: 0.8436 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.4734e-04 - accuracy: 1.0000 - val_loss: 0.8427 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.6410e-04 - accuracy: 1.0000 - val_loss: 0.8419 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.4970e-04 - accuracy: 1.0000 - val_loss: 0.8424 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.1208e-04 - accuracy: 1.0000 - val_loss: 0.8435 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.2624e-04 - accuracy: 1.0000 - val_loss: 0.8428 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.6798e-04 - accuracy: 1.0000 - val_loss: 0.8429 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.1283e-04 - accuracy: 1.0000 - val_loss: 0.8434 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.0602e-04 - accuracy: 1.0000 - val_loss: 0.8448 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.9277e-04 - accuracy: 1.0000 - val_loss: 0.8435 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.7506e-04 - accuracy: 1.0000 - val_loss: 0.8419 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 3.8179e-04 - accuracy: 1.0000 - val_loss: 0.8403 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.5360e-04 - accuracy: 1.0000 - val_loss: 0.8396 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.6294e-04 - accuracy: 1.0000 - val_loss: 0.8425 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.2973e-04 - accuracy: 1.0000 - val_loss: 0.8446 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.62218\n",
      "3/3 - 10s - loss: 4.3935e-04 - accuracy: 1.0000 - val_loss: 0.8479 - val_accuracy: 0.4375 - 10s/epoch - 3s/step\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.5033e-04 - accuracy: 1.0000 - val_loss: 0.8500 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.1311e-04 - accuracy: 1.0000 - val_loss: 0.8487 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.2292e-04 - accuracy: 1.0000 - val_loss: 0.8470 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.6923e-04 - accuracy: 1.0000 - val_loss: 0.8469 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.4790e-04 - accuracy: 1.0000 - val_loss: 0.8475 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.7496e-04 - accuracy: 1.0000 - val_loss: 0.8482 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.9344e-04 - accuracy: 1.0000 - val_loss: 0.8491 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.1268e-04 - accuracy: 1.0000 - val_loss: 0.8496 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.8245e-04 - accuracy: 1.0000 - val_loss: 0.8497 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.7130e-04 - accuracy: 1.0000 - val_loss: 0.8490 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.9968e-04 - accuracy: 1.0000 - val_loss: 0.8481 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.3905e-04 - accuracy: 1.0000 - val_loss: 0.8472 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 5.7081e-04 - accuracy: 1.0000 - val_loss: 0.8482 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 3.6174e-04 - accuracy: 1.0000 - val_loss: 0.8495 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.0603e-04 - accuracy: 1.0000 - val_loss: 0.8490 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 3.6706e-04 - accuracy: 1.0000 - val_loss: 0.8493 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.4204e-04 - accuracy: 1.0000 - val_loss: 0.8490 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.1258e-04 - accuracy: 1.0000 - val_loss: 0.8506 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 4.1733e-04 - accuracy: 1.0000 - val_loss: 0.8524 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.62218\n",
      "3/3 - 11s - loss: 3.5294e-04 - accuracy: 1.0000 - val_loss: 0.8533 - val_accuracy: 0.4375 - 11s/epoch - 4s/step\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.8547e-04 - accuracy: 1.0000 - val_loss: 0.8544 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.1407e-04 - accuracy: 1.0000 - val_loss: 0.8553 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 4.0665e-04 - accuracy: 1.0000 - val_loss: 0.8554 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.7578e-04 - accuracy: 1.0000 - val_loss: 0.8539 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.62218\n",
      "3/3 - 12s - loss: 3.9156e-04 - accuracy: 1.0000 - val_loss: 0.8540 - val_accuracy: 0.4375 - 12s/epoch - 4s/step\n",
      "1/1 [==============================] - 1s 675ms/step\n",
      "Classification accuracy: 0.875000 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = EEGNet_classification_2(train_data_arr, test_data_arr, val_data_arr, train_labels_arr, test_labels_arr, val_labels_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
